#+TITLE: Parallel Simulation of SystemC Loosely-Timed Transaction Level Models
#+AUTHOR:Konstantinos Sotiropoulos
#+EMAIL: kisp@kth.se
#+STARTUP: overview
#+KEYWORDS: parallel discrete event simulation, conservative synchronization algorithms, transaction level models, SystemC TLM 2.0
#+OPTIONS: toc:nil title:nil date:nil creator:nil email:nil author:nil broken-links:mark tasks:nil

* Latex Preamble                                                     :ignore:
#+LATEX_HEADER: \documentclass[11pt,a4paper,oneside,openright,abstractoff,titlepage,final,BCOR10mm]{scrreprt}
#+LATEX_HEADER: \usepackage[margin=25mm]{geometry}
#+LATEX_HEADER: \usepackage[margin=25mm]{geometry}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage{inputenc}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{lastpage}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage[table]{xcolor}
#+LATEX_HEADER: \usepackage{tabularx}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{rotating} 
#+LATEX_HEADER: \usepackage{lmodern} 
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{microtype}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage[colorlinks]{hyperref}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \usepackage{pdfpages} 
#+LATEX_HEADER: \usepackage{glossaries} 
#+LATEX_HEADER: \usepackage[intoc]{nomencl}
#+LATEX_HEADER: \usepackage{verse}
#+LATEX_HEADER: \newcommand{\attrib}[1]{\nopagebreak{\raggedcenter \footnotesize #1\par}}
#+LATEX_HEADER: \renewcommand{\poemtitlefont}{\raggedright\normalfont\large\bfseries\hspace{\leftmargin}}
#+LATEX_HEADER: \hypersetup{ colorlinks = true, urlcolor = cyan, linkcolor = blue, citecolor = red }
#+LATEX_HEADER: \usepackage{dsfont}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{tikz-uml}
#+LATEX_HEADER: \usetikzlibrary{arrows,shapes,automata,positioning}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{algpseudocode}
#+LATEX_HEADER: \usepackage[automark,headsepline]{scrlayer-scrpage}	
#+LATEX_HEADER: \clearpairofpagestyles
#+LATEX_HEADER: \lefoot[\pagemark]{\pagemark}
#+LATEX_HEADER: \rofoot[\pagemark]{\pagemark}
#+LATEX_HEADER: \lehead{\leftmark}
#+LATEX_HEADER: \rohead{\leftmark}

* Titlepage                                                          :ignore:
#+BEGIN_EXPORT latex
\begin{titlepage}
\pagestyle{empty}
\begin{center}
  
  \vspace{5cm}
  
  \huge{Parallel Simulation of SystemC Loosely-Timed Transaction Level Models}
  \vspace{5cm} 
  
  \Large Master of Science Thesis\\
  \vspace{2cm}
  
  \today
  \vspace{6cm}
  
  \begin{tabular}{ll} 
  \noindent Author: 	 		& Konstantinos Sotiropoulos \\
  \noindent Supervisor: 		& Björn Runåker (Intel Sweden AB) \\ 

  \noindent Examiner:  	 		& Prof. Ingo Sander (KTH)\\ 
  \noindent Academic advisor: 	        & PhD student George Ungureanu (KTH)
  \end{tabular}
  \vspace{2.5cm}

  \small
  \begin{tabular}{l}
  \textsc{KTH Royal Institute of Technology}\\
          School of Information and Communication Technology\\
          Department of Electronic Systems\\
	  Stockholm, Sweden
  \end{tabular}
  
\end{center} 
\end{titlepage}
\clearpage
#+END_EXPORT

* Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:
\pagestyle{empty}
\pagenumbering{roman}

\addcontentsline{toc}{section}{Abstract}
Parallelizing the development cycles of hardware and software is becoming the industry's norm for reducing electronic devices time to market.
In the absence of hardware, software development is based on a virtual platform; 
a fully functional software model of a system under development, able to execute unmodified code.

A Transaction Level Model, expressed with the SystemC TLM 2.0 language, is one of the many possible ways for constructing a virtual platform.
Under SystemC's simulation engine, hardware and software is being co-simulated.
However, the sequential nature of the reference implementation of the SystemC's simulation kernel, is a limiting factor.
Poor simulation performance often constraints the scope and depth of the design decisions that can be evaluated.

It is the main objective of this thesis' project to demonstrate the feasibility of parallelizing the co-simulation of hardware and software using Transaction Level Models, outside SystemC's reference simulation environment.
The major obstacle identified is the preservation of causal relations between simulation events.
The solution is obtained by using the process synchronization mechanism known as the Chandy/Misra/Bryantt algorithm.

To demonstrate our approach and evaluate under which conditions a speedup can be achieved, we use the model of a cache-coherent, symmetric multiprocessor executing a synthetic application. 
Two versions of the model are used for the comparison; the parallel version, based on the Message Passing Interface 3.0, which incorporates the synchronization algorithm and an equivalent sequential model based on SystemC TLM 2.0.
Our results indicate that by adjusting the parameters of the synthetic application, a certain threshold is reached, above which a significant speedup against the sequential SystemC simulation is observed.
Although performed manually, the transformation of a SystemC TLM 2.0 model into a parallel MPI application is deemed feasible.

*Keywords:* parallel discrete event simulation, conservative synchronization algorithms, transaction level models, SystemC TLM 2.0



\clearpage

* Acknowledgement
:PROPERTIES:
:UNNUMBERED: t
:END:
\pagestyle{empty}
\addcontentsline{toc}{section}{Acknowledgement}

My Master's Thesis project was sponsored by Intel Sweden AB and was supervised by KTH's ICT department.
Most of the work was carried out in Intel's offices in Kista, where I was kindly provided with all the necessary experimentation infrastructure.\\

Björn Runåker was the project's supervisor from the company's side.
I would like to thank you Björn, for placing your trust in me, for carrying out this challenging task.
Furthermore, I would also like to thank Magnus Karlsson for his valuable feedback.\\

Professor Ingo Sander and PhD student George Ungureanu were the examiner and academic advisor from the university's side. 
I blame you for my intellectual Odyssey in the vast ocean of mathematical abstractions.
I am now a sailor, on course for an Ithaka I may never reach.
And I am most grateful for this beautiful journey.
May our ForSyDe come true: the day when the conceptual wall between software and hardware collapses.
\textit{Let there be computation}.\\

Mother and father you shall be acknowledged, I owe my existence to you.
Maria, I want to express my gratitude for your tolerance and support.
Finally, Spandan, my comrade, you must always remember the price of intellect.
Social responsibility and chronic insomnia.

\vspace{1.0cm}

\noindent
Stockholm, \today

\textit{Konstantinos Sotiropoulos}
\clearpage

#+BEGIN_LATEX
\begin{verse}[\linewidth]
\itshape  As you set out for Ithaka \\
          hope the voyage is a long one, \\
          full of adventure, full of discovery. \\!

          But do not hurry the journey at all. \\
          Better if it lasts for years, \\
          so you are old by the time you reach the island, \\
          wealthy with all you have gained on the way, \\
          not expecting Ithaka to make you rich. \\!

          Ithaka gave you the marvelous journey. \\
          Without her you would not have set out. \\
          She has nothing left to give you now. \\!
 
          And if you find her poor, Ithaka won’t have fooled you. \\
          Wise as you will have become, so full of experience, \\
          you will have understood by then what these Ithakas mean. \\!
	  
	  \attrib{ Konstantinos Kavafis, Ithaka }
\end{verse}
\clearpage
#+END_LATEX

* Table of Contents                                                  :ignore:
#+TOC: headlines 3
\addcontentsline{toc}{section}{Contents}
\clearpage

* List of Acronyms and Abbreviations
:PROPERTIES:
:UNNUMBERED: t
:END:

#+ATTR_LATEX: :center nil
| *ASIC*:  | Application Specific Integrated Circuit |
| *DE*:    | Discrete Event                          |
| *DES*:   | Discrete Event Simulator/Simulation     |
| *DMI*:   | Direct Memory Interface                 |
| *ES*:    | Electronic System                       |
| *ESLD*:  | Electronic System-Level Design          |
| *FPGA*:  | Field Programmable Gate Array           |
| *FSM*    | Finite State Machine                    |
| *HDL*    | Hardware Description Language           |
| *HPC*:   | High Performance Computing              |
| *IC*     | Integrated Circuit                      |
| *IP*     | Intellectual Property                   |
| *MoC*:   | Model of Computation                    |
| *MPI*    | Message Passing Interface               |
| *MPSoC*: | Multiprocessor System on Chip           |
| *OoO*:   | Out-of-Order                            |
| *PDES*:  | Parallel Discrete Event Simulation      |
| *RISC*   | Recoding Infrastructure for SystemC     |
| *SLDL*:  | System-Level Design Language            |
| *SMP*:   | Symmetric Multiprocessing               |
| *SoC*:   | System on Chip                          |
| *SR*:    | Synchronous Reactive                    |
| *TLM*:   | Transaction Level Model(ing)            |
| *CMB*:   | Chandy/Misra/Bryant algorithm           |
\addcontentsline{toc}{section}{List of Acronyms and Abbreviations}
\clearpage

* List of Figures                                                    :ignore:
#+BEGIN_EXPORT latex
\listoffigures
\addcontentsline{toc}{section}{List of Figures}
\clearpage
#+END_EXPORT 

* Introduction
\pagenumbering{arabic}
\pagestyle{scrheadings}

Section [[Overview]], provides an insight to the pragmatics of the project; 
without disclosing any commercially sensitive information, the reader is exposed to the use case, which became the raison d'être of this project.
The problem definition is then presented in Section [[Problem Definition]].
Section [[Purpose]] attempts to provide a general answer to the cui bono question. For a specific answer, the reader is encouraged to jump to Section [[Reflections]].
Section [[Objectives]] and [[Delimitations]] clarify the software engineering extend; what artifacts need to be constructed, in order to address the problem statement.
Section [[Hypothesis]] presents the hypothesis; an optimistic assumption that motivated this work.
Section [[Research Methodology]] describes the research methodology followed.
A synopsis of this document can be found in [[Structure of this thesis]]

** Overview
This project follows the work of Björn Runåker[fn:bjorn] \cite{Runaker2015} on his effort to parallelize the simulation of the next generation (5G) of radio base stations.
Telecom radio base stations are indeed a very heterogeneous system.
To say the least, a virtual platform describing the system consists of a Network Processing Unit (NPU), Field Programmable Gate Array (FPGA) logic and a group of Digital Signal Processors (DSP).
For a more pictorial exposition of the situation the reader is encouraged to refer to the work of Björn.

The approach followed was defined as "coarse-grained";
parallelism is achieved through multiple instantiations of SystemC's simulation engine, one per major component.
However, a question is left open;
the feasibility and merits of a "fine-grained" treatment, where parallelism is achieved within a single instance of the simulation engine.

[fn:bjorn] Björn Runåker works as a Platform Application Engineer for Intel in Stockholm: https://www.linkedin.com/in/runaker

** Problem Definition
The analytic presentation of SystemC's simulation environment, 
presented in Section [[SystemC's Discrete Event Simulator]], 
yields a categorical verdict: if parallel simulation is to be achieved, 
a new simulation environment must be built, from the ground up.

** Purpose
An increasing amount of an Electronic System's (ES) expected use value is becoming software based.
Companies which neglect this fact face catastrophic results.
A well identified narrative, for example in \cite{Surowiecki2013}, 
is how Nokia was marginalized in the "smartphone" market, 
despite possessing the technological know-how for producing superior hardware.

If an ES company is to withstand the economical pressure a competitive market introduces, the need for performing software and hardware development in parallel is imperative.
Established ways of designing ESs, that delay software development until hardware is available, are therefore obsolete.
The de facto standard of dealing with this situation has become the development of virtual platforms.
It is obvious, that if a virtual platform is to be used for software development, it must be able to complete execution in the same order of magnitude as the actual hardware.
Poor simulation performance often constraints the scope and depth of the design decisions that can be evaluated.

** Objectives
The engineering extend of this thesis aims at producing the following artifacts:
+ An MPI realization of the Chandy Misra Bryantt process synchronization algorithm that would be the cornerstone of the proposed Parallel Discrete Event Simulator (PDES).
+ Case Study 1: An airtraffic simulation, as the first evaluation framework for the proposed PDES.
+ Case Study 2: Two versions of a Cache-coherent multiprocessor model: the first expressed in SystemC TLM 2.0 and the second being "manually compiled" from the first, in order to "fit" the proposed PDES.

** Hypothesis
The \textit{primum movens} of this project can be summarized as follows: there is a healthy amount of parallelism available in the simulation of Electronic Systems, especially in the context of virtual platforms, where hardware and software are co-simulated.
It all boils down to a simple question; how can the model of a parallel machine not be parallel itself?

** Delimitations
The following list demonstrates a number of artifacts that are not to be expected from this work, mainly due to their implementation complexity, given the limited time scope of a thesis project.
However, one must keep in mind that the term "implementation complexity" often conceals the more fundamental question of feasibility.

+ A modified version of the reference SystemC simulation kernel, capable of orchestrating a parallel simulation.
 
+ A compiler for translating SystemC TLM 2.0 models into parallel applications. In fact, the previous statement should be generalized, for the shake of brevity:
  this thesis will not produce any sort of tool or utility.

+ Any form of quantitative comparison between the proposed and existing attempts to parallelize SystemC TLM 2.0 simulations.

** Research Methodology
The presentation of the research methodology, adopted in this work, is influenced by Anne Håkansson's paper titled \textit{"Portal of Research Methods and Methodologies for Research Projects and Degree Projects"} \cite{Hakansson2013}.
This work presents a qualitative research on the field of Parallel Discrete Event Simulator development for Electronic Systems Simulation.
The novelty of the subject makes qualitative research a necessary step for establishing the relevant theories and experimentation procedures needed by more quantitative approaches.
The methodology applied is illustrated in Figure \ref{fig:methodology}.
A further explanation of the figure is imminent:

#+BEGIN_EXPORT latex
\begin{figure}[htpb]
\centering
\tikzstyle{block} = [draw, fill=white, rectangle, minimum height=3em, minimum width=6em]
\tikzstyle{sum} = [draw, fill=blue!20, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

% The block diagram code is probably more verbose than necessary
\begin{tikzpicture}[node distance=3cm]%auto,>=latex']
    % We start by placing the blocks
    \node [block, pin={[pinstyle]above:Philosophical Assumption}] (crit) {\small Criticalism};
    \node [block, pin={[pinstyle]below:Research Approach}, right of=crit] (conc) {\small Conceptual};
    \node [block, pin={[pinstyle]above:Research Strategy}, right of=conc, text width=2.5cm, align=center] (ind) {\small Induction on Case Studies};
    \node [block, pin={[pinstyle]below:Quality Assurance}, right of=ind] (tra) {\small Transferability};

    % Once the nodes are placed, connecting them is easy. 
    \draw [draw,->]  (crit) -- node {} (conc);
    \draw [->]       (conc) -- node {} (ind);
    \draw [->]       (ind) --  node {} (tra);

\end{tikzpicture}
\caption{Qualitative Research Methodology}
\label{fig:methodology}
\end{figure}
#+END_EXPORT


+ *Criticalism*: The reality of Parallel Discrete Event Simulator development is being historically determined by the evolution of computational hardware.
+ *Conceptual*: Simulator development has not been properly associated with their relevant theoretical understanding: the Discrete Event Model of Computation.
                Terms like process, time, concurrency, determinism and causality are inconsistently used and usually lack of a proper mathematical definition within a solid framework.
		The development of the proposed Parallel Discrete Event Simulator is steered by this conceptual exploration.
                The importance of formalizing concepts with mathematics before development can be seen in the book \textit{"From Mathematics to Generic Programming"} by Alexander Stepanov and Daniel Rose \cite{Stepanov2014}, 
+ *Coded Case studies*: The proposed Parallel Discrete Event Simulator is tested by the implementation of the two case studies.
+ *Inductive*: The hypothesis is tested against the successful implementation of the two case studies. 
+ *Transferability*: The verification of two case studies can only be the basis step of inductive inference.
                     There is still the induction step, that is hoped to be addressed by the proposition of a compiler, that will allow every Loosely-Timed Transaction Level Model to "fit" the proposed Parallel Discrete Event Simulator.

** Structure of this thesis
The chapter assumes familiarity with C++.
+ Chapter [[Background]] wishes to inform the reader about the theoretical constituents of this project. 
+ Chapter [[Out of Order PDES with MPI]] presents the process synchronization algorithm that will be applied in the proposed PDES.
+ Chapter [[Methodology]] is a synoptic presentation of the case studies constructed for the evaluation of the proposed PDES.
+ Chapter [[Analysis]] will perform the inductive step.
+ Chapter [[Conclusion and Future Work]] concludes and provides the necessary reflections.
\clearpage

* Background
Section [[Electronic System-Level Design]] presents the outermost context; that is the engineering discipline of *Electronic System-Level Design (ESLD)* and how SystemC TLM 2.0 fits into the whole picture.
Section [[The Discrete Event Model of Computation]] hopes to help the reader understand why *Electronic System-Level Design Language* (ESLDL) models can be executed.
In Section [[SystemC's Discrete Event Simulator]], SystemC's simulation engine is presented. This section is complemented by the code example found in Appendix \ref{AppendixA}.
Before proceeding, the reader is advised to abandon momentarily any preconceptions about design, system, model, computation, time, concurrency and causality.

** Electronic System-Level Design
Section [[The Design Process]] defines the fundamental concepts of design, system, model and simulation.
In Sections [[Electronic Systems Design]] to [[Transaction-Level Model]], using Gajski and Kuhn's Y-Chart, the concept of a Transaction-Level Model is determined, as an instance in the engineering practice of Electronic System-Level Design (ESLD).
Section [[SystemC and TLM]] a rudimentary look on SystemC's role in ESLD.


*** The Design Process
We define the process of *designing* as the engineering art of incarnating a desired functionality into a perceivable, thus concrete, artifact.
An engineering artifact is predominantly referred to as a *system*, 
to emphasize the fact that it can be viewed as a structured collection of components and that its behavior is a product of the interaction among its components.

Conceptually, designing implies a movement from abstract to concrete, fueled by the engineer's *design decisions*, incrementally adding implementation details.
This movement is also known as the *design flow* and can be facilitated by the creation of an arbitrary number of intermediate artifacts called models.
A *model* is thus an abstract representation of the final artifact in some form of a language.
The design flow can be now semi-formally defined as a process of model refinement, with the ultimate model being the final artifact itself.
We use the term semi-formal to describe the process of model refinement, because to the best of our knowledge, 
such model semantics and algebras that would establish formal transformation rules and equivalence relations are far from complete \cite{Gajski2009}.

A desired property of a model is executability that is its ability to demonstrate portions of the final artifact's desired functionality in a controlled environment.
An *executable model*, allows the engineer to form hypotheses, conduct experiments on the model and finally evaluate design decisions.
It is now evident that executable models can firmly associate the design process with the scientific method.
The execution of a model is also known as *simulation* \cite{Editor2014}.



*** Electronic Systems Design
An Electronic System (ES) provides a desired functionality, by manipulating the flow of electrons.
Electronic systems are omnipotent in every aspect of human activity; 
most devices are either electronic systems or have an embedded electronic system for their cybernisis.

The prominent way for visualizing the ES design/abstraction space is by means of the Y-Chart.
The concept was first presented in 1983 \cite{Gajski1983} and has been constantly evolving to capture and steer industry practices.
Figure \ref{fig:Y-Chart} presents the form of the Y-Chart found in \cite{Gajski2009}.

#+BEGIN_EXPORT latex
\begin{figure}[htpb]
  \centering
  \begin{tikzpicture}[>=stealth',join=bevel,font=\sffamily,auto,on grid,decoration={markings, mark=at position .5 with \arrow{>}}]

    \coordinate (behaviouralNode) at (135:4cm);
    \coordinate (structuralNode) at (45:4cm);
    \coordinate (physicalNode) at (270:4cm);
    \coordinate (originNode) at (0:0cm);

    \node [above=1em] at (behaviouralNode) {\textbf{Behavioural Domain}};
    \node [above=1em] at (structuralNode) {\textbf{Structural Domain}};
    \node [below=1em] at (physicalNode) {\textbf{Physical Domain}};

    \draw[-, very thick] (behaviouralNode.south) -- (0,0) node[left,pos=0]{System Requirements} node[left,pos=0.2]{} node[left,pos=0.4]{} node[left,pos=0.6]{} node[left,pos=0.8]{Transfer Functions};

    \draw[-, very thick] (structuralNode.south) -- (0,0) node[pos=0]{Model of Computation} node[pos=0.2]{} node[pos=0.4]{} node[pos=0.6]{} node[pos=0.8]{Transistors};

    \draw[-, very thick] (physicalNode.south) -- (0,0) node[right,pos=0]{Virtual Platform} node[right,pos=0.2]{} node[right,pos=0.4]{} node[right,pos=0.6]{} node[right,pos=0.8]{Transistor layout};

    \draw[fill] (barycentric cs:behaviouralNode=1.0,originNode=0) circle (2pt);
    \draw[fill] (barycentric cs:behaviouralNode=0.8,originNode=0.2) circle (2pt);
    \draw[fill] (barycentric cs:behaviouralNode=0.6,originNode=0.4) circle (2pt);
    \draw[fill] (barycentric cs:behaviouralNode=0.4,originNode=0.6) circle (2pt);
    \draw[fill] (barycentric cs:behaviouralNode=0.2,originNode=0.8) circle (2pt);

    \draw[fill] (barycentric cs:structuralNode=1.0,originNode=0) circle (2pt);
    \draw[fill] (barycentric cs:structuralNode=0.8,originNode=0.2) circle (2pt);
    \draw[fill] (barycentric cs:structuralNode=0.6,originNode=0.4) circle (2pt);
    \draw[fill] (barycentric cs:structuralNode=0.4,originNode=0.6) circle (2pt);
    \draw[fill] (barycentric cs:structuralNode=0.2,originNode=0.8) circle (2pt);

    \draw[fill] (barycentric cs:physicalNode=1.0,originNode=0) circle (2pt);
    \draw[fill] (barycentric cs:physicalNode=0.8,originNode=0.2) circle (2pt);
    \draw[fill] (barycentric cs:physicalNode=0.6,originNode=0.4) circle (2pt);
    \draw[fill] (barycentric cs:physicalNode=0.4,originNode=0.6) circle (2pt);
    \draw[fill] (barycentric cs:physicalNode=0.2,originNode=0.8) circle (2pt);

    \draw[black!50] (0,0) circle (4.0cm);
    \draw[black!50] (0,0) circle (3.2cm);
    \draw[black!50] (0,0) circle (2.4cm);
    \draw[black!50] (0,0) circle (1.6cm);
    \draw[black!50] (0,0) circle (0.8cm);

  \end{tikzpicture}
  \caption{Gajski-Kuhn \index{Gajski-Kuhn Y-chart}Y-chart} 
  \label{fig:Y-Chart}
\end{figure}
#+END_EXPORT

The Y-Chart quantizes the design space into four levels of abstraction; system, processor, logic and circuit, represented as the four concentric circles.
For each abstraction level, one can use different ways for describing the system: behavioral, structural and physical.
These are represented as the three axises, hence the name Y-Chart.
Models can now be identified as points in this design space.

A typical design flow for an Integrated Circuit (IC) begins with a high-level behavioral model capturing the system's specifications and proceeds non-monotonically to a lower level structural representation, expressed as a netlist of, still abstract, components.
From there, Electronic Design Automation (EDA) tools will pick up the the task of reducing the abstraction of a structural model by translating the netlist of abstract components to a netlist of standard cells.
The nature of the standard cells is determined by the IC's fabrication technology (FPGA, gate-array or standard-cell ASIC).
Physical dimensionality is added by place and route algorithms, part of an EDA framework, signifying the exit from the design space, represented in the Y-Chart by the the "lowest" point of the physical axis.

The adjective non-monotonic is used to describe the design flow, because as a movement in the abstraction space, it is iterative:
design \rightarrow test/verify \rightarrow redesign.
This cyclic nature of the design flow is implied by the errors the human factor introduces, under the lack of formal model transformation methodologies in the upper abstraction levels.
The term *synthesis* is also introduced to describe a variety of monotonic movements in the design space: from a behavioral to a less-equally abstract structural model, from a structural to a less-equally abstract physical model, or for movement to less abstract models on the same axis.
Synthesis is distinguished from the general case of the design flow, in order to disregard the testing and verification procedures.
Therefore, the term synthesis may indicate the presence, or the desire of having, an automated design flow.
Low-level synthesis is a reality modern EDA tools achieve, while high-level synthesis is still a utopia modern tools are converging to.





*** System-Level Design
To meet the increasing demand for functionality, ES complexity, as expressed by their heterogeneity and their size, is increasing.
Terms like Systems on Chip (SoC) and Multi Processor SoC (MPSoC), used for characterizing modern ES, indicate this trend.
With abstraction being the key mental ability for managing complexity, the initiation of the design flow has been pushed to higher abstraction levels.
In the Y-Chart the most abstract level, depicted as the outer circle, is the system level.
At this level the distinction between hardware and software is a mere design choice thus *co-simulation of hardware and software* is one of the main objectives.
Thereby the term *system-level design* is used to describe design activity at this level.



*** Transaction-Level Model
A *Transaction-Level Model* (TLM) can now be defined as the point in the Y-Chart where the physical axis meets the system abstraction level.
As mentioned in the previous unit, a TLM can be thought of as a *Virtual Platform* (VP), where an application can be mapped \cite{Rigo2011}.
Another way of perceiving the relationship between these three terms (TLM, VP and application) is to say the following:
An application "animates" the virtual platform by making its components communicate through transactions.
A TLM It is a fully functional software model of a complete system that facilitates *co-simulation of hardware and software*.

There are three pragmatic reasons that stimulate the development of a transaction level model.
At first, as already mentioned, software engineers must be equipped with a virtual platform they can use for *software development*, early on in the design flow, without needing to wait for the actual silicon to arrive.
Secondly, a TLM serves as a testbed for *architectural exploration* in order to tune the overall system architecture, with software in mind, prior to detailed design.
Finally, a TLM can be a reference model for hardware *functional verification*, that is, a golden model to which an RTL implementation can be compared.




*** SystemC and TLM
One fundamental question, for completing the presentation of ESLD, remains; How can models be expressed on the system level?
While maintaining the expressiveness of a Hardware Description Language (HDL), *SystemC* is meant to act as an *Electronic System Level Design Language* (ESLDL).
It is implemented as a C++ class library, thus its main concern is to provide the designer with executable rather than synthesizable models.
The language is maintained and promoted by Accellera (former Open SystemC Initiative OSCI) and has been standardized (IEEE 1666-2011 \cite{OpenSystemCInitiative2012}).
A major part of SystemC is the TLM 2.0 library, which is exactly meant for expressing TLMs.
Despite introducing different language constructs, TLM 2.0 is still a part of SystemC because it depends on the same simulation engine.
TLM 2.0 has been standardized separately in \cite{OpenSystemCInitiative2009}.
\clearpage

** The Discrete Event Model of Computation
With Section [[Models of Computation]] the reader will be able to understand why a linguistic artifact, such as a model, can be "animated".
In Sections [[Discrete Event Model of Computation]] we present the *Discrete Event Model of Computation* (DE MoC).
As with any MoC, the section presents what constitutes a component and what actions the component can perform.
Sections [[Causality and Concurrency]] and [[Time and Determinism]] define the concepts of causality, concurrency, time and determinism in the theoretical framework developed in the previous section.

*** Models of Computation
A *language* is a set of symbols, rules for combining them (its syntax), and rules for interpreting combinations of symbols (its semantics). 
The process of resolving the semantics of a linguistic artifact is called *computation*.
Two approaches to semantics have evolved: denotational and operational.
*Operational semantics*, which dates back to Turing machines, give the meaning of a language in terms of actions taken by some abstract machine. 
The word "machine" indicates a system that can be set in "motion" through "space" and time.

With operational semantics it is implied that a language can not determine computation by itself \cite{Jantsch2005}. 
Computation is an epiphenomenon of the "motion" of the underlying abstract machine, just like time indication in a mechanical watch is a byproduct of gear motion.
Consider the language of regular expressions.
A linguistic artifact in this language describes a pattern that is either matched or not by a string of symbols.
A Finite State Machine (FSM) is the underlying abstract machine.
Computation is a byproduct of the FSM changing states; was the final state an accepting state or not.
The rules that describe an abstract machine constitute a *Model of Computation (MoC)* \cite{Edwards1997}.

All of the above painstaking narrative has been formed to reach the following conclusion: 
The dominant MoC related to an ESLDL is called the *Discrete Event (DE)* MoC, and it is the presence of the DE MoC that makes an ESLDL model executable.

*** Discrete Event Model of Computation
First things first: why is this MoC called discrete?
The system is mathematically represented as a set of variables $\mathds{V}$.
The system's *state* is a mapping from $\mathds{V}$ to a value domain $\mathds{U}$.
The system changes states in a *discrete* fashion; 
the set $\mathds{A}$ of all possible system states can be enumerated by natural numbers ($|\mathds{A}| = \aleph_0$).

Now let us proceed to the event part.
The components of a DE MoC are called *processes*.
The set of processes is denoted by $\mathbb{P}$.
Processes introduce a spatial decomposition of a system; the set of processes define a partition on $\mathds{V}$.
A process can now be defined as a set of *events* $P_i \subseteq \mathds{E}$ where $i\in\mathbb{N}$.
An event denotes a system state change; from the system's perspective, it can be regarded as a mapping $\mathds{A} \rightarrow \mathds{A}$.
$\mathds{E}$ is a universal set on which processes $P_i$ define a partition.
The above description can be crystallized in the following axiom:

#+BEGIN_EXPORT latex
\begin{equation}
\tag{Axiom 1}
(e_k \in P_i \land e_l \in P_j) \implies (v(e_k) \cap v(e_l) = \emptyset)
\end{equation}
#+END_EXPORT
where $v$ denotes the set of variables that change values, between the system state change induced by an event.

$\mathds{E}$ is a partially ordered set under the relationship *"happens before"*, denoted by the symbol $\sqsubset$ \cite{Lamport1978}.
The binary relationship $\sqsubset$, apart from being antisymmetric and transitive, is irreflexive; 
an event can not "happen before" itself.

On a process two actions are performed: communication and execution.
Both of these can be defined as functions $\mathds{E} \rightarrow \mathds{E}$.
*Execution* $f: P_i \rightarrow P_i$ is the processing of events (hence the name process to describe the entity that performs this action).
In simpler terms, execution "consumes" an event, changes the system's state and thus "produces" an event.
*Communication* $g: P_i \rightarrow P_j$ is the exchange of events.
In simpler terms, communication maps an event from one process to an event in another process.

One final remark about Axiom 1 now that the terms communication and execution have been defined.
Axiom 1 leads to the conclusion that a DE MoC directly incorporates the software engineering principle of \textit{"Separation of concerns between execution and communication"}.
In the absence of shared variables, processes can only interact "explicitly", through their communication functions.
From a theoretical standpoint, demanding this separation of concerns, yields simpler reasoning about the behavior of a system.
However, one would argue that this is a distortion of reality; in modern multiprocessors communication is implicitly performed through shared memory.
Given our critical approach on reality, we therefore encourage the reader to question this trend.
For example, in XMOS' XS1 architecture \cite{May2009}, the separation of concerns has been directly realized in hardware.

*** Causality and Concurrency
The relationship *"causally affects"*, denoted by the symbol $\propto$, is introduced as an irreflexive, antisymmetric and transitive binary relationship on the set $\mathds{E}$.
*Causality*, as a philosophical assumption about the behaviour of a system, can now be mathematically captured by the following three axioms:
#+BEGIN_EXPORT latex
\begin{equation}
\tag{Axiom 2}
e_1 \propto e_2 \implies e_1 \sqsubset e_2
\end{equation}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{equation}
\tag{Axiom 3}
e = f(e) \implies e \propto f(e) \implies e \sqsubset f(e) 
\end{equation}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{equation}
\tag{Axiom 4}
e = g(e) \implies e \propto g(e) \implies e \sqsubset g(e)
\end{equation}
#+END_EXPORT

Axiom 3 also implies the the sets $P_i$ are totally ordered under both $\sqsubset$ and $\propto$.
Two events $e_1,e_2 \in \mathds{E}$ are *concurrent* if neither $e_1 \sqsubset e_2$ nor $e_2 \sqsubset e_1$ holds.
It follows, that concurrent events are not causally related.

#+BEGIN_EXPORT latex
\begin{figure}[htpb]
\centering
\begin{tikzpicture}[
arrow/.style={draw,->,>=stealth},
point/.style={circle,fill=black},
every node/.style={node distance = 10},
]

\node (p1) at (0,2) {$p_1$};
\node (p2) at (0,1) {$p_2$};
\node (p3) at (0,0) {$p_3$};

\node (p1l) at (0.2,2) {};
\node (p2l) at (0.2,1) {};
\node (p3l) at (0.2,0) {};

\node (p1r) at (8,2) {};
\node (p2r) at (8,1) {};
\node (p3r) at (8,0) {};

\path[draw] (p1l.center) edge (p3l.center);

\draw[arrow] (p1l.center) to (p1r);
\path[arrow] (p2l.center) to (p2r);
\path[arrow] (p3l.center) to (p3r);

\node[point] (a) at (1,2) {};
\node [below of = a] {a};
\node[point] (b) at (2.5,2) {};
\node [below of = b] {b};
\node[point] (c) at (1,1) {};
\node [below of = c] {c};
\node[point] (d) at (5.5,1) {};
\node [below of = d] {d};
\node[point] (e) at (1.75,0) {};
\node [below of = e] {e};
\node[point] (f) at (7,0) {};
\node [below of = f] {f};

\path[arrow] (b) edge node [right] {} (c);
\path[arrow] (d) edge node [right] {} (f);
\end{tikzpicture}
\caption{DE spacetime decomposition} 
\label{fig:DE}
\end{figure}
#+END_EXPORT

Figure \ref{fig:DE} provides a visual understanding of a DE system, as a spaceXtime diagram.
A discrete perception of space is obtained by process decomposition (y-axis), while the perception of time (x-axis) is obtained by process actions.
The horizontal arrows indicate process execution, while non-horizontal arrows indicate process communication.
Events are represented as points in this plane.
The execution and communication properties are denoted by placing the input event on the start of the arrow and the output event at its tip [fn:223].

To move forward in time, one must follow a *chain* of ordered, under the $\sqsubset$ relationship, events.
One such chain is the sequence $a,b,c,d,f$.
Event $a$ *may* causally affect $f$.
Events $d,e$ are concurrent: there is no chain that contains both.
Event $d$ cannot causally affect $e$ and vice versa.
The time axis is not resolved; a time modeling technique for relating an event with a number, its timestamp, has not yet been defined. 
That is why the placement of events on the plane, for example events $d,e$ is quite arbitrary, non-unique and maybe counter intuitive.

[fn:223] For execution, the reader has to imagine the presence of many intermediate arrows, between two subsequent events on the same horizontal arrow. The start is at the left event and the tip at the right.

*** Time and Determinism
A realization of the DE abstract machine is called a *Discrete Event Simulator (DES)*.
When implementing a DES, one needs to differentiate between two notions of time: Simulated/logic time and real/wallclock time.
*Real/Wallclock time* refers to the notion of time existing in the simulator's environment; for example a x86 Time Stamp Counter (TSC) measuring the number of cycles since reset.
*Logic/real time* is defined as a the notion of time in the DES; a *logic time modeling* technique associates an event with a value, which is called its *timestamp*.
Since $\mathds{E}$ is partially ordered and only the sets $P_i$ are totally ordered, one is forced to reach the conclusion that the nature of the DE MoC instigates a *relativistic notion of logic time*.
Logic time may be different across processes, at any moment in real time, and it is only through communication that a global perception of logic time can be formulated.

Logic time modeling is deferred to the implementation of the DE abstract machine and is highly depended on the nature of the underlying hardware. 
Is it *parallel*, where the spatial decomposition defined in the DE can be preserved? 
Or is it *sequential*, where the space dimensionality must be emulated.
The only restrictions DE semantics impose on a logic time modeling technique $C$ are:

#+BEGIN_EXPORT latex
\begin{equation}
\tag{Axiom 5}
       e_1 \sqsubset e_2 \implies C(e_1) < C(e_2) 
\end{equation}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{equation}
\tag{Axiom 6}
       |Range(C)| \geq \aleph_0
\end{equation}
#+END_EXPORT

If a DES can infer a total ordering of $\mathds{E}$, through a logic time modeling technique, then the simulation is said to be *deterministic*.
A total ordering of $\mathds{E}$ also infers a total ordering of the set $\mathds{S}$: the system states encountered during simulation ($\mathds{S} \subseteq \mathds{A}$).
Determinism is a very important reasoning facility, engineers seek from the simulation of the systems they construct, in order to provide any formal statement about the system's behavior.
Physicists, especially those engaged with quantum mechanics, are more tolerant to non-determinism.

\clearpage

** SystemC's Discrete Event Simulator
The easiest way to realize the DE MoC concept of a process, in SystemC, is through an \texttt{SC\_MODULE} equipped with a *single* "thread" (\texttt{SC\_THREAD}, \texttt{SC\_METHOD} or \texttt{SC\_CTHREAD}). 
The encapsulation of a "thread" within an \texttt{SC\_MODULE} is a necessary, but not sufficient, condition for achieving spatial decomposition.
The designer can still abuse the fact that SystemC is embedded on C++.
Quoting Bjarne Stroustrup: \textit{"C makes it easy to shoot yourself in the foot; C++ makes it harder, but when you do it blows your whole leg off"}.

Section [[Coroutines]] presents the fundamental mechanism behind SystemC's DES: coroutines.
With this section, the reader will also understand why the previously mentioned term "threads" was quoted.
Sections [[The kernel]] to [[Event Notification and Process Yielding]] give an analytic description of the actions performed in SystemC's simulation environment.
An algorithmic description of the simulator's main event loop can be found in Section [[SystemC's Main Event Loop]].
The Section is complemented by the code examples found in Appendices \ref{AppendixA} and \ref{AppendixB}.

[fn:bjarne] Verified in: http://www.stroustrup.com/bs_faq.html#really-say-that

*** Coroutines
SystemC's distribution comes with a sequential realization of the DE MoC, referred to as the reference *SystemC simulation engine* \cite{OpenSystemCInitiative2012}.
It is a sequential implementation because the spatial decomposition of the system is emulated through *coroutines* (also known as co-operative multitasking). 
Co-routines in SystemC have been counterintuively named as \texttt{SC\_METHOD}, \texttt{SC\_THREAD} or \texttt{SC\_CTHREAD}.
A coroutine is neither a function nor a thread.

Processes, realized as coroutines[fn:pthread], perform their actions (computation, communication), henceforth *run*, without interruption.
At any moment in real time only a single process can be running.
No other process can run until the running process has voluntarily *yielded*.
Furthermore, a non-running process can not preempt or interrupt the running process.

A process can be declared sensitive to a number of events (static sensitivity).
Moreover, a process can declare itself sensitive to events (dynamic sensitivity).
All of the events the process is sensitive to, form its *sensitivity list*.
A yielded process is awaiting for events in its sensitivity list to to be triggered.

Before yielding, a process saves its context and registers its identity in a global structure of coroutine handlers called the *waiting list*.
Along comes the question: to whom does a yielding process pass the baton of control flow?

[fn:pthread] The exact library that realizes co-routines in C++ is determined during the compilation of the SystemC distribution. 
             In GNU/Linux, SystemC version 2.3.1 supports QuickThreads and Posix Threads.
	     However, it is highly probable that future revisions of the C++ standard will include *resumable functions*, a concept semantically equivalent to coroutines.



*** The kernel
The *kernel* is the simulation's director \cite{Editor2014}, the maestro of a well orchestrated simulation music.
Processes yield to the kernel, a coroutine himself.
In the presence of an ill-behaved never yielding process, the kernel is powerless [fn:kernel].

The kernel is responsible for many things[fn:forward]:
1. If there are no events in the *global event queue* and the list of runnable processes is empty, it must *terminate* the simulation.
2. It sorts the global event queue according to timestamp order.
3. It possesses a global perspective over logic time:
   *global time* advances according to the timestamp of the event (from the global event queue) last triggered.
4. When the list of runnable processes has been depleted, it is his duty to trigger the next, according to timestamp order, event.
   It first checks whether there are events in the *delta notification queue*. 
   Triggering these events do not advance global time.
   It then checks the global event queue.
5. When *triggering* an event, it must identify which processes can be moved from the waiting to the runnable list. 
   The decision is based on a process' sensitivity list.
6. It is responsible for *context switching* between the running and a runnable process. 
   The selection of the running process from the list of runnable processes is implementation-defined.
   An example of such a situation can be found in Appendix \ref{AppendixB}.

A spectre is haunting the previous description of the kernel: how is logic time modeled?

[fn:kernel] This is exactly the most important problem faced by early operating systems (16-bit era). 
            Their cooperative nature could not discipline poorly designed applications.
[fn:forward] Please note that many terms are forward-declared and defined either further down in the description or in upcoming sections.


*** Modeling Time
Logic time can be represented as a vector [fn:dense] $\in \mathbb{N}^n$ where $n \in \mathbb{N}}$.
This time modeling technique is referred to as *superdense time* \cite{Editor2014}. 
Every event is associated with a vector; in other words, every event has a timestamp.
Ordering of events comes as a lexicographical comparison between timestamps.

SystemC explicitly defines logic time as a vector $(t,n)$.
Although, as demonstrated in Appendix \ref{AppendixB}, there is an implied third dimension.

The first co-ordinate of a logic time vector is meant for modeling real time.
*Modeled real time values* are used as timing annotations the designer injects into the system in order to describe the duration of communication and execution in the physical system.
The choice of using the term "superdense" for this logic time modeling technique can now be understood: 
between any two events $e_1, e_2$, with modeled real time values $t_1, t_2$, $\exists e_3$, such that $timestamp(e_1) < timestamp(e_3) < timestamp(t_2)$.
Two events $e_1, e_2$ associated with the timestamps $(t_1,n_1), (t_2, n_2)$ are said to be *simultaneous* if $t_1 = t_2$.
If both $t_1 = t_2$ and $n_1 = n_2$ they are *strongly simultaneous*.

To avoid quantization errors and the non-uniform distribution of floating point values, SystemC internally represented logic time as an integral multiple of an SI unit referred to as the time resolution.
The integral multiplier is limited by the underlying machine's capabilities: in a 64-bit architecture its maximum value is $2^{64}-1$.
The minimum time resolution SystemC can provide is that of a femtosecond ($10^{-15}$ seconds).

To assist in the construction of modeled real time values, SystemC provides the class \texttt{sc\_time}.
\texttt{sc\_time}'s constructor takes two arguments: (\texttt{double}, \texttt{SC\_TIME}) [fn:unit].
The designer needs to be very careful when providing timing annotations: modeled real time is internally represented as an integral value, despite \texttt{sc\_time}'s constructor having a floating point argument.
The mistake of using a value of \texttt{sc\_time(0.5, SC\_FS)} can only be detected during *run-time*.
The same applies for a value of \texttt{sc\_time(1, SC\_SEC)} with a time resolution of 1 \texttt{SC\_FS}.



[fn:dense] This terminology is not consistent across literature, for example the term *dense* \cite{Furia2010} may also imply that logic time $\in \mathbb{R}$ or $\mathbb{Q}$.
           By Cantor's \textit{"diagonal count"}, $|\mathbb{N}\times...\times\mathbb{N}| = \aleph_0 < |R|$.
           The terms *superdense* and *dense* in this case are semantically different.
	   
[fn:unit] \texttt{SC\_TIME} is an enumeration: \texttt{SC\_SEC} for a second, \texttt{SC\_MS} for a millisecond etc.

*** Event Notification and Process Yielding
Events in SystemC are realized as instances of the class \texttt{sc\_event}.
Processes perform event notifications, by calling either of these variations of the \texttt{sc\_event.notify} method:
+ \texttt{notify(sc\_time t)}:     (Scheduled occurrence) The process adds the event to the global event queue. All sensitive processes will become runnable when the kernel triggers the event.
+ \texttt{notify()}:               (Immediate notify)    The process signals a flag within the kernel. All sensitive processes in the waiting list are moved to the runnable list, at the next context switch.
+ \texttt{notify(SC\_ZERO\_TIME)}: (Delayed occurrence)   The process adds the event to delta notification queue. All sensitive processes in the waiting list are moved to the runnable list, after the runnable list becomes empty.


Yielding is explicitly stated by a calling a variant of the \texttt{sc\_module.wait} method. The most important are:
+ \texttt{wait()}:            The process remains in the waiting list, until events in its sensitivity list are triggered.
+ \texttt{wait(sc\_time t)}   Before yielding, the process adds a newly created event in the global event queue, with timestamp = \texttt{t + global\_time}. It also becomes sensitive to this event.
+ \texttt{wait(sc\_event e)}  Before yielding, the process modifies its sensitivity list, so as to include \texttt{e}

*** SystemC's Main Event Loop
What follows is an algorithmic description of SystemC's main event loop.
#+BEGIN_EXPORT latex
\begin{algorithm}
\caption{SystemC's event loop (kernel's perspective)}
\label{alg:kernel}
\begin{algorithmic}[1]

   \While{scheduled events exist}          \Comment{Global clock progression loop}
      \State order events in global event queue
      \State trigger the event with the smallest timestamp
      \State advance global time
      \State make all sensitive processes runnable
      \While {runnable processes exist}    \Comment{Delta cycle progression loop}
          \While {runnable processes exist}\Comment{Immediate notifications loop}
	     \State run a process
             \State trigger all immediate notifications
             \State make all sensitive processes runnable
         \EndWhile
	 \State trigger all delta notifications
         \State make all sensitive processes runnable
       \EndWhile
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_EXPORT



\clearpage

** Parallel Discrete Event Simulation
The previous section has made evident that the reference implementation of the SystemC DES is sequential and therefore can not utilize modern massively parallel host platforms. 
The most logical step in achieving faster simulations is to *realize and not emulate the DE MoC's spatial decomposition*.
By assigning each process to a different processing unit of a host platform (core or hardware thread) we enter the domain of *Parallel Discrete Event Simulation (PDES)*.

In Section [[Prior Art]] we give an overview of prior art in the field of PDES in SystemC.
Section [[Causality and Synchronization]] indicates under which conditions a PDES may break forward logic time movement and thus produce a *causality hazard*.

*** Prior Art
After making the strategical decision that for improving DES performance one must orchestrate parallel execution, the first tactical decision encountered is whether to keep a single simulated time perspective, or distribute it among processes.
For PDES implementations that enforce a global simulation perspective, the term *Synchronous PDES* has been coined \cite{Schumacher2010} \cite{Moy}.
In Synchronous PDES, parallel execution of processes is performed within a delta cycle. 
With respect to Alg \ref{alg:kernel}, a Synchronous PDES parallelizes the execution of the innermost loop (line 4).
However, as we will see in the next section, this approach will bare no fruits in the simulation of TLM Loosely Timed simulations, since delta cycles are never triggered \cite{Chen2012}.

Therefore, our interest is shifted towards *Out-of-Order PDES (OoO PDES)* \cite{Chen2015};  where each process has its own perception of simulated time, determined by the last event it received.
The most important project in OoO PDES for SystemC is \textit{RISC: Recoding infrastructure for SystemC} \cite{Liu2015}.
The project is ongoing [fn:version], and it is being carried out at the Center for Embedded and Cyber-physical Systems at the University of California, Irvine.
However, TLM 2.0 as a subset of SystemC, is not (yet) supported (Section 4.3 in \cite{Liu2015}).
The reason behind this absence can be found in Section [[Criticism]].
It is this lack of a SystemC TLM 2.0 compatible OoO PDES framework that justifies any novel approach on the matter.

[fn:version] When this thesis' literature study was being carried out, the project was at version V0.2.1. 

             

*** Causality Hazards 
The distribution of simulation time opens up Pandora's box.
Protecting an OoO PDES from *causality hazards* requires:
1. The partition of the system's state variables amongst processes.
2. The deployment of a process synchronization mechanism.

Consider Figure \ref{fig:hazard}.
Events $a,c$ are concurrent, since there can be no chain that contains both.
Neither $a \sqsubset c$ nor $c \sqsubset a$.
Therefore, in a PDES, they could be executed in parallel.
As a result, there is the possibility that event $f$ will occur before event $e$ in *real time*.
The need for *blocking* process $p_2$ until both events $e,f$ occur in real time, becomes evident.
In other words, the fundamental problem in an OoO PDES, can be understood as the following question: how can a process deduce that it is safe to advance its perception of time?
The answer to this question lies in *process synchronization*.
Process synchronization can be understood as a mechanism for blocking a process, until it gathers all the necessary information, about the perception of time its peer processes have.

#+BEGIN_EXPORT latex
\begin{figure}[htpb]
\centering
\begin{tikzpicture}[
arrow/.style={draw,->,>=stealth},
point/.style={circle,fill=black},
every node/.style={node distance = 10},
]

\node (p1) at (0,2) {$p_1$};
\node (p2) at (0,1) {$p_2$};
\node (p3) at (0,0) {$p_3$};

\node (p1l) at (0.2,2) {};
\node (p2l) at (0.2,1) {};
\node (p3l) at (0.2,0) {};

\node (p1r) at (8,2) {};
\node (p2r) at (8,1) {};
\node (p3r) at (8,0) {};

\path[draw] (p1l.center) edge (p3l.center);

\draw[arrow] (p1l.center) to (p1r);
\path[arrow] (p2l.center) to (p2r);
\path[arrow] (p3l.center) to (p3r);

\node[point] (a) at (1,2) {};
\node [below of = a] {a};
\node[point] (b) at (3.5,2) {};
\node [below of = b] {b};

\node[point] (e) at (3,1) {};
\node [below of = e] {e};
\node[point] (f) at (4.5,1) {};
\node [below of = f] {f};

\node[point] (c) at (1,0) {};
\node [below of = c] {c};
\node[point] (d) at (1.75,0) {};
\node [below of = d] {d};

\path[arrow] (b) edge node [right] {} (f);
\path[arrow] (d) edge node [right] {} (e);
\end{tikzpicture}
\caption{Causality Hazard in PDES} 
\label{fig:hazard}
\end{figure}
#+END_EXPORT

Synchronization mechanisms, with respect to how they deal with causality hazards, can be classified into two categories: *conservative* and *optimistic* \cite{Fujimoto2015}.
Conservative mechanisms strictly avoid the possibility of any causality hazard ever occurring by means of model introspection and process synchronization.
On the other hand, optimistic/speculative approaches use a detection and recovery approach: when *causality errors* are detected a rollback mechanism is invoked to restore the system in its prior state.
An optimistic compared to a conservative approach will theoretically yield better performance in models where communication, thus the probability of causality errors, is below a certain threshold \cite{Fujimoto1990}.

Both groups present severe implementation difficulties.
For conservative algorithms, model introspection and static analysis tools might be very difficult to develop,
while the rollback mechanism of an optimistic algorithm may require complex entities, such as a hardware/software transactional memory \cite{Anane2015} .

\clearpage

** SystemC TLM 2.0                           
At the time of writing and to the best of our knowledge, we can not verify the existence of a comprehensive guide[fn:groundup] about system level modeling with SystemC TLM 2.0.
A common practice among engineers, who want to learn system-level modeling with SystemC TLM 2.0, is to attend courses offered by training companies [fn:doulos].
Hence, there is an obligation to provide a quick introduction into the subject, and in particular to the SystemC TLM 2.0 Loosely-Timed (LT) coding style.

Section [[The Role of SystemC TLM 2.0]] presents the typical use case of TLM[fn:tlm2].
Section [[Criticism]] presents the dominant source of criticism for TLM.
In Sections [[TLM 2.0 Terminology]] and [[Generic Payload]] TLM's basic jargon is presented: transactions, initiator/interconnect/target components, sockets and the generic payload.
In Section [[Coding Styles and Transport Interfaces]] the Loosely-Timed coding style is defined.
The chapter is complemented by Appendix \ref{AppendixE}, where the reader can find a simple Loosely-Timed model.

[fn:groundup] From the preface of the second edition of \textit{"SystemC: From the Ground Up"} \cite{Black2010}, we quote: 
              \textit{"Those of you who follow the industry will note that this is not TLM 2.0. This new standard was still emerging during the writing of this edition. But not to worry! Purchasers of this edition can download an additional chapter on TLM 2.0 when it becomes available within the next six months at www.scftgu.com"}.
	      The additional chapter has not yet been produced...
[fn:doulos] For example Cadence and Doulos.
[fn:tlm2] From now on when the term TLM is mentioned, it strictly refers to SystemC TLM 2.0. 
          Earlier versions of TLM will not be examined. 

*** The Role of SystemC TLM 2.0
As stated in unit [[Electronic System-Level Design]], a Transaction Level Model is considered a virtual platform where a software application can be mapped.
TLM enhances SystemC's expressiveness in order to facilitate the *modular description* and *fast simulation* of virtual platforms.
TLM as a language, unlike C/C++, VHDL or pure SystemC, is not meant for describing individual functional/architectural/system blocks/modules/components (henceforth *Intellectual Property (IP)*).
Its role is to make these individual IP blocks communicate with each other, as demonstrated in Figure [[fig:tlm_as_wrapper]].

#+CAPTION: TLM 2.0 as a mixed language simulation technology
#+NAME: fig:tlm_as_wrapper
[[file:Figures/mixedSimulation.pdf]]

Modularity or else IP block *interoperability*, is TLM's niche.
It enables the reuse of IP components in a "plug and play" fashion.
Having a library of verified IP blocks at his disposal, the engineer is able to create new virtual platforms fast and "effortlessly".
TLM is relevant at every interface where an IP block needs to be plugged into a bus.
TLM was designed with *memory-mapped* communication in mind.

To be suitable for productive software development, a virtual platform needs to be fast: it must be able to boot operating systems in seconds.
It also needs to be accurate enough such that, code developed using standard tools on the virtual platform, will run unmodified on real hardware \cite{Leupers2010}. 
Compared to a standard RTL simulation, a TLM achieves a significant speed up by replacing communication through pin-level events with a single function call.
The logic is quite simple: less events \rightarrow less context switches.
This is exactly what makes simulations faster, but at the same time being TLM's major source of criticism.

*** Criticism
System level designers consider TLM 2.0 a step towards the wrong direction \cite{Liu2015}.
The root problem with TLM lies in the elimination of explicit channels, which were a key contribution in the early days of research on system-level design \cite{Liu2015}.
Communication in TLM looks like a remote function call\cite{Ecker2009}: a process, encapsulated in a module, executes a method of another module, in its own context.
The term *transaction* in TLM indicates exactly this remote function call, while the term *payload* indicates its most important argument.

Time for contemplation.
First and foremost, the principle of "Separation of concerns between execution and communication" has been scrapped; execution obfuscates communication.
The RISC project (see Section [[Prior Art]]) has not (yet) supported the TLM API for this exact reason.
The need for *recoding* SystemC TLM 2.0 models, in order to allow parallel execution, has manifested. 
Recoding must reconstitute the separation of concerns between computation and communication.
However, due to its simplicity, TLM could still serve as a front end language.
Furthermore, due to the overhead parallelism may add to a simulation, it would be useful to keep a sequential option for models not "sufficiently large".

*** TLM 2.0 Terminology
TLM 2.0 classifies IP blocks as initiator, target and interconnect component.
The terms initiator and target come forth as a replacement for the anachronistic terms master and slave.

An *initiator* is a component that initiates new transactions.
It is the initiator's duty to allocate memory for the payload.
Payloads are always passed by reference.

A *target* component acts as the end point of a transaction. 
As such, it is responsible for providing a response to the initiator.
Request and response are combined into a payload.
Thus, the target responds by modifying certain fields in the payload.

An *interconnect* component is responsible for routing a transaction on its way from initiator to target.
The route of a transaction is not predefined.
Routing is dynamic; it depends on the attributes of the payload, mainly its address field.
There is no limitation on the number of interconnect components participating in a transaction. 
An initiator can also be directly connected to a target.
Since an interconnect can be connected to multiple initiators and targets, it must be able to perform *arbitration* in case transactions "collide".

The role of a component is not statically defined and it is not limited to one.
It is determined on a transactions basis. 
For example, it may function as an interconnect component for some transactions, and as a target for other transactions.

Transactions are sent through initiator *sockets*, and received through target sockets.
Initiator sockets are used to forward method calls "up and out of" a component, while target sockets are used to allow method calls "down and into" a component.
It goes without saying that an initiator must have at least one initiator socket, a target at least one target socket and a interconnect must possess both.

All the above terms are illustrated in Figure [[fig:tlm_terminology]].
Each initiator-to-target socket connection supports both a forward and a backward path by which interface methods can be called in either direction.

#+CAPTION: A basic TLM system
#+NAME: fig:tlm_terminology
#+RESULTS:
[[file:Figures/TLMterminology.pdf]]

*** Generic Payload
The basic argument that is passed, by reference, in communicative method calls is called the *payload*.
The choice of \texttt{tlm\_generic\_payload} as the type of the payload is a necessary condition for enabling interoperability between IP blocks from different vendors.
\texttt{tlm\_generic\_payload} is a *structure* that encapsulates generic attributes relevant to a generic memory mapped bus communication.

The structure possesses an extensions mechanism, the designer can use to define more specific memory mapped bus architectures (e.g. ARM's AMBA).
An *interoperable* TLM 2.0 component must depend only on the generic attributes of the generic payload.
The presence of attributes through the extension mechanism can be ignored without breaking the functionality of the model.
In such a case, the extensions mechanism carries simulation metadata like pointers to module internal data structures or timestamps.

The following table lists all fields applicable on a \texttt{tlm\_generic\_payload}:

| Attribute           | Type                                  | Modifiable        |
|---------------------+---------------------------------------+-------------------|
| Command             | \texttt{tlm\_command} (enum)          | Initiator only    |
| Address             | \texttt{uint64}                       | Interconnect only |
| Data pointer        | \texttt{unsigned char*}               | Initiator only    |
| Data length         | \texttt{unsigned int}                 | Initiator only    |
| Byte enable pointer | \texttt{unsigned char*}               | Initiator only    |
| Byte enable length  | \texttt{unsigned int}                 | Initiator only    |
| Streaming width     | \texttt{unsigned int}                 | Initiator only    |
| DMI hint            | \texttt{bool}                         | Yes               |
| Response status     | \texttt{tlm\_response\_status} (enum) | Target only       |
| Extensions          | \texttt{(tlm\_extension\_base*)[]}    | Yes               |

+ *Command:* Set to either \texttt{TLM\_READ} for read, \texttt{TLM\_WRITE} for write or \texttt{TLM\_IGNORE} to indicate that the command is set in the extensions mechanism.
+ *Address:* Can be modified by interconnects since by definition an interconnect must bridge different address spaces.
+ *Data pointer:* A pointer to the actual data being transferred.                                                                                                                                                                                                                                                                                                                                      
+ *Data length:* Related to the data pointer, indicates the number of bytes that are being transfer-ed.
+ *Byte enable pointer:* A pointer to a byte enable mask that can be applied on the data (0xFF for data byte enabled, 0X00 for disabled).
+ *Byte enable length:* Only relevant when the byte enable pointer is not null. If this number is less than the data length, the byte enable mask is applied repeatedly.
+ *Streaming width:* Must be greater than 0. If the data length $\neq$ streaming width, then a streaming transaction is implied. Largest address defined by the transaction is (address + streaming width - 1), at which point the address wraps around. 
+ *DMI hint:* A hint given to the initiator of whether he can bypass the transport interface and access a target's memory directly through a pointer.
+ *Response status:* The initiator must set it to \texttt{TLM\_INCOMPLETE\_RESPONSE} prior to initiating the transaction. The target will set it to an appropriate value indicating the outcome of the transaction. For example for a successful transaction the value is \texttt{TLM\_OK\_RESPONSE} 
+ *Extensions:* The mechanism for allowing the generic payload to carry protocol specific attributes.

*** Coding Styles and Transport Interfaces
TLM defines two coding styles: the *Loosely-Timed* (LT) and the *Approximately-Timed* (AT).
Coding styles are not syntactically enforced: they are just guidelines that improve code readability.
LT is suited for describing virtual platforms intended for software development.
However, where additional timing accuracy is required, usually in architectural analysis, the AT style is employed.
Virtual platforms typically do not contain many cycle-accurate models of complex components because of the performance impact. 
The two coding styles are distinguished by the *transport interface* components realize.

#+BEGIN_EXPORT latex
\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}
\begin{umlseqdiag}
\umlobject[x=0, class=\texttt{sc\_module}]{Initiator}
\umlobject[x=5, class=\texttt{sc\_module}]{Interconnect}
\umlobject[x=9, class=\texttt{sc\_module}]{Target}
\begin{umlcall}[op={\texttt{b\_transport(...)}}, type=synchron, with return, dt=8]{Initiator}{Interconnect}
\begin{umlcall}[op={\texttt{b\_transport(...)}}, type=synchron, with return, dt=2]{Interconnect}{Target}
\node (p1) at (0,-3.1) {\texttt{wait(delay)}} ;
\end{umlcall}
\end{umlcall}
\end{umlseqdiag}
\end{tikzpicture}
\end{center}
\caption{Blocking interface sequence} 
\label{fig:sequence}
\end{figure}
#+END_EXPORT

LT uses the *blocking transport interface*, distinguished by the forward path method \texttt{b\_transport(PAYLOAD&, sc\_time&)}.
It is the simplest of the transport interfaces, in which each transaction is required to complete in a single interface method call.
The method, apart from the payload, takes a timing annotation argument.
By definition, the blocking transport method *may block*, that is call \texttt{wait}, somewhere along the forward path from initiator to target.
However, LT advises the designer to make the actual call to \texttt{wait} upon completion of the transaction, in the initiator.
Interconnect components and the target need only to increment the timing annotation argument.
The timing annotation argument would then reflect the accumulated delay of the transaction.
The initiator can then call \texttt{wait(sc\_time)} to register this delay with the simulation environment.
Figure \ref{fig:sequence} visualizes the interaction between components, during a blocking transport.

Appendix \ref{AppendixE} demonstrates the simplest TLM model that can be constructed: a system with one initiator (imagine a processor) and one target (imagine a memory).
\clearpage
  
** Message Passing Interface
In any Message Passing Interface, the concept of communication is (obviously) modeled as message passing.
The DE MoC concept of an event is associated with either a message transmission or a message reception statement.
This fact must be emphasized: an event is not a message, it is not something to be exchanged.
It is rather the exchange of a message that yields two events.
The DE MoC concept of a process can be reduced to an instance of a computer program that is being executed \cite{Tanenbaum1998} in an Operating System's (OS) environment.

Section [[Rationale]] presents the rationale behind choosing MPI, as the means for achieving spacial decomposition, in the proposed OoO PDES.
In unit [[Semantics of point-to-point Communication in MPI]] and [[MPI Communication Modes]] we present the semantics of the Message Passing Interface (MPI) communication primitives.
This Chapter is complemented by Appendix \ref{AppendixD}, where the reader can experience MPI's elegance, by means of an example implementation of the pipeline pattern.


*** Rationale
*Message Passing Interface* 3.0 (MPI) was the preferred implementation framework for the proposed OoO PDES.
The rationale behind this choice can be summarized as follows:
+ The ease of expressing process communication, that leads to improved readability and maintainability, when compared to other process manipulation APIs (e.g. POSIX)
+ Scalability. Tons of it. Any computing device or cluster with Internet Access, from a Raspberry Pi to Tianhe-2, is more than welcome to participate in the simulation.
  If the MPI runtime environment is configured properly, the software developer may remain agnostic about the exact communication fabric (e.g. shared memory, TCP/IP, DAPL).
+ High performance. Prior to version 3.0, MPI was deemed a bad choice for applications confined in shared memory nodes. 
  Threading APIs (e.g. OpenMP), or hybrid approached were a more favorable choice.   
  With the introduction of MPI 3.0, shared memory regions, for conducting communication apart from message passing, can be exposed to processes.
  



*** Semantics of point-to-point Communication in MPI
MPI is a message passing library interface specification, standardized and maintained by the Message Passing Interface Forum.
It is currently available for C/C++, FORTRAN and Java from multiple vendors (Intel, IBM, OpenMPI).
MPI addresses primarily the message passing parallel programming model, 
in which data is moved from the address space of one process to that of another process through cooperative operations on each process \cite{MessagePassingInterfaceForum2012}.

The basic communication primitives are the functions \texttt{MPI\_Send(...)} and \texttt{MPI\_Recv(...)}.
Their arguments specify, among others things, a data buffer and the peer process' or processes' unique id assigned by the MPI runtime.
By default, message reception is blocking, while message transmission may or may not block.
One can think of message transfer as consisting of the following three phases
1. Data is pulled out of the send buffer and a message is assembled
2. A message is transferred from sender to receiver
3. Data is pulled from the incoming message and disassembled into the receive buffer

*Order:*
Messages are non-overtaking.
If a sender sends two messages in succession to the same destination, 
and both match the same receive (a call to \texttt{MPI\_Recv}), 
then this operation cannot receive the second message if the first one is still pending. 
If a receiver posts two receives in succession,
and both match the same message, 
then the second receive operation cannot be satisfied by this message, if the first one is still pending. 
This requirement facilitates matching of sends to receives and also guarantees that message passing code is deterministic.

*Fairness:*
MPI makes no guarantee of fairness in the handling of communication. 
Suppose that a send is posted. 
Then it is possible that the destination process repeatedly posts a receive that matches this send, 
yet the message is never received, 
because it is each time overtaken by another message, 
sent from another source. 
It is the programmer’s responsibility to prevent starvation in such situations.



*** MPI Communication Modes
The MPI API contains a number of variants, or *modes*, for the basic communication primitives.
They are distinguished by a single letter prefix (e.g. \texttt{MPI\_Isend(...)}, \texttt{MPI\_Irecv(...)}).
As dictated by the MPI version 3.0, the following communication modes are supported \cite{MessagePassingInterfaceForum2012}:

*No-prefix for standard mode: \texttt{MPI\_Send(...)}*
In this mode, it is up to MPI to decide whether outgoing messages will be buffered. 
MPI may buffer outgoing messages. 
In such a case, the send call may complete before a matching receive is invoked. 
On the other hand, buffer space may be unavailable, or MPI may choose not to buffer outgoing messages, for performance reasons. 
In this case, the send call will not complete, blocking the transmitting process, until a matching receive has been posted, and the data has been moved to the receiver.

*B for buffered mode: \texttt{MPI\_Bsend(...)}* 
A buffered mode send operation can be started whether or not a matching receive has been posted. 
It may complete before a matching receive is posted. 
However, unlike the standard send, this operation is local, and its completion does not depend on the occurrence of a matching receive. 
Thus, if a send is executed and no matching receive is posted, then MPI must buffer the outgoing message, so as to allow the send call to complete. 
A buffered send operation that cannot complete because of a lack of buffer space is erroneous. 
When such a situation is detected, an error is signaled that may cause the program to terminate abnormally. 
On the other hand, a standard send operation that cannot complete because of lack of buffer space will merely block, 
waiting for buffer space to become available or for a matching receive to be posted. 
This behavior is preferable in many situations. 
Consider a situation where a producer repeatedly produces new values and sends them to a consumer. 
Assume that the producer produces new values faster than the consumer can consume them. 
If buffered sends are used, then a buffer overflow will eventually occur. 
Additional synchronization has to be added to the program so as to prevent this from occurring. 

*S for synchronous mode: \texttt{MPI\_Ssend(...)}*
A send that uses the synchronous mode can be started whether or not a matching receive was posted. 
However, the send will complete successfully only if a matching receive is posted, and the receive operation has started to receive the message sent by the synchronous send.
Thus, the completion of a synchronous send not only indicates that the send buffer can be reused, 
but it also indicates that the receiver has reached a certain point in its execution, 
namely that it has started executing the matching receive. 
If both sends and receives are blocking operations then the use of the synchronous mode provides synchronous communication semantics: 
a communication does not complete at either end before both processes *rendezvous* at the communication point.

*R for ready mode: \texttt{MPI\_Rsend(...)}*
A send that uses the ready communication mode may be started only if the matching receive is already posted. 
Otherwise, the operation is erroneous and its outcome is undefined.
Ready sends are an optimization when it can be guaranteed that a matching receive has already been posted at the destination.
On some systems, this allows the removal of a hand-shake operation that is otherwise required and results in improved performance. 
A send operation that uses the ready mode has the same semantics as a standard send operation, or a synchronous send operation; 
it is merely that the sender provides additional information to the system (namely that a matching receive is already posted), that can save some overhead. 

*I for non-blocking mode: \texttt{MPI\_Isend(...)}, \texttt{MPI\_Ibsend(...)}, \texttt{MPI\_Issend(...)} and \texttt{MPI\_Irecv(...)}*
Non-blocking message passing calls return control immediately (hence the prefix I), 
but it is the user's responsibility to ensure that communication is complete, 
before modifying/using the content of the data buffer.
It is a complementary communication mode that works en tandem with all the previous.
The MPI API contains special functions for testing whether a communication is complete, or even explicitly waiting until it is finished.
In Appendix \ref{AppendixD} the reader can find an example use case for this communication mode.


\clearpage

* Out of Order PDES with MPI
In Section [[The Chandy/Misra/Bryant synchronization algorithm]] and [[Deadlock Avoidance]] we present the conservative synchronization algorithm known as *Chandy Misra Bryantt* (CMB).
In Section [[MPI Realization of CMB]] a pseudocode description of the CMB is demonstrated. The pseudocode incorporates MPI communication primitives.

** The Chandy/Misra/Bryant synchronization algorithm
The synchronization algorithm at the heart of the proposed OoO PDES is known as the *Chandy/Misra/Bryant (CMB)* \cite{Bryant} \cite{Chandy1979}.
Historically, it has been the first of the family of conservative synchronization algorithms \cite{Fujimoto1990}.
According to the algorithm, the physical system to be simulated must be modeled as a number of communicating sequential processes.
The system's state, a set of variables, is partitioned amongst the system's processes.
Execution is reactive; it is sparked by an event and produces further events and side-effects (changes in the system's state variables).
Each process keeps its own perspective of logic time through a counter.
The counter advances according to the timestamp of the last event selected for execution.

Based on the system's state segregation, a static determination of which processes are interdependent can be established.
This is indicated by placing a *link* for each pair of dependent processes.
From a process' perspective a link can be either outgoing, meaning that events are sent via the link, or incoming, meaning that events are received through it.
An incoming link must encapsulate an unbounded [fn:kahn] First-In-First-Out (FIFO) data structure  for storing incoming events, in the order they are received.

The order by which events are received is *chronological*; non decreasing timestamp order.
This system-wide property is maintained by making each process select for computation the event that has the smallest timestamp.
A formal proof of how this local property *induces* a system-wide property can be found in \cite{Bryant} \cite{Chandy1979}.

Chronological reception of events is a necessary, but not sufficient, condition for ensuring *causality*.
The algorithm deals with the "is an event safe to execute" dilemma by *blocking* a process until each of its incoming links contains an event.
All the above are demonstrated in Algorithm \ref{alg:initial_CMB}. 
The synchronization algorithm is realized as a process' main event loop.

#+BEGIN_LATEX
\begin{algorithm}
\caption{Process event loop, without deadlock avoidance}
\label{alg:initial_CMB}
\begin{algorithmic}[1]

   \While{process time < some T}  
      \State \textbf{Block} until each incoming link contains at least one event
      \State select event M, with the \textbf{smallest} timestamp across all incoming links.
      \State set process' \textbf{counter} = timestamp(M)
      \State \textbf{execute} event M
      \State \textbf{communicate} resulting events over the appropriate links
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX

[fn:kahn] The system description is quite similar to that of another MoC called \textit{"Kahn process networks"} \cite{Editor2014}, which also uses unbounded FIFOs as a channel communication mechanism.
          The difference is qualitative: the DE MoC incorporates timing semantics.
          A Kahn process network is *untimed* by definition.

** Deadlock Avoidance
The naive realization of the process' event loop presented in Algorithm \ref{alg:initial_CMB} leads to deadlock situations, like the one depicted in Figure [[fig:deadlock]].
The links placed along the outer loop are empty (dashed lines), thus simulation has halted, even though there are pending events (across the links of the inner loop).

A global simulation moderator could easily detect deadlocks and allow the process, that has access to the event with the global minimum timestamp, to resume execution.
The presence of a moderator, however, would violate the distributed nature of the simulation, and thus increase the implementation complexity of the simulation environment.
For the context of this thesis, a distributed mechanism is more favorable.
What follows is the presentation of a distributed mechanism for overcoming these situations, referred to as the *null-event deadlock avoidance* \cite{Fujimoto1999}.

#+CAPTION: Deadlock scenario justifying the use of Null messages in the CMB
#+ATTR_LATEX: :width 0.64\linewidth 
#+NAME: fig:deadlock
[[file:Figures/deadlockScenario.pdf]]

Figure [[fig:deadlock]] demonstrates an air traffic simulation, where the airports (ARL, CDG and SKG) constitute the simulation processes.
The events exchanged between the airports represent flights (the time unit being arbitrary).
Furthermore, it is assumed that there is an *a priori* knowledge concerning the flight time between airports.
This knowledge is referred to as the *lookahead* and takes the form of a function $(P \times P) \rightarrow \mathbb{N}$.
By selecting the distance between every airport to be 3 time units, one can deduce the following:
If SKG is at time 5, then ARL or CDG should not expect any flight arriving from SKG before time 8.

The simulation is deadlocked: all of the airports contain an empty link and therefore, according to Algorithm \ref{alg:initial_CMB}, they must block.
At deadlock, the counter values for each airport are: (ARL,2), (SKG,5), (CDG,3).
The intuition behind any technique, that could break the deadlock, should rely on the following observation:
if CDG knew that SKG is at time 5, then it could be able to accept the incoming flight from ARL, without breaking causality.

To "communicate" this information, SKG could create a special kind of event, a *null event* that does not represent a flight. 
Its timestamp should be 8 (counter+lookahead) and the event should be placed on all of SKG's outgoing links.
With this null event, SKG is informing the other airports about its time perspective.
A null event is still an event, so CDG would acknowledge it during the selection phase, and thus would be able to receive the flight from ARL.
CDG now sits at 5 and in the same fashion it could broadcast a null event with timestamp 8, that would in turn unblock ARL.
It is evident that the deadlock situation has been resolved, at the expense of flooding the communication links with null events.

The modified, for deadlock avoidance, algorithm is described in Algorithm \ref{alg:null-event}.
The important facts one must keep in mind with this deadlock avoidance mechanism are:
- The logic time counter of a process is still determined by the last event selected for execution.
- Null events are created when a process updates its logic time counter.
- Each process propagates null events on all of its outgoing links.
- The efficiency of this mechanism is highly dependent on the designer's ability to determine sufficiently large lookaheads. 

#+BEGIN_LATEX
\begin{algorithm}
\caption{Process event loop, with deadlock avoidance}
\label{alg:null-event}
\begin{algorithmic}[1]

   \While{process clock < some T}  
      \State \textbf{Block} until each incoming link FIFO contains at least one event
      \State Remove event M with the smallest timestamp from its FIFO.
      \State Set process' clock = timestamp(M)
      \State \textbf{React} to event M
      \State \textbf{Communicate} either a null or meaningful event to each outgoing link with timestamp = clock + lookahead
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX

** MPI Realization of CMB
Listing \ref{alg:CMB_mpi} is a pseudo code, sketching out the CMB synchronization algorithm with null event deadlock avoidance, using MPI's communication primitives.
The mechanism should be incorporated in a process' main event loop.
It is quite obvious that the concept of an event has been reduced to a simple data structure, with the timestamp being the most important field.
Much like SystemC, logic time modeling is an implied vector $(t,n,l)$: $t$ is the value of a process' counter, $n$ (delta) and $l$ are implied by, the event's position in the links' FIFO and the process' rank, respectively.
#+BEGIN_LATEX
\begin{algorithm}
\caption{CMB Process event loop in MPI}
\label{alg:CMB_mpi}
\begin{algorithmic}[2]

   \While{process clock < some T}  
      \State post a \texttt{MPI\_Irecv} on each incoming peer process
      \State post a MPI\_Wait: block until every receive has been completed
      \State save each message received in a separate, per incoming link, FIFO.
      \State identify message M with the smallest timestamp
      \State set counter = timestamp(M)
      \State process message M
      \State post a \texttt{MPI\_Issend} to each outgoing link with timestamp = counter + Lookahead(myRank, recvRank)
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX


\clearpage

* Methodology
This chapter is a synoptic presentation [fn:github] of the case studies constructed for the evaluation of the proposed OoO PDES.
All necessary simulations were carried out in a server equipped with two Intel Xeon E5-2603V3 processors, with a total of 128 GB of DDR4-1600 RAM.

Section [[Case Study 1: Airtraffic Simulation]] presents an airtraffic simulation, following the example presented in Section [[Deadlock Avoidance]].
The simulation incorporates a validation procedure: Causality hazards are detected and lead to simulation termination.
The Section is complemented with Appendix \ref{AppendixC}.
Section [[Case Study 2: Cache-coherent Multiprocessor]] presents the simulation of a cache-coherent multiprocessor.
For this case study 2 models where constructed: A Loosely-Timed SystemC TLM 2.0 model, simulated by SystemC's DES, 
and a "manually compiled" translation of it, compatible with the the proposed OoO PDES.

[fn:github] The source code for the case studies is publicly accessible in the following github repository: https://github.com/kromancer/Thesis.

** Case Study 1: Airtraffic Simulation
The simulation is parameterized on the number of airports, their topological arrangement and each airport's flight schedule.
The *topological arrangement* of the airports is determined at compile time.
For example, the three airport topology described in Figure [[fig:deadlock]] is demonstrated in Appendix \ref{AppendixC}

Figure \ref{fig:val} demonstrates the validation procedure for the simulation.
The following measures are taken to ensure correctness and remove bias:
+ In a pre simulation step, a randomized *global flight schedule* is created.
  Based on a flight's source field, the schedule is then distributed to the airports.
+ Prior to segregation, the global schedule is also "simulated" sequentially. 
  The sequential "simulation" is quite trivial and does not require a DES:
  The global schedule is traversed, and for every event a departure log entry and an arrival log entry are created.
  The entries are sorted and stored in the *reference global log*.
+ During simulation, airports exchange messages indicating flights, and every airport is responsible for creating a *log* of departures and arrivals.
  The logs are saved as .csv files for post simulation inspection.
  From an airport's perspective, its *flight schedule* is modeled as an incoming link, which is filled upon instantiation.
  The system's computational objective is to create a *global log* of departures/arrivals.
  The global log is consolidated post parallel simulation.
+ An airport can detect a *causality hazard* by simply checking if its counter (its perspective of logic time) is about to become less than its current value.
  When causality hazards are detected, a process aborts simulation.
+ Finally, the global log is checked against the reference global log for completeness.

#+BEGIN_EXPORT latex
\begin{figure}[htpb]
\centering
\tikzstyle{block} = [draw, fill=white, rectangle]

\begin{tikzpicture}[node distance=2cm, block/.style={ rectangle, draw=black, thick, fill=white, text centered, rounded corners, minimum height=2em }]
    \node [anchor=south,block, text width=6.3cm]                      (n1)   {\scriptsize Generate randomized global flight schedule};
    \node [anchor=south,block, below left of=n1, text width=4.3cm]    (n2)   {\scriptsize Segragate per airport \\ e.g. \texttt{fligth\_schedule\_ARL.csv}};
    \node [anchor=south,block, below of=n2, text width=4.3cm]         (n3)   {\scriptsize OoO PDES \\ Causality hazard $\rightarrow$ \texttt{MPI\_ABORT}};
    \node [anchor=south,block, below of=n3, text width=4.3cm]         (n5)   {\scriptsize Consolidate \& order logs};
    \node [block, right=0.6cm of n3, text width=1.3cm]                  (n4)   {\scriptsize Simulate Serially };
    \node [block, below of=n5] (n6) {\scriptsize Compare Logs};

    \draw[draw,->]       (n2|-n1.south) --  (n2.north);
    \draw[->]            (n2) --  (n3);
    \draw[->]            (n4|-n1.south) -- (n4.north);
    \draw[->]            (n3) --  (n5);
    \draw[->]            (n5) --  (n6);
    \draw[->]            (n4.south) |-  (n6.east);

\end{tikzpicture}
\caption{Case Study 1: Validation Procedure} 
\label{fig:val}
\end{figure}
#+END_EXPORT

The implementation is structured upon three C++ classes: \texttt{Process}, \texttt{Links} and \texttt{Flight}. 
Their relationship is quite simple: a \texttt{Process} "has a" \texttt{Links} and a \texttt{Links} "has many" \texttt{Flight}.
The class \texttt{Links} realizes a process' incoming links.
Outgoing links are realized implicitly: 
+ The system's topology is deserialized by MPI, in the form  of *distributed-graph group communicators* \cite{MessagePassingInterfaceForum2012}, as demonstrated in Appendix \ref{AppendixC}.
  Each process gets a communicator that represents its neighborhood, that is other processes that send and receive messages to/from this process.
+ The series of communication primitives used in Algorithm \ref{alg:CMB_mpi} have almost the same aggregate effect as the collective communication primitive \texttt{MPI\_Neighbor\_Allgather}.
  With this collective communication primitive, the process sends the same message to all of its outgoing neighbors, and receives a (different) message from every incoming neighbor.
+ The communication will only block the process if either a message has not been received from every incoming neighbor, or the MPI runtime can not buffer the outgoing message.
  The fact that outgoing messages can be buffered must be emphasized. 
  This communication primitive is not synchronous, it does not denote a rendezvous point (see Section [[MPI Communication Modes]]).
  Moreover, it is now evident why outgoing links are realized implicitly: a process relies on MPI's buffering capabilities.
+ By employing this collective operation, communication and execution confront to the their simple definition, presented in Section [[The Discrete Event Model of Computation]], where both input and output require/produce one event.
  Furthermore, the implementation of the null-event deadlock avoidance mechanism becomes simple: a null-event occurs when a process receives a message that was not meant for it, that is the destination field of the flight does not match its own airport identity.
An airport's main event loop, the "hotspot" of \texttt{Process::run()}, is demonstrated in Figure \ref{fig:airport}.

#+BEGIN_EXPORT latex
\begin{figure}[htpb]
\centering
\begin{tikzpicture} [
    auto,
    decision/.style = { diamond, draw=black, thick, fill=white,
                        text badly centered,
                        inner sep=1pt, rounded corners },
    block/.style    = { rectangle, draw=black, thick, 
                        fill=white, text centered,
                        rounded corners, minimum height=2em },
    line/.style     = { draw, thick, ->, shorten >=2pt },
  ]
  % Define nodes in a matrix
  \matrix [column sep=5mm, row sep=5mm] {
                    & \node [decision] (n0) {\scriptsize\texttt{!readyToTerminate}};  & \\
                    & \node [block, text centered] (n1) {\small\texttt{MPI\_Neighbor\_Allgather(...)}}; & \\
                    & \node [block, text centered, text width=5.2cm] (n2) {\small\texttt{Flight e = links.nextFlight()} \small\texttt{counter = e.tstamp()}};    & \\
                    & \node [decision] (n3) {\scriptsize\texttt{!e.isOutbound()}};  & \node [block, text width=2.5cm] (n4) {\small\texttt{updateLog()} \texttt{sendbuf = e}}; & \\
                    & \node [decision] (n5) {\scriptsize\texttt{!e.isInbound()}};   & \node [block] (n6) {\small\texttt{updateLog()}}; & \\
                    & \node  (n7) {};   &  \node (n8) {};& \\
                    & \node [decision, text width=2.8cm] (n9) {\scriptsize\texttt{links.isAnyEmpty()} \small\texttt{||} \scriptsize\texttt{!e.isOutbound()}};  & \\
  };
  % connect all nodes defined above
  \begin{scope} [every path/.style=line]
    \path (n0)        --    (n1);
    \path (n1)        --    (n2);
    \path (n2)        --    (n3);
    \path (n3)        --    node [near start] {yes} (n4);
    \path (n4)        --++ (2,0) |-    (n8);
    \path (n3)        --    (n5);
    \path (n5)        --    node [near start] {yes} (n6);
    \path (n6)        --    (n8);
    \path (n5)        --    (n7);
    \path (n8)        --    (n7);
    \path (n7)        --    (n9);
    \path (n9)        --++ (-5,0) |-   node [near end] {yes}(n0);
    \path (n9)        --++ (-5,0) |-   node [near end] {no} (n2);
  \end{scope}
\end{tikzpicture}
\caption{Case Study 1: Airport's event loop} 
\label{fig:airport}
\end{figure}
#+END_EXPORT


\clearpage

** Case Study 2: Cache-coherent Multiprocessor
A SystemC TLM 2.0 diagram of the cache-coherent multiprocessor that will be modeled can be found in Figure [[fig:case2]].
Every component was coded in C++.
The processors are executing a "pseudoprogram": they are just generating memory accesses, based on a previously performed memory trace collection of an actual program.
The actual program was multithreaded (4 threads), and was experiencing (deliberately) the phenomenon known as "false sharing" \cite{Hennessy2011}.
The L1 Data caches are 8-way set associative, their size being parameterizable.
Coherence amongst the caches is realized through a Modified Shared Invalid (MSI) scheme, by a directory which acts like an inclusive L2 cache.
Since actual data are not needed, main memory presence is implied by the directory.

#+CAPTION: Case Study 2: A cache-coherent multiprocessor
#+ATTR_LATEX: :width 0.64\linewidth 
#+NAME: fig:case2
#+RESULTS:
[[file:Figures/multiprocessor.pdfno]]







\clearpage

* Analysis
Sections [[Time Complexity]] and [[Monotonicity of Communication]] are related to the analysis of the first case study.
Section [[TLM translation]] discusses the most important aspect of the second case study: the transformation of a SystemC Loosely-Timed TLM 2.0 model to a model for the proposed OoO PDES.

** Time Complexity
The following assumption is made: the time complexity of a deterministic DES (Section [[Time and Determinism]]), is of the form $\mathcal{O}(f(|\mathds{E}|))$.
In the proposed OoO PDES:

#+BEGIN_LATEX latex
\begin{equation} 
   |\mathdbs(E)| = f(|\mathbb{P}|, \min\{lookahead(P_i,P_j)\}_{i,j \leq |\mathbb{P}|}).
\end{equation}
#+END_LATEX

In simpler terms: 
+ The total number of simulation events is highly sensitive to the number of null messages.
+ The number of null messages produced is proportional to the minimum lookahead value across the system.
+ In the worst case, the minimum lookahead will be 1. This introduces a qualitative shift in time complexity, which now becomes: 
  $\mathcal{O}(f(t_{end}-t_{start}))$ with $t_{end}$ and $t_{start}$ being the timestamps of the first and last simulation events.
This well established empirical observation \cite{Fujimoto1999} for simulators dependent on the CMB algorithm, has been confirmed.

** Monotonicity of Communication
In the DE MoC, any a communication function possessing the property $e_1 \sqsubset e_2 \implies g(e_1) \sqsubset g(e_2)$ is called *monotonic*.
In the context of the proposed PDES, it can be easily proven that any communication function that uses a lookahead which is only influenced by $(P_i, P_j)$ is monotonic.
The communication functions that are used in the case studies are monotonic.

And here comes a reasonable question: What happens if the lookahead function is not only influenced by $(P_i, P_j)$.
For example, consider the following situation: 
at an airport, an airplane's departure is followed by the departure of a faster airplane.
Both the flights are destined for the same target airport.
The faster aircraft is meant to arrive at the target airport sooner.
Using the visual understanding of the DE MoC from Section [[Causality and Concurrency]], the situation is captured in Figure \ref{fig:nonmon}.

#+BEGIN_EXPORT latex
\begin{figure}[htpb]
\centering
\begin{tikzpicture}[
arrow/.style={draw,->,>=stealth},
point/.style={circle,fill=black},
every node/.style={node distance = 10},
]
\node (p1) at (0,2) {$p_1$};
\node (p2) at (0,1) {$p_2$};
\node (p1l) at (0.2,2) {};
\node (p2l) at (0.2,1) {};
\node (p1r) at (8,2) {};
\node (p2r) at (8,1) {};

\draw[arrow] (p1l.center) to (p1r);
\path[arrow] (p2l.center) to (p2r);
\path[draw]  (p1l.center) edge (p2l.center);

\node[point, label={\small slow dep.}] (a) at (3,2) {};
\node[point, label={\small fast dep.}] (b) at (5,2) {};
\node[point, label=below:{\small fast arr.}] (c) at (2,1) {};
\node[point, label=below:{\small slow arr.}] (d) at (6,1) {};

%\node[point] (k) at (4,1) {};
%\node [below of = k] {a};
%\node[point, label=b] (l) at (7,2) {};

\path[arrow] (b) edge node [right] {} (c);
\path[arrow] (a) edge node [right] {} (d);
%\path[arrow] (k) edge node [right] {} (l);

\end{tikzpicture}
\caption{Non-monotonic communication in the DE MoC} 
\label{fig:nonmon}
\end{figure}
#+END_EXPORT

How can a DES cope with non-monotonic communication?
A naive approach would be to define a static execution schedule: first $p_1$ and then $p_2$.
The naivety of the approach lies in the following self-contradiction: why run a simulation if you already know its outcome?

The proposed PDES can not handle such a situation. 
Timestamps in $p_2$ 's incoming link are not placed in an increasing timestamp order by $p_1$ and thus $p_1$ is bound to face a causality error: its local time will advance backwards.
It is therefore evident that the only way to tackle this situation, while keeping the CMB synchronization mechanism, is to transform the model in a way that all communication functions are monotonic.
To identify the needed transformation, one must take a closer look on the nature of the imbalance that causes non-monotonicity.
The concept of airplane speed was introduced, and speed is nothing more than a backdoor for time: 
alas, time has managed to break loose from the confining cage of logic time modeling; he demands explicit introduction into the system as a process!

The needed transformation is depicted in Figure \ref{fig:nonmon}. The weights over the links denote the lookahead ($L$ for lookahead).
The two airport processes, $p_1$ and $p_2$, no longer communicate directly with each other.
They rely on process "time" to advance the flight through space.

#+BEGIN_EXPORT latex
\begin{figure}[htpb]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,semithick]
  %\tikzstyle{every state}=[fill=white,draw=none,text=white]

  \node[state]         (p1)                         {$p_1$};
  \node[state]         (time) [above right of=p1]   {$time$};
  \node[state]         (p2)   [below right of=time] {$p_2$};

  \path[every node/.style={sloped,anchor=south,auto=false}]
        (p1)   edge [bend left]  node {$L(p_1, p_2)$} (time)
        (p2)   edge [bend right] node {$L(p_2, p_1)$} (time)
        (time) edge [loop above] node {1}           (time)
               edge [bend right] node {1} (p2)
               edge [bend left]  node {1} (p1);
		
\end{tikzpicture}
\caption{Non-monotonic transformation using the CMB synchronization algorithm} 
\label{fig:nonmon}
\end{figure}
#+END_EXPORT

Process "time" self communicates, thus forms a causal loop.
Process "time" is a *clock*, with the term "clock" being now formally defined as any process with a feedback loop governed by a computation function without a fixed-point.
An intuitive understanding of this definition can be easily conceived, if one thinks of a digital logic NOT gate that has its output wired to its input.

It is quite clear that this system has the worst-case time complexity of the proposed OoO PDES: the minimum lookahead is 1.
A DES that always produces worst-case behavior is called a *step simulator* [fn:magic].

For SystemC's DES the situation is quite similar.
The transformation can be described in many ways, but the core idea remains the same:
there is a need for an \texttt{sc\_clock} instance, which is nothing more than a predefined process, functioning in the same way as the "time" process.





\clearpage

[fn:magic] Handling non-monotonic communication, while having $\mathcal{O}(f(|\mathdbs{\mathds{E}}|))$ time complexity, is an interesting property, a DES might possess.
           If this situation has not been explicitly identified, the author would like to claim the name \texttt{"Sander-Ungureanu-Sotiropoulos (SUS)"}, to describe this property.

** TLM translation
The "manual compilation" procedure comprised of three basic steps.
+ TLM wrappers were replaced by the MPI wrappers, developed for the airtraffic simulation.
+ Every pair of initiator-target socket binding,  has been replaced with two pairs of incoming-outgoing links: one in the "initiating" process and one in the "target" process.
+ Memory hierarchy access delays have been the foundation for the formulation of the lookahead values. 
Unfortunately, the proposed OoO PDES was not able to outperform SystemC's DES, mainly due to the size of the model.
\clearpage

* Conclusion and Future Work
The major contributions of this work can be found in Section [[Contributions]].
Section [[Limitations]] provides a list of actions that the author believes that should have been performed
This work is far from complete: The brave Theseus that would like to confront the Minotaur can find Ariadne's thread in Section [[Future Work]]
Section [[Reflections]] revisits, in a more specific way, the cui bono question answered in Section [[Purpose]].

** Contributions
The following are the main research contributions of +this work:
+ In Section [[The Discrete Event Model of Computation]] a different approach is adopted for presenting the DE MoC, 
  when compared to the reference work in MoCs by the Ptolemy Project [fn:ptolemy].
  It is the fact that time modeling is not included in the description of the DE MoC itself; 
  Time modeling is an implementation concern.
  For the abstract/mathematical description of the DE MoC, Lamport's "happens before" relationship \cite{Lamport1978} suffices in describing 
  the important concepts emanating (e.g. causality, concurrency and determinism).
+ *Topology mapping* To the best of our knowledge
  One of the major features of MPI's topology interface is that it can easily be used to adapt the MPI process layout to the underlying network and system topology.

** Limitations
+ The theoretical description of the DE MoC in section [[The Discrete Event Model of Computation]] is far from complete.
  Since the DE MoC is considered as an abstract machine, there should be a proof that would indicate its equivalence with some form of a Turing machine.
  In the same spirit, Section [[Monotonicity of Communication]] assumes that a deterministic DES is equivalent to a deterministic Turing machine, without presenting a proof.

+ Intel's Xeon Phi coprocessor was not used as an experimentation tool, despite this being specified as a primary objective in the project plan.
  Its Multiple Instruction Multiple Date (MIMD) architecture and its highly parameterized MPI implementation, makes it an ideal platform for performing the proposed OoO PDES simulation.
  However, we are able to report that SystemC 2.3.1 can be compiled with Intel's C++ compiler 16.0 for the Xeon Phi platform. 
  Moreover, the compiled package was verified against the accompanying test suite. 

+ Not establishing an open communication channel with the following two scientists/engineers/researchers: Professor Rainer Dömer [fn:domer] and Dr. Jakob Engblom [fn:engblom]
  It is a researcher's ethical obligation towards society to take the initiative for disseminating his work.
  This work could be of some infinitesimal value towards the important, for the collective, work they do on ES design.
  Vice versa, their feedback would have greatly increased the quality of the work.


[fn:ptolemy] The Ptolemy Project, Center for Hybrid and Embedded Software Systems (CHESS),
             Department of Electrical Engineering and Computer Sciences, University of California at Berkeley: http://ptolemy.eecs.berkeley.edu/
[fn:domer] Professor Rainer Dömer works at the University of California Irvine, The Henry Samueli School of Engineering: http://www.cecs.uci.edu/~doemer/. 
           His current project \textit{Parallel SystemC Simulation on Many-Core computer architectures} is highly relevant to this thesis.
[fn:engblom] Dr. Jakob Engblom works as a Product Management Engineer at Intel in Uppsala: https://www.linkedin.com/in/jakobengblom.
           His academic research and professional experience with virtual platforms would be a significant source of feedback.

** Future Work
In Section [[Delimitations]], the automatic compilation of a SystemC TLM 2.0 model into our proposed MPI implementation was indicated as a delimitation of this project.
However, it is the next logical step in progressing this work, since it has been deemed feasible.
Some general guidelines are:
+ For the critical task of analyzing the model (identifying the processes and the links between them),  ForSyDe SystemC's approach could be mimicked \cite{Hosein2012}.
  Using SystemC's well defined API for module hierarchy (e.g. \texttt{get\_child\_objects()}), along with the introduction of meta objects, the system's structure can be
  serialized at runtime, in the pre-simulation phase of elaboration.
+ After elaboration simulation should halt. The desirable outcome, probably in some XML format, was the serialization of the system's structure. 
  The proposed compiler can now use this abstract representation in conjunction with a library of code skeletons to generate the desired MPI implementation.

Although not relevant to the thesis, during the implementation of the cache hierarchy, the author has identified the need for an open-source framework for designing, documenting, implementing and testing FSMs.
[[http://perso.ensta-paristech.fr/~kielbasi/tikzuml/][TikZ-UML]] could serve as the front-end. 
It can express most of the UML 2.0 statechart defined concepts and produce a visual representation.
Since the syntax follows a structural manner, a compiler for the following backends could be developed:
+ [[http://nusmv.fbk.eu/][NuSMV]] for model checking by expressing requirements as temporal logic expressions.
+ [[http://www.state-machine.com/][Quantum Leaps]] can provide a well structured, easily maintained and tested C/C++ real-time implementation. 
Furthermore, [[http://orgmode.org/][Emacs' Org mode]] could be used for housing the compilation procedure, by unifying the editing of all the above representations of the FSM.
Emacs Org mode is more than a text editor: it is an ecosystem that enables the symbiosis of source code and document, in an unprecedented way, that follows Donald Knuth concept of literate programming.
It is an indispensable tool when reproducibility is a desirable feature \cite{Schulte2011}.

** Reflections
On May the 3\textsuperscript{rd} 2016 the SystemC user community came together at Intel's headquarters in Munich, 
for a full-day workshop about the evolution of the various SystemC standards.
The event was called [[http://accellera.org/news/events/systemc-evolution-day-2016][SystemC Evolution Day 2016]]  [fn:PRES] and was organized by [[http://accellera.org/about][Accelera]], the organization responsible for advancing the language.
Professor Rainer Dömer gave a highly influential presentation titled \textit{"Seven Obstacles in the Way of Parallel SystemC Simulation"}, 
from where the following views can be induced:
+ A formal understanding of the DE MoC is needed.
+ The progression from sequential DES to PDES is of vital importance for the longevity of the language. 
  As Professor Dömer humorously remarks: \textit{"SystemC must embrace true parallelism otherwise it will go down the same path as the dinosaurs"}

The fact that that this project's initiation precedes ($\sqsubset$) the event, can be regarded as an indication of proper alignment:
this project is organically bound to the ongoing discussion about SystemC's new major revision.

\clearpage

[fn:PRES] All presentations from the event are available at: [http://accellera.org/news/events/systemc-evolution-day-2016]

* References
\renewcommand\refname{}
\bibliography{References}
\bibliographystyle{myIEEEtran}
\clearpage

* Appendices                                                         :ignore:
#+BEGIN_EXPORT latex
\appendix
\section*{Appendices}
\addcontentsline{toc}{section}{Appendices}
\renewcommand{\thesubsection}{\Alph{subsection}}
#+END_EXPORT
** SystemC: Producer Consumer                                        :ignore:
#+BEGIN_EXPORT latex
\subsection{SystemC: Producer Consumer}
\label{AppendixA}
#+END_EXPORT

This example complements the presentation of the SystemC simulation engine in Section [[Sequential Discrete Event Simulation]].
It is an example of a producer-consumer system.
The producer communicates with the consumer via a FIFO channel.
Its primary purpose is to demystify the way primitive channels are implemented in SystemC, by revealing their event driven nature.

These are the interfaces the channel must be able to provide to the actors: 
#+BEGIN_SRC cpp
  #include "systemc.h"
  #include <iostream>
  using namespace sc_core;

  //****************
  // FIFO Interfaces
  //****************
  // Fifo interface exposed to Producers
  class fifo_write_if: virtual public sc_interface
  {
  public:
      // blocking write
      virtual void write(char) = 0;
      // number of free entried
      virtual int numFree() const = 0;
  };
  // Fifo interface exposed to Consumers
  class fifo_read_if: virtual public sc_interface
  {
  public:
      // blocking read
      virtual char read() = 0;
      // number of available entries
      virtual int numAvailable() const = 0;
  };
#+END_SRC


Following, is the interface of the fifo channel, which internally acts like a circular buffer.
#+BEGIN_SRC cpp
  //*************
  // FIFO channel
  //*************
  class Fifo: public sc_prim_channel, public fifo_write_if, public fifo_read_if
  {
  protected:
      int   size;
      char *buf;
      int   free;
      int   ri; // read index
      int   wi; // write index
      int   numReadable, numRead, numWritten;
      // For notifying Producer and Consumer
      sc_event Ev_dataRead;
      sc_event Ev_dataWritten;
  public:
      // Constructor
      explicit Fifo(int _size=16):
	  sc_prim_channel(sc_gen_unique_name("thefifo"))
	  {
	      size = _size;
	      buf = new char[_size];
	      reset();
	  }
      // Destructor
      ~Fifo(){ delete [] buf; }
      int  numAvailable() const { return numReadable - numRead; }
      int  numFree() const { return size - numReadable; }
      void reset() { free=size; ri=0; wi=0; }
      void write(char c);
      char read();
      void update();
  };
#+END_SRC

Next we see how the channel realizes the blocking read and write interfaces.
The \texttt{read} and \texttt{write} methods are executed during the *evaluation phase*.
The co-routine that executes these methods will yield immediately if it reaches the \texttt{wait} statement.
When an event is passed as an argument to the \texttt{wait} function, the co-routine's sensitivity is said to change dynamically.
The \texttt{request\_update} method (inherited from \texttt{sc\_prim\_channel}) is a kernel callback. 
It signals the kernel that during the *update phase* he should execute the channel's \texttt{update} method.
#+BEGIN_SRC cpp
  // Blocking write implementation
  void Fifo::write(char c)
  {
      if (numFree() == 0)
	  wait( Ev_dataRead );
      numWritten++;
      buf[wi] = c;
      wi = (wi+1) % size; // Circular buffer
      free--;
      request_update();
  }
  // Blocking read implementation
  char Fifo::read()
  {
      if (numAvailable() == 0)
	  wait( Ev_dataWritten );
      numRead++;
      char temp  = buf[ri];
      ri = (ri+1) % size; // Circular buffer
      free++;
      request_update();
      return temp;
  }
#+END_SRC

Following, is the implementation of the \texttt{update} method, which is executed during the update phase by the kernel's.
A yielded (*blocked*) co-routine might end up in the *runnable* set if it has declared its sensitivity to the event being notified. 
#+BEGIN_SRC cpp
  // Update method called in the UPDATE phase of the simulation
  void Fifo::update()
  {
      if (numRead > 0)
	  Ev_dataRead.notify(SC_ZERO_TIME);
      if (numWritten > 0)
	  Ev_dataWritten.notify(SC_ZERO_TIME);
      numReadable = size - free;
      numRead = 0;
      numWritten = 0;
  }

#+END_SRC

Next we see the implementation of the producer and consumer modules.
The co-routine is declared sensitive (static sensitivity) to a clock's rising edge.
The co-routine that represents these modules executes the \texttt{run} function.
Since all co-routines are declared runnable at *elaboration*, 
they need to yield immediately after entering the function.
#+BEGIN_SRC cpp
class Producer: public sc_module
{
public:
    sc_port<fifo_write_if> out;
    sc_in<bool> clock;
    void run()
	{
	    while(1)
	    {
		wait(); // wait for clock edge
		out->write(1);
		cout << "Produced at: " << sc_time_stamp() << endl;
	    }
	}
    // Constructor
    SC_CTOR(Producer)
	{
	    SC_THREAD(run);
	    sensitive << clock.pos();
	}
};


class Consumer: public sc_module
{
public:
    sc_port<fifo_read_if> in;
    sc_in<bool> clock;
    void run()
	{
	    while(1)
	    {
		wait(); // wait for clock edge
		char temp = in->read();
		cout << "Consumed at: " << sc_time_stamp() << endl;
	    }
	}
    SC_CTOR(Consumer)
	{
	    SC_THREAD(run);
	    sensitive << clock.pos();
	}

};
#+END_SRC

Finally, the modules are linked with the fifo and their clock, and simulation is started.
#+BEGIN_SRC cpp
int sc_main(int argc, char *argv[])
{
    sc_clock clkFast("ClkFast", 1, SC_NS);
    sc_clock clkSlow("ClkSlow", 500, SC_NS);

    Fifo fifo1;

    Producer p1("p1");
    p1.out(fifo1);
    p1.clock(clkFast);

    Consumer c1("c1");
    c1.in(fifo1);
    c1.clock(clkSlow);

    sc_start(5000, SC_NS);
    
    return 0;
}
#+END_SRC

** SystemC: Non-Determinism in SystemC                               :ignore:
#+BEGIN_EXPORT latex
\subsection{SystemC: Non-Deterministic yet Repeatable}
\label{AppendixB}
#+END_EXPORT

The following code example should in theory lead to non-deterministic behavior.
It models a race condition.
The system contains 3 processes which access a sharedVariable: 2 of them write it and 1 reads it.
At every clock pulse, all 3 processes are made runnable.
In practice however there is a repeatable pattern: processes are selected in the order in which their modules are instantiated.
If this holds, the one can draw the conclusion that logic time in SystemC has an implied third dimension: it is a vector $(t,n,p_{id}) \in \mathbb{N}^3$, and thus simulation events are totally ordered, which makes any simulation deterministic.
SystemC's LRM explicitly states:
\textit{"The order in which process instances are selected from the set of runnable processes is implementation-defined. However, if a specific version of a specific implementation runs a specific application using a specific input data set, the order of process execution shall not vary from run to run."}
One could device the following terms to describe this situation: *non-deterministic yet repeatable* or *pseudo non-deterministic*.

#+BEGIN_SRC cpp
  #include "systemc.h"

  using namespace sc_core;


  std::string sharedVariable;

  SC_MODULE(chaos1)
  {
      sc_in<bool> clock;

      void run()
      {
	  while(1)
	  {
	      wait();
	      sharedVariable = "chaos";
	  }
      }

      SC_CTOR(chaos1)
      {
	  SC_THREAD(run);
	  sensitive << clock.pos(); // static sensitivity
      }
  };

  SC_MODULE(chaos2)
  {
      sc_in<bool> clock;

      void run()
      {
	  while(2)
	  {
	      wait();
	      sharedVariable = "and destruction";
	  }
      }

      SC_CTOR(chaos2)
      {
	  SC_THREAD(run);
	  sensitive << clock.pos(); // static sensitivity
      }
    
  };

  SC_MODULE(observer)
  {
      sc_in<bool> clock;

      void run()
      {
	  while(2)
	  {
	      wait();
	      cout << sharedVariable << endl;
	  }
      }

      SC_CTOR(observer)
      {
	  SC_THREAD(run);
	  sensitive << clock.pos(); // static sensitivity
      }
    
  };





  int sc_main(int argc, char *argv[])
  {
      sc_clock clock("clock", 1, SC_NS);
      chaos1 c1("c1");
      chaos2 c2("c2");
      observer ob("ob");
	
      c1.clock(clock);
      c2.clock(clock);
      ob.clock(clock);
    
      sc_start(2, SC_NS);
    
      return 0;
  }


#+END_SRC
\clearpage

** SystemC TLM 2.0 Example: A Loosely-Timed Model                    :ignore:
#+BEGIN_EXPORT latex
\subsection{SystemC TLM 2.0 Example: A Loosely-Timed Model}
\label{AppendixE}
#+END_EXPORT
What follows is an example of a LT TLM model comprising of an *Initiator* and a *Target* component.
An initiator component must inherit \texttt{sc\_module} and realize the \texttt{tlm\_bw\_transport\_if} interface.
It must contain at least one initiator socket.
From SystemC's viewpoint, a socket is basically a convenience class, wrapping an \texttt{ sc\_port} and an \texttt{sc\_export}.
#+BEGIN_SRC cpp
#include "systemc"
#include "tlm.h"
#define BASE_ADDRESS 400
#define NUM_MEM_OPS  128 // How many transactions will the Initiator generate
#define MEM_SIZE     256 // The Target acts like a memory

using namespace std;
using namespace sc_core;
using namespace tlm;


struct Initiator: sc_module, tlm_bw_transport_if<>
{
    tlm_initiator_socket<> socket; 
    int data;

    SC_CTOR(Initiator) : socket("socket")
	{
	    socket.bind(*this);
	    SC_THREAD(run);
	}
#+END_SRC
The initiator will generate a series of transactions, that represent memory accesses.
Prior to initiating a transaction, the payload must be prepared.
#+BEGIN_SRC cpp
  void run()
  {
      tlm::tlm_generic_payload trans;
      sc_time delay;

      for(int i=0; i<NUM_MEM_OPS; i+=4)
      {
	  // Generate 4-byte aligned addresses
	  int address = BASE_ADDRESS + i;
	  // Generate a random command (either read or write)
	  tlm_command cmd = static_cast<tlm_command>(rand()%2);
	  // If write then put some junk as the data
	  if (cmd == TLM_WRITE_COMMAND) data = address;
	  // Set payload's field
	  trans.set_command(cmd);
	  trans.set_address( address );
	  trans.set_data_ptr( reinterpret_cast<unsigned char*>(&data) );
	  trans.set_data_length( 4 );
	  trans.set_streaming_width( 4 ); // not used
	  trans.set_byte_enable_ptr( 0 ); // not used
	  trans.set_dmi_allowed( false ); // not used
	  trans.set_response_status( tlm::TLM_INCOMPLETE_RESPONSE );
#+END_SRC
The Loosely-timed coding style uses the blocking transport interface.
Upon return, the transaction has been completed and delay may be accumulated.
#+BEGIN_SRC cpp
	  // Initiate Transaction
	  delay = SC_ZERO_TIME;
	  socket->b_transport( trans, delay );
	  // Transaction is completed

	  // Check if everything went good
	  if (trans.is_response_error())
	      SC_REPORT_ERROR("TLM-2", trans.get_response_string().c_str());

	  // Display payload
	  cout << name() << " completed " << (cmd ? "write" : "read") << ", addr = " << hex << i
	       << ", data = " << hex << data << ", time " << sc_time_stamp()
	       << " delay = " << delay << endl;

	  // Yield!
	  wait(delay);
      }
  }
#+END_SRC
The initiator inherits \texttt{tlm\_bw\_transport\_if<>}, so he must realize the following methods.
These methods are defined on the backward path (may be called by an interconnect or target component) and are only applicable with the AT coding style.
#+BEGIN_SRC cpp
    // TLM-2 backward non-blocking transport method
    virtual tlm::tlm_sync_enum nb_transport_bw( tlm::tlm_generic_payload& trans,
						tlm::tlm_phase& phase, sc_time& delay )
	{
	    // Dummy method
	    return tlm::TLM_ACCEPTED;
	}

    // TLM-2 backward DMI method
    virtual void invalidate_direct_mem_ptr(sc_dt::uint64 start_range,
					   sc_dt::uint64 end_range)
	{
	    // Dummy method
	}
}; //END_INITIATOR_DEFINITION
#+END_SRC
The target component must inherit \texttt{sc\_module} and realize the \texttt{tlm\_fw\_transport\_if} interface.
It must contain at least one target socket.
#+BEGIN_SRC cpp
struct Memory: sc_module, tlm::tlm_fw_transport_if<>
{
    int *mem;
    tlm::tlm_target_socket<> socket;


    SC_CTOR(Memory)
	: socket("socket")
	{
	    mem = new int[MEM_SIZE];
	    socket.bind(*this);

	    // Initialize memory with random data
	    for (int i = 0; i < MEM_SIZE; i++)
		mem[i] = rand() % 256;
	}
#+END_SRC
The most important method in a target component, in the LT coding style, is the \texttt{b\_transport}:
#+BEGIN_SRC cpp
    // TLM-2 blocking transport method
    virtual void b_transport( tlm_generic_payload& trans, sc_time& delay )
	{
	    tlm::tlm_command cmd = trans.get_command();
	    sc_dt::uint64    adr = trans.get_address();
	    unsigned char*   ptr = trans.get_data_ptr();
	    unsigned int     len = trans.get_data_length();
	    unsigned char*   byt = trans.get_byte_enable_ptr();
	    unsigned int     wid = trans.get_streaming_width();

	    // Obliged to check address range and check for unsupported features,
	    //   i.e. byte enables, streaming, and bursts
	    // Can ignore DMI hint and extensions
	    // Using the SystemC report handler is an acceptable way of signalling an error

	    if (adr >= sc_dt::uint64(MEM_SIZE) * 4 || adr % 4 || byt != 0 || len > 4 || wid < len)
		SC_REPORT_ERROR("TLM-2", "Target does not support given generic payload transaction");

	    // Obliged to implement read and write commands
	    if ( cmd == tlm::TLM_READ_COMMAND )
		memcpy(ptr, &mem[adr/4], len);
	    else if ( cmd == tlm::TLM_WRITE_COMMAND )
		memcpy(&mem[adr/4], ptr, len);

	    // Memory access time
	    delay = delay + sc_time(100, SC_NS);

	    // Obliged to set response status to indicate successful completion
	    trans.set_response_status( tlm::TLM_OK_RESPONSE );
	}
#+END_SRC
Since the target inherits the \texttt{tlm\_fw\_transport\_if<>} it must also realize the following methods.
These methods are only applicable with the AT coding style, so they are not properly implemented.
#+BEGIN_SRC cpp
    // TLM-2 forward non-blocking transport method
    virtual tlm::tlm_sync_enum nb_transport_fw( tlm::tlm_generic_payload& trans,
						tlm::tlm_phase& phase, sc_time& delay )
	{
	    // Dummy method (not TLM-2.0 compliant)
	    return tlm::TLM_ACCEPTED;
	}

    // TLM-2 forward DMI method
    virtual bool get_direct_mem_ptr(tlm::tlm_generic_payload& trans,
				    tlm::tlm_dmi& dmi_data)
	{
	    // Dummy method
	    return false;
	}

    // TLM-2 debug transport method
    virtual unsigned int transport_dbg(tlm::tlm_generic_payload& trans)
	{
	    // Dummy method
	    return 0;
	}
}; // END_TARGET_DEFINITION
#+END_SRC
Finally, the components are instantiated and their sockets are bound.
#+BEGIN_SRC cpp
int sc_main(int argc, char *argv[])
{
    Initiator proc("proc");
    Memory    mem("mem");

    proc.socket.bind( mem.socket );

    sc_start();
    return 0;
}
#+END_SRC

\clearpage

** MPI: The Pipeline Pattern                                         :ignore:
#+BEGIN_EXPORT latex
\subsection{MPI: The Pipeline Pattern}
\label{AppendixD}
#+END_EXPORT

Much like an assembly line, 
the pipeline pattern is useful when the problem at hand can be understood/modeled as a series of chained actions.
The magic happens by overlapping execution and communication.
Just think of how cars are manufactured.
The following example simulates the daily routine of an assembly line worker in a car factory.

As with any MPI program, the reader must keep in mind that, unless explicitly specified, all of the processes will get a copy of the *same executable*.
So as to differentiate control flow, the process must acquire its unique ID from the MPI runtime environment.
#+BEGIN_SRC cpp
#include "mpi.h"
#include <iostream>
#include <climits>
#include <unistd.h> 

int main(int argc, char *argv[])
{
    int myId, workerLeft, workerRight, numWorkers;
    bool isFirst(false), isLast(false);
    int completedTask(INT_MIN), nextTask(INT_MIN), taskAtHand(INT_MIN);

    // Register with the factory's administration
    MPI_Init(&argc, &argv);
    // What is my id
    MPI_Comm_rank(MPI_COMM_WORLD, &myId);
    // How many workers in the factory
    MPI_Comm_size(MPI_COMM_WORLD, &numWorkers);

#+END_SRC

Each assembly line worker must know who is on the left and who is on the right.
#+BEGIN_SRC cpp
    // Who is on my left
    if( myId != 0)
	workerLeft = myId - 1;
    else
    {
	// The first worker has access to the raw materials
	isFirst = true;
	taskAtHand = 0;
	nextTask = 0;
    }

    // Who is on my right
    if( myId != numWorkers-1)
	workerRight = myId + 1;
    else
    {
	isLast = true;
	workerRight = myId;	
    }
#+END_SRC


For each worker there is a task at hand, a new task coming from its left and the task he completed.
The communication is carried out by the conveyor belt, so it is not her/his concern.
She/He just places the completed task on the belt, takes the next task and continues working.
While the task at hand is being computed, communication is happening on the background.

After some initial delay, proportional to the size of the pipeline (the number of workers), cars are produced at a constant rate!
#+BEGIN_SRC cpp
    // Daily routine (or lifelong slavery?)
    while(1)
    {
	MPI_Request signals[2];
	MPI_Status  statuses[2];
	// Communicate but don't block
	if (!isLast)
	    MPI_Isend(&completedTask, 1, MPI_INT, workerRight, 1, MPI_COMM_WORLD, &signals[0]);
	if (!isFirst)
	    MPI_Irecv(&nextTask, 1, MPI_INT, workerLeft, 1, MPI_COMM_WORLD, &signals[1]);

	// Simulate Work
	taskAtHand++;
	sleep(1);

	// Work is done, prepare for communication
	completedTask = taskAtHand;
	taskAtHand    = nextTask;

	// This is meaningful only to the last worker
	if( completedTask == numWorkers)
	    std::cout << "Car is ready!" << std::endl;

	if (isFirst)
	    MPI_Wait(&signals[0], &statuses[0]);
	else if (isLast)
	    MPI_Wait(&signals[1], &statuses[1]);
	else
	    MPI_Waitall(2, signals, statuses);
    }

    MPI_Finalize();
    return 0;
}
#+END_SRC


An open source and gratis implementation of the MPI API is the [[https://www.open-mpi.org/][Open MPI]].
Under the assumption that an MPI implementation has been installed, the steps for compilation and execution look like:
#+BEGIN_SRC sh
mpic++ pipeline.cpp
mpirun -n 4 a.out
#+END_SRC

\clearpage

** Case Study 1: Airport Topology :ignore:
#+BEGIN_EXPORT latex
\subsection{Case Study 1: Airport Topology}
\label{AppendixC}
#+END_EXPORT

The following header file describes the airport topology presented in Figure [[fig:deadlock]]
#+BEGIN_SRC cpp
// FILE: topology.hpp
#ifndef TOPOLOGY_HPP
#define TOPOLOGY_HPP
#include <string>
enum  Airport {SKG, ARL, CDG};
const int lookahead[3][3] = { {0,3,2}, {3,0,1}, {2,1,0} };
const Airport airports[3] = {SKG, ARL, CDG};
const std::string airport_to_string[3] = { "SKG", "ARL", "CDG"};
const int indegree[3] = {2,2,2};
const int sources[3][2] = { {1,2}, {0,2},  {0,1} };
const int sourceweights[3][2] = { {1,1}, {1,1}, {1,1}};
#endif
#+END_SRC

The information is deserialized at runtime by \texttt{MPI\_Dist\_graph\_create\_adjacent}, 
which is responsible for creating the airport's "neighborhood".
Therefore, the fundamental communication primitive used, \texttt{MPI\_Neighbor\_allgather}, 
will have all the necessary information to perform the broadcast and collect (allgather) operation.
#+BEGIN_SRC cpp
// FILE: process.cpp
#include "process.hpp"
#include "topology.hpp"

Process::Process(int rank)
    : rank(rank),
      counter(0),
      num_neighbors(indegree[rank]),
      no_inbound_flights_in_links(true)
{
    MPI_Info info;
    MPI_Dist_graph_create_adjacent( MPI_COMM_WORLD, \
                                num_neighbors, \
                                sources[rank], \
                                sourceweights[rank], \
                              P  num_neighbors, \
                                sources[rank], \
                                sourceweights[rank], \
                                info, 1, &my_neighbors);
...
}
#+END_SRC

* Use Case                                                         :noexport:
** Cache Hierarchy Design
Caching shared data introduces a new problem becauPse 
the view of memory held by two different processors is through their individual caches
which without any additional precautions could end up seeing two different values.

This difficulty is generally referred to as the cache coherence problem.

Notice that the coherence problem exists because we habe both a global state, defined primarily by
the main memory, and a local state, defined by the individual caches, which are private to each processor
core.

Thus in a multicore where some level of caching may be shared, while some levels are private
the coherence problem still exists and must be solved.

Informall we could say that a memory system is coherent if any read of a data item returns
the most recently written value of that data item

A program running on multiple processors will normally have copies of the same data in several caches.

The protocols to maintain coherence for multiple processors are called cache coherence protocols
Key to implementing a cache coherence protocol is tracking the state of any sharing of a data block.
There are two classes of protocols in use, each of which uses different techniques to track the sharing status.

Directory based: The sharing status of a particular block of physical memory is kept in one location, called the directory.
There are two very different types of directory-based cache coherence.
In an SMP, we can use one centralized directory, associated with the memory or some other single serialization point,
such as the outermost cache in a multicore.

A directory keeps the state of every block that may be cached.
Information in the directory includes which caches have copies of the block,
whether it is dirty and so on.

The simplest directory implementations associate an entry in the directory with each memory block.
In such implementations, the amount of information is proportional to the product of the number of memory blocks
times the number of nodes.
This overhead is not a problem for multiprocessors with less than a few hundred processors
because the directory overhead with a reasonable block size will be tolerable.

For efficiency reasons, we also track the state of each cache block at the individual caches.

The states and transitions for the state machine at each cache are identical to what we used for the snooping cache,
although the actions on a a transition are slightly different.
The processes of invalidating and locating an exclusive copy of a data item are different,
since they both involve communication between the requesting node and the directory and between
the directory and one or more remote nodes.
In a snooping protocol, these two steps are combined through the use of a broadcast to all the nodes.

We can start with simple state diagrams that show the state transitions for an individual cache block
and the examine the state diagram for the directory entry corresponding to each block in memory.

Presented as UML state machine diagram

#+BEGIN_LATEX
\begin{tikzpicture}

  \umlbasicstate[name=invalid, fill=white, anchor=north]{invalid}
  \umlbasicstate[name=shared, right=12cm of invalid-body.north, anchor=north, fill=white]{shared}
  \umlbasicstate[name=modified, below left=4cm and 4.5cm of shared-body.south, fill=white]{modified}
  \umlstateinitial[above=1cm of invalid, name=initial]

  \umltrans{initial}{invalid}

  % Invalid transition  
  \umltrans[arg={CPU\_read/}, pos=0.7, anchor1=30, anchor2=150]{invalid}{shared}
  \umlVHtrans[anchor2=150, arg={CPU\_write/}, pos=1.6]{invalid}{modified}

  % Shared transitions
  \umltrans[anchor1=170, anchor2=10, arg={invalidate/}, pos=0.7]{shared}{invalid}
  \umlVHtrans[anchor1=245, anchor2=30, arg={CPU\_write\_hit/}, pos=1.5]{shared}{modified}
  \umlVHtrans[anchor1=280, anchor2=5, arg={CPU\_write\_miss/}, pos=1.5]{shared}{modified}
  \umltrans[pos=1.2, arg={CPU\_read\_miss || CPU\_read\_hit}, recursive=90|10|3cm, recursive direction=top to right]{shared}{shared}

  % Modified transitions
  \umlVHtrans[pos=0.5, arg={CPU\_read\_miss/}, anchor1=90, anchor2=190]{modified}{shared}
  %\umlVHtrans[arg={fetch/}, anchor1=70, anchor2=210]{modified}{shared}
  \umlHVtrans[pos=0.75, anchor1=175, anchor2=245, arg={fetch\_invalidate/}]{modified}{invalid}
  \umltrans[pos=2.2, arg={CPU\_write\_miss}, recursive=-20|280|2.3cm, recursive direction=right to bottom]{modified}{modified}
  \umltrans[pos=1.5, arg={CPU\_read\_hit || CPU\_write\_hit}, recursive=260|200|4cm, recursive direction=bottom to left]{modified}{modified}

\end{tikzpicture}
#+END_LATEX

In a directory-based protocol, the directory implements the other half of the coherence protocol.
A message sent to a directory causes two different types of actions:
updating the directory state and sending additional messages to satisfy the request.
The states in the directory represent the three standard states for a block;
unlike in a snooping scheme, however, the directory state indicates the state of all the cached copies
of a memory block, rather than for a single cache block.

The memory block may be uncached by any node, cached in multiple nodes and readable (shared), or
cached exclusively and writable in exactly one node.
In addition to the state of each block, the directory must track the set of nodes that have a copy
of a block; we use a set called Sharers to perform this function.

we use a set called Sharers to perform this function. 
In multiprocessors with fewer than 64 nodes (each of which may represent four to eight times as many processors),
this set is typically kept as a bit vector.
Directory requests need to update the set Sharers and also read the set to perform invalidations.

The directory receives three different requests: read miss, write miss, and data write-back.
The messages sent in response by the directory are shown in bold,
while the updating of the set Sharers is shown in bold italics.
Because all the stimulus messages are external, all actions are shown in gray.
Our simplified protocol assumes that some actions are atomic, such as requesting a value and sending
it to another node; a realistic implementation cannot use this assumption.

* Caches :noexport:

** Directory Based Cache Coherence
Avoid broadcast.

The absence of any centralized data structure that tracks the state of the caches is both the fundamental
advantage of a snooping-based scheme, since it allows it to be inexpensive, as well as its Achille's heel
when it comes to scalability.

The sharing status of a particular block of physical memory is kept in one location,
called the *directory*.
There are two very different types of directory-based cache coherence.
In an *SMP*, we can use one centralized directory, 
associated with the memory or some other _single serialization point_, such as the outermost cache in a multicore.

In a *DSM*, it makes no sense to have a single directory, since that would create a single point of contention
and make it difficult to scale to many multicore chips given the memory demands of multicores with eight or more cores.

A directory keeps the state of every block that may be cached.
Information in the directory includes:
    1. which caches (or collections of caches) have copies of the block
    2. whether it is dirty, and so on.

Within a multicore with a shared outermost cache (say, L3), it is *easy* to implement a directory scheme.
_Simply keep a bit vector of the size equal to the number of cores for each L3 block._
The bit vector indicates which private caches may have copies of a blockin L3, and invalidations are only sent to those caches.

This works perfectly for a single multicore if *L3 is inclusive*, 
and _this scheme is the one used in the Intel i7_.


*** Basics

Just as with snooping protocol, there are two primary operations that a directory protocol must implement:
    1. handling a read miss
    2. handling a write to a shared ( thus clean) cache block.

To implement these operatations, a directory must track the state of each cache block.
In a simple protocol, these states could be the following:
   1. *Shared:* One of more nodes have the block cached, and the value in memory is up to date (as well as in all the caches)
   2. *Uncached:* No node has a copy of the cache block.
   3. *Modified:* Exactly one node has a copy of the cache block, 
       and it has written the block, so the memory copy is out of date. 

In addition to tracking the state of each potentially shared memory block, 
we must track which nodes have copies of that block, 
since those copies will need to be invalidated on a write.

_The simplest way to do this is to keep a bit vector for each memory block._

We can also use the bit vector to keep track of the owner of the block when the block is in the exclusive state.
_For efficiency reasons, we also track the state of each cache block at the individual caches._

The states and transitions for the state machine at each cache are identical to what we used for the snooping cache,
although the actions on a transition are slightly different.
The processes of invalidating and locating an exclusive copy of a data item are different,
since the both involve communication between the requesting node and the directory 
and between the directory an one or more remote nodes.

In a snooping protocol, these two steps are combined through the use of a broadcast to all the nodes.



*** Coherence Messages

A catalog of the message types that may be sent between the processors and the directories
for the purpose of handling misses and maintaining coherence.

| Message Type        | Source         | Destination    | Message contents | Function of this message                                                                                            |
|---------------------+----------------+----------------+------------------+---------------------------------------------------------------------------------------------------------------------|
| 1. Read Miss        | Local cache    | Home directory | P, A             | Node P has a read miss at address A; request data and make P a read sharer                                          |
| 2. Write Miss       | Local cache    | Home directory | P, A             | Node P has a write miss at address A; request data and make P the exclusive owner                                   |
| 3. Invalidate       | Local cache    | Home directory | A                | Request to send invalidates to all remote caches that are caching the block at address A                            |
| 4. Invalidate       | Home directory | Remote cache   | A                | Invalidate a shared copy of data at address A                                                                       |
| 5. Fetch            | Home directory | Remote cache   | A                | Fetch the block at address A and sent it to its home directory; change the state of A in the remote cache to shared |
| 6. Fetch/invalidate | Home directory | Remote cache   | A                | Fetch the block at address A and send it to its home directory; invalidate the block in the cache                   |
| 7. Data value reply | Home directory | Local cache    | D                | Return a data value from the home memory                                                                            |
| 8. Data write-back  | Remote cache   | Home directory | A, D             | Write-back a data value for address A                                                                               |

- The first 3 messages are requests sent by the local node to the home.
- The 4 through 6 messages are messages sent to a remote node by the home 
  when the home needs the data to satisfy a read or write miss request.
- Data value replies are used to send a value from the home node back to the requesting node.
- Data value write-backs occur for two reasons: 
     a. When a block is replaced in a cache and must be written back to its home memory
     b. In reply to fetch or fetch/invalidate messages from the home.

_Writing back the data value whenever the block becomes shared_
simplifies the number of states in the protocol since 
     a. Any dirty block must be exclusive 
     b. Any shared block is always available in the home memory.
  

*** Protocol from the Cache's Side

The basic states of a cache block in a directory-based protocol are exactly like those in a snooping protocol.
Thus, we can start with simple state diagrams that show 
    1. The state transitions for *an individual cache block*
    2. The state for the *directory entry* corresponding to each block in memory.



*** Protocol from the Diretory's Side

A message sent to a directory  causes two different types of actions:
  1. Updating the directory state.
  2. Send additional messages to satisfy the request.  

The memory block may be 
  1. Uncached by any node, 
  2. Cached in multiple nodes and readable (shared).
  3. Cached exclusively and writable in exactly one node.

In addition to the state of each block, the directory must track the set of nodes that
have a copy of a block; we use a set called Sharers to perform this function.
_Directory requests need to update the set Sharers and also read the set to perform invalidations._

The directory receives three different requests: read miss, write miss, and data write-back.

_Our simplified protocol assumes that some actions are atomic, such as requesting a value and sending it to another node;
a realistic implementation cannot use this assumption_


** MESI
Adds the state *Exclusive* to the basic MSI protocol
to indicate when a cache block is resident only in a single cache but is clean.

If a block is in the *E* state, it can be written without generating any invalidates,
which optimizes the case where a block is read by a single cache before being written by that same cache.

Of course, when a *read miss* to a block in the *E* state occurs, the block must be changed
to the *S* state to maintain coherence.

Because all sugsequent accesses are snooped, it is possible to maintain the accuracy of this state.
In particular, if another processor issues a read miss, the state is changed from exclusive to shared.
The advantage of adding this state is that a subsequent write to a block in the exclusive state
by the same core need not acquire bus access or generate any invalidate, since the block is known to be
exclusively in this local cache; the processor merely changes the state to modified.

This state is easily added by using the bit that encodes the coherent state as an exclusive state
and using the dirty bit to indicate that a block is modified.

The Intel i7 uses a variant of a MESI protocol, called MESIF, which adds a state (Forward) to designate
which sharing processor should respond to a request. It is designed to enhance performance in distributed
memory organizations.

* Graveyard :noexport:
For amusement purposes only, the reader can regard her/his brain as a DES.
How does the human brain handles the relativistic nature of time; it infers total orderings for the events of reality.
Alas, human intuition is biased towards a deterministic understanding of the physical world.
Intellect, though, is (hopefully) much more capable!

Unfortunately the library of the or1ksim is not reentrant and thus does not allow multiple instances of the core
simulator to be executed in one address space. Historically all data is stored in global variables.

A common practice among modern system-level design tools/methodologies, like Intel's CoFluent Studio, is for the designer to construct two intermediate models;
An application model, that is the behavioral view of the system and 
a platform model, assembled using a component database of Processing Elements (PE, processors, hardware accelerators etc) and Communication Elements (CE, buses, interfaces etc).
The final step towards *system-level synthesis*, that is the transition from a behavioral to a structural model on the system level, is called system mapping;
the partitioning of the application to the elements of the platform.

The first evaluation metric of the proposed PDES implementation will be its performance against the reference SystemC kernel.
It will be measured by experimentation on the project's use case.
The simulation's size can be easily related to the duration of the simulation (in simulated time).
Another way of describing the simulation's size is through the conception of a formula involving the number of system processes, the number of links, the system's topology and the amount of events generated.
The accuracy of the simulation can be measured by the aggregate number of causality errors.
The detection of causality errors must be facilitated in a per process level and the aggregation shall be performed at the end of the simulation.
A concrete realization of the accuracy metric comes in the form of a counter each process increments whenever it executes an event with a time-stamp lower than its clock (the time-stamp of the last processed event).
Ideally, if the synchronization algorithms have been realized correctly, no causality errors should be detected.

Relating to the DE MoC (Section [[Discrete Event Model of Computation]]), state variable partition follows the partition of $mathds{E}$ defined by the set of processes.
A trivial realization of the above scenario is depicted in figure [[fig:causality_shared_state]]. 
Processes $p_1$ and $p_2$ are executing in parallel, while sharing the system's state variable $x$.
Events $e_1$ and $e_2$ are executed by $p_1$ and $p_2$ respectively. 
If we assume that in real time $e_2$ is executed before $e_1$, then we have implicitly broken causality, since $e_1$ might be influenced
by the value of $x$ that the execution of $e_2$ might have modified. Furthermore, one must observe that this kind of implicit interaction between $P_1$ and $P_2$ can not be expressed in a DE MoC. 
This is a meta-implication of the host platform's shared memory architecture.

#+CAPTION: Causality error caused by the sharing of the system's state variable $x$ by $P_1$ and $P_2$.
#+NAME: fig:causality_shared_state
[[file:Figures/causality_shared_state.png]]

TLM 2.0 API \cite{OpenSystemCInitiative2009} consists of the following features ([[fig:TLM_features]]):
- A set of core interfaces
  - A Blocking interface which is coupled with the *Loosely-Timed (LT)* coding style.
  - A non-blocking interface, which is coupled with the *Approximately-Timed (AT)* coding style.
  - The *Direct Memory Interface (DMI)* to enable an initiator to have direct access to a target's memory, bypassing the usual path through the interconnect components used by the transport interfaces. 
  - The *Debug transport interface* to allow an non-intrusive inspection of the system's state.
- The *global quantum* used by the *temporal decoupling* mechanism of the LT coding style, which facilitates faster simulations by reducing the number of context switches performed by the kernel. 
- Initiator and target *sockets* to denote the links (causal dependencies) between processes.
- The *generic payload* which supports the abstract modeling of memory-mapped buses.
- A set of *utilities*, in the form of pre configured sockets and interconnect components, to facilitate the rapid development of models.

#+CAPTION: TLM 2.0 use cases (adopted from \cite{OpenSystemCInitiative2009}).
#+NAME: fig:TLM_features
[[file:Figures/TLM_features.png]]

* Refinements :noexport:
+ Chapter Titles bigger, occupying half of the page
+ Increase a spacing between lines
+ Header for every page
+ Section about FSM, put footnotes
+ The "Ingo Paradox"
+ Maybe add a dedication page
+ What to do with this equation:
#+BEGIN_EXPORT latex
\begin{equation}
\bigcup_{i=1}^{n} P_i = \mathds{E} \text{and} P_i \cap P_j = \emptyset \text{where} i,j,n \in \mathbb{N}, i \neq j, n=|\mathbb{P}|
\end{equation}
#+END_EXPORT
+ Tikzify

* Completeness :noexport:
As an axiom of the DE MoC, the following statement is proposed:

\begin{equation} \label{eq:start}
\forall{e}  \in \mathds{E} . \exists e_{start} \in \mathds{E} .  e_{start} \sqsubset e
\end{equation}

Event $e_{start}$ can be understood as the first event of the simulation, which is neither the outcome of communication nor execution, but just occurs.
There is an intuitive way of "confirming" the validity of such an axiomatic proposition. 
The presence of an endogenous point of primordial singularity, can be also found in the prevailing cosmological model for the universe ("Big Bang").
Furthermore, the presence of another special event is assumed. Event $e_{end}$ can be understood as the simulation's termination:

\begin{equation} \label{eq:end}
\forall e \in \mathds{E} . \exists e_{end} \in \mathds{E} . e \sqsubset e_{end}
\end{equation}

With the introduction of $e_{start}$ and $e_{end}$, set $\mathds{E}$ becomes a *Bottomed Complete Partial Order* (BCPO).
Having set $\mathds{E}$ as a BCPO brings forth a further classification of execution and communication functions.
For example, scott-continuity
These properties along with assumption \ref{eq:end} can be used when reasoning about a simulation's termination.
A system that can be proven not to satisfy assumption \label{eq:end} is called a *Zeno model*[fn:zeno].


[fn:zeno] A quite typical example of a non terminating DE simulation is Zeno's \textit{"Achilles and the Tortoise"} paradox, hence the term "Zeno model"
\clearpage

* Epigraphs :noexport:
To see a World in a Grain of Sand 
And a Heaven in a Wild Flower 
Hold Infinity in the palm of your hand 
And Eternity in an hour

William Blake
