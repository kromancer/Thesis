#+TITLE:   Parallel Simulation of SystemC Loosely-Timed Transaction Level Models
#+AUTHOR:  Konstantinos Sotiropoulos
#+EMAIL:   kisp@kth.se
#+STARTUP: overview


* Abstract
Keywords: parallel discrete event simulation, conservative synchronization algorithms, transaction level models, SystemC TLM 2.0


* Maguire's Notes for Writing an Abstract 			   :noexport:
_1. What is the topic area?_
The vision of a connected and automated society, 
the Internet of Things era has promised,
is depending on the industry's ability 
to design novel and complex electronic systems,
while maintaining a short time to market.


_2. Short problem statement_
One of the first steps in the design of such systems is the in tandem simulation of hardware and software.
Transaction Level Models, expressed in the SystemC modeling language, can facilitate this co-simulation.
However, the sequential nature of the SystemC's Discrete Event simulation kernel is a limiting factor.
Poor simulation performance often constraints the scope and depth of the design decisions that can be evaluated.


_3. Why was this problem worth a Master's thesis project? Why no one else solved it yet?_
The increase in computing power, modern processing units deliver, is only available for applications that can expose parallel operations.
The major obstacle one faces, when trying to parallelize a simulation, is the preservation of causality; simulation events need to be processed in a chronological order.


_4. How did you solve the problem?_
It is the main objective of this thesis' project to demonstrate the feasibility of parallelizing the simulation of Transaction Level Models, outside SystemC's reference simulation environment.
The difficult task of achieving causal, yet parallel, processing of simulation events, is accomplished by using proper process synchronization mechanisms.
Our proposed implementation does not depend on the presence of a centralized simulation moderator. 
It is implemented using the Message Passing Interface 3.0 framework.



_5. Results/Conclusions/Consequences/Impact:_
   _What are your key results/conclusions?_
   _What will others do based upon your results?_
   _What can be done now that you have finished - that could not be done before your thesis project was completed?_

To demonstrate our approach and evaluate different process synchronization algorithms,
we use the model of a cache-coherent, symmetric multiprocessor based on the OpenRisc 1000 Instruction Set Simulator.
Our results indicate a significant speedup against the reference SystemC simulation.



* Acronyms 							   
| *ASIC*:  | Application Specific Integrated Circuit |
| *DE*:    | Discrete Event                          |
| *DES*:   | Discrete Event Simulator/Simulation     |
| *DMI*:   | Direct Memory Interface                 |
| *ES*:    | Electronic System                       |
| *ESLD*:  | Electronic System-Level Design          |
| *FPGA*:  | Field Programmable Gate Array           |
| *HDL*    | Hardware Description Language           |
| *HPC*:   | High Performance Computing              |
| *IC*     | Integrated Circuit                      |
| *MoC*:   | Model of Computation                    |
| *MPI*    | Message Passing Interface               |
| *MPSoC*: | Multicore System on Chips               |
| *OoO*:   | Out-of-Order                            |
| *PDES*:  | Parallel Discrete Event Simulation      |
| *SLDL*:  | System-Level Design Language            |
| *SMP*:   | Symmetric Multiprocessing               |
| *SoC*:   | System on Chip                          |
| *SR*:    | Synchronous Reactive                    |
| *TLM*:   | Transaction Level Modeling              |
| *CMB*:   | Chandy/Misra/Bryant algorithm           |
\clearpage





* Preface 							
This is a Master's Thesis project that will be carried out in Intel Sweden AB and is supervised by KTH's ICT department.
Mr. Bjorn Runaker (\texttt{bjorn.runaker@intel.com}) is the project's supervisor from the company's side, 
while professor [[https://people.kth.se/~ingo/][Ingo Sander]] (\texttt{ingo@kth.se}) and PhD student [[http://people.kth.se/~ugeorge/][George Ungureanu]] (\texttt{ugeorge@kth.se}) are the examiner and supervisor from KTH. 
The project begun on 2016-01-16 and will finish on 2016-06-30, as dictated by the contract of employment that I, Konstantinos Sotiropoulos a Master's student at the Embedded Systems program, have signed with the company.

The scope of this project has been mutually agreed on and dialectically determined between the company's needs and the institute's research agenda.
As Master's Thesis project, it will expose a scientific ground on which the engineering effort shall be rooted.
 
All the necessary equipment (software and hardware) has been kindly provided by the company.
The exact legal context that will apply to any software produced as a result of this project is yet to be determined, 
but will conform to the general context dictated by the documents already signed (documents' titles:  "Statement of Terms and Conditions of Fixed Term Employment" and "Employee Agreement") .
\clearpage


* Introduction
Felix, qui potuit rerum cognoscere causas...
(Happy, he who could capture the origins of things...)
Publio Virgilio Marone, Mantova 70 B.C.  Brindisi 19 B.C.

The aim of this chapter is to present the general context of the problem statement.
that is the engineering discipline of *Electronic System-Level Design (ESLD)*.

In unit [[The Design Process]] we provide a definition for the fundamental concepts of design, system, model and simulation.
In units [[Electronic Systems Design]] to [[Transaction-Level Model]], using Gajski and Kuhn's Y-Chart, we determine the concept of a Transaction-Level Model, as an instance in the engineering practice of Electronic System-Level Design (ESLD).
In unit [[SystemC and TLM]] we have a rudimentary look on SystemC's role in ESLD.
The structure of this document is given in unit [[Document Overview]].

** The Design Process
We define the process of *designing* as the engineering art of incarnating a desired functionality into a perceivable, thus concrete, artifact.
An engineering artifact is predominantly referred to as a *system*, 
to emphasize the fact that it can be viewed as a structured collection of components and that its behavior is a product of the interaction among its components.

Conceptually, designing implies a movement from abstract to concrete, fueled by the engineer's *design decisions*, incrementally adding implementation details.
This movement is also known as the *design flow* and can be facilitated by the creation of an arbitrary number of intermediate artifacts called models.
A *model* is thus an abstract representation of the final artifact. 
The design flow can be now semi-formally defined as a process of model refinement, with the ultimate model being the final artifact itself.
We use the term semi-formal to describe the process of model refinement, because to the best of our knowledge, 
such model semantics and algebras that would establish formal transformation rules and equivalence relations are far from complete \cite{Gajski2009}.

A desired property of a model is executability that is its ability to demonstrate portions of the final artifact's desired functionality in a controlled environment.
An *executable model*, allows the engineer to form hypotheses, conduct experiments on the model and finally evaluate design decisions.
It is now evident that executable models can firmly associate the design process with the scientific method.
The execution of a model is also known as *simulation* \cite{Editor2014}.


** Electronic Systems Design
An Electronic System (ES) provides a desired functionality, by manipulating the flow of electrons.
Electronic systems are omnipotent in every aspect of human activity; 
most devices are either electronic systems or have an embedded electronic system for their cybernisis.

The prominent way for visualizing the ES design/abstraction space is by means of the Y-Chart.
The concept was first presented in 1983 \cite{Gajski1983} and has been constantly evolving to capture and steer industry practices.
Figure \ref{fig:Y-Chart} presents the form of the Y-Chart found in \cite{Gajski2009}.

#+CAPTION: The Y-Chart (adopted from \cite{Gajski2009})
#+NAME: fig:Y-Chart
[[file:Figures/y-chart.png]]


The Y-Chart quantizes the design space into four levels of abstraction; system, processor, logic and circuit, represented as the four concentric circles.
For each abstraction level, one can use different ways for describing the system; behavioral, structural and physical.
These are represented as the three axises, hence the name Y-Chart.
Models can now be identified as points in this design space.

A typical design flow for an Integrated Circuit (IC) begins with a high-level behavioral model capturing the system's specifications and 
proceeds non-monotonically to a lower level structural representation, expressed as a netlist of, still abstract, components.
From there, Electronic Design Automation (EDA) tools will pick up the the task of reducing the abstraction of a structural model by translating the netlist of abstract components to a netlist of standard cells.
The nature of the standard cells is determined by the IC's fabrication technology (FPGA, gate-array or standard-cell ASIC).
Physical dimensionality is added by place and route algorithms, 
part of an EDA framework, 
signifying the exit from the design space, 
represented in the Y-Chart by the transition from the structural to the physical axis.

We have used the adjective non-monotonic to describe the design flow, because as a movement in the abstraction space, it is iterative; 
design \rightarrow test/verify \rightarrow redesign or proceed.
This cyclic nature of the design flow is implied by the errors the human factor introduces, under the lack of formal model transformation methodologies in the upper abstraction levels.
The term *synthesis* is therefore introduced to describe a monotonic movement from a behavioral to a structural model, or the realization of an upper level structural model using finer components.
We distinguish synthesis from the general case of the design flow, to disregard the testing and verification procedures.
Therefore, the term synthesis may indicate the presence, or the desire of having, an automated design flow.
Low-level synthesis is a reality modern EDA tools achieve, while high-level synthesis is still a utopia modern tools are converging to.



*** Notes for completing this section 				   :noexport:
Explain processor, logic, circuit, structural, behavioral, etc
Physical dimensionality added by automated place and route software.
geometrical positioning of the components on the silicon wafer
The final result is a from the chosen device technology (standard cell ASIC, gate array ASIC, FPGA)
Verification and validation 
High-Level synthesis


** System-Level Design
To meet the increasing demand for functionality, ES complexity, as expressed by their heterogeneity and their size, is increasing.
Terms like Systems on Chip (SoC) and Multi Processor SoC (MPSoC), used for characterizing modern ES, indicate this trend.
With abstraction being the key mental ability for managing complexity, the initiation of the design flow has been pushed to higher abstraction levels.
In the Y-Chart the most abstract level, depicted as the outer circle, is the system level.
At this level the distinction between hardware and software is a mere design choice thus *co-simulation of hardware and software* is one of the main objectives.
Thereby the term *system-level design* is used to describe design flows that enter the design space at this level.

A common practice among modern system-level design tools/methodologies, 
like Intel's CoFluent Studio \cite{citation}, 
is for the designer to construct two intermediate models;
An application model, that is the behavioral view of the system and 
a platform model, assembled using a component database of Processing Elements (PE, processors, hardware accelerators etc) and Communication Elements (CE, buses, interfaces etc).
The final step towards *system-level synthesis*, that is the transition from a behavioral to a structural model on the system level, is called system mapping;
the partitioning of the application to the elements of the platform.


** Transaction-Level Model
A *Transaction-Level Model* (TLM) can now be defined as the point in the Y-Chart where the structural axis meets the system abstraction level.
As mentioned in the previous unit, a TLM can be thought of as a platform model, or *virtual platform*, where an application can/is mapped \cite{Rigo2011}.
It is the model that facilitates co-simulation of hardware and software.
The notion of the transaction as an abstraction of communication will be clarified in [[SystemC & TLM 2.0]].

What are the pragmatic reasons that make the development of a virtual platform imperative?
To begin with, an increasing amount of an ES's functionality is becoming software based. 
Moreover, ES related companies are facing the economical pressure of reducing new products' time to market.
Thus, software engineers must be equipped with a virtual platform they can use for software development, early on in the design flow, without needing to wait for the actual silicon to arrive.


** SystemC and TLM
One fundamental question, for completing the presentation of ESLD, remains; How can executable models be expressed on the system level?
While maintaining the expressiveness of a Hardware Description Language (HDL), *SystemC* is meant to act as an *Electronic System Level Design Language* (ESLDL);
a language with which system-level models can be expressed.
It is implemented as a C++ class library, thus its main concern is to provide the designer with executable rather than EDA synthesizable models.
The language is maintained and promoted by Accellera (former Open SystemC Initiative OSCI) and has been standardized (IEEE 1666-2011 \cite{OpenSystemCInitiative2012}).

Why is SystemC regarded as Specific Domain Language (SDL)?
In what way does SystemC provide support for Transaction Level Modeling? Through the TLM 1.0 and 2.0 API.











** Document Overview
This unit be completed in the end
\clearpage



* Formulating The Problem Statement 
The aim of this chapter is to present a theoretical framework that will eventually lead to the formulation of the problem statement.
Picking up Ariadne's thread from the introduction, this chapter begins its journey by the fact that SystemC is an Electronic System-Level Design *Language* (ESLDL) for expressing system-level models.

In unit [[Models of Computation]] we link the concepts of operational semantics and Models of Computation (MoC) with that of the ESLDL.
In units [[The Discrete Event Model of Computation]] and [[The Discrete Event Simulation(or)]] the SystemC simulation engine or kernel is presented as an algorithm that realizes the operational semantics of a Discrete Event (DE) MoC.
Units [[Parallel Discrete Event Simulation(or)]] and [[Causality and Synchronization]] introduce the concept of Parallel Discrete Event Simulation (PDES) and present the fundamental causality hazards it introduces.
The prime concern of this thesis' is presented in a concise way in [[Problem statement]].
Unit [[Objectives]] introduces the objectives, that is the engineering endeavor of this project.

** Models of Computation
A *language* is a set of symbols, rules for combining them (its syntax), and rules for interpreting combinations of symbols (its semantics). 
Two approaches to semantics have evolved: denotational and operational.
*Operational semantics*, which dates back to Turing machines, gives the meaning of a language in terms of actions taken by some abstract machine. 
How the abstract machine in an operational semantics can behave is a feature of what we call the *Model of Computation (MoC)* \cite{Edwards1997}.
This definition implies that languages are not computational models themselves, but have underlying computational models \cite{Jantsch2005}.

How does the concept of a MoC fit specifically in ESLDLs?
Above all the engineer needs executable models.
Furthermore, an ESLDL describes an electronic artifact as a system; a (hierarchical) network of interacting components.
Therefore, a MoC is a collection of rules to define what constitutes a component and what are the semantics of execution, communication and concurrency of the abstract machine that will execute the model \cite{Jantsch2005} \cite{Editor2014}.
To ensure meaningful simulations, the MoC of the abstract machine that simulates a model must be equivalent with that of the abstract machine that will realize the system.

#+CAPTION: Categorization of three of the most explored MoCs: State Machine, Synchronous Dataflow and Discrete Event(adopted from \cite{Editor2014})
#+NAME: fig:MoCs
[[file:Figures/MoCs.pdf]]


** Discrete Event Model of Computation
The dominant MoC that underlies most industry standard HDLs (VHDL, Verilog, SystemC) is the *Discrete Event (DE)* MoC.
The components of a DE system are called *processes*.
In this context processes usually model the behavior and functionality of hardware entities.
The execution of processes is concurrent and the communication is achieved through *events*.
An event can be considered as a time-stamped value.

Concurrent execution does not imply parallel/simultaneous execution. 
The notion of *concurrency* is more abstract. 
Depending on a machine's computational resources, it can be realized as either parallel/simultaneous execution or as sequential interleaved execution.

Systems whose semantics are meant to be interpreted by a DE MoC, in order to be realizable, must have a *causal* behavior: they must process events in a chronological order, 
while any output events produced by a process are required to be no earlier in time than the input events that were consumed \cite{Editor2014}.
At any moment in real time, the model's time is determined by the last event processed.

In figure [[fig:MoCs]] one can observe that the DE MoC is also considered to be *Synchronous-Reactive (SR)*. 
This demonstrates the possibility of the MoC to "understand" entities with zero execution time, where output events are produced at the same time input events are consumed.
We can also extend/rephrase the previous definitions and say that Synchronous-Reactive MoCs are able to handle, in a causal way, systems where events happen at the same time, instantaneously.
The DE MoC handles the aforementioned situations by extending time-stamps(the notion of simulated time) with the introduction of delta delays (also referred to as cycles or micro-steps).
A delta delay signifies an infinitesimal unit of time and no amount of delta delays, if summed, can result in time progression.
A time-stamp is therefore represented as a tuple of values, $(t,n)$ where $t$ indicates the model time and $n$ the number of delta delays that have advanced at $t$.




** Discrete Event Simulation(or)
A realization of the DE abstract machine is called a *Discrete Event Simulator (DES)*.
SystemC's reference implementation of the DES is referred to as the *SystemC kernel* \cite{OpenSystemCInitiative2012}.

Concurrency of the system's processes is achieved through the co-routine mechanism (also known as co-operative multitasking). 
Processes execute without interruption. In a single core machine that means that only a single process can be running at any (real) time, 
and no other process instance can execute until the currently executing process instance has yielded control to the kernel.
A process shall not preempt or interrupt the execution of another process \cite{OpenSystemCInitiative2012}.

To avoid quantization errors and the non-uniform distribution of floating point values, time is expressed as an integer multiple of a real value referred to as the time resolution. 

The kernel maintains a *centralized event queue* that is sorted by time-stamp and knows which process is *running*, which are *runnable*, and which processes are waiting for events.
Runnable processes have had events to which they are sensitive triggered and are waiting for the running process to yield to the kernel so that they can be scheduled.
The kernel controls the execution order by selecting the earliest event in the event queue and making its time-stamp the current simulation time.
It then determines the process the event is destined for, and finds all other events in the event queue with the same time-stamp that are destined for the same process \cite{Black2010}.
The operation of the kernel is exemplified in listing \ref{alg:kernel}.

#+BEGIN_LATEX
\begin{algorithm}
\caption{SystemC event loop, adopted from \cite{Schumacher2010}}
\label{alg:kernel}
\begin{algorithmic}[1]

   \While{timed events to process exist}  \Comment{Simulation time progression}
      \State trigger events at that time
      \While {runnable processes exist}   \Comment{Delta cycle progression}
         \While {runnable processes exist}
	     \State run all triggered processes
             \State trigger all immediate notifications
         \EndWhile
         \State update values of changed channels
	 \State trigger all delta time events
       \EndWhile
       \State advance time to next event time
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX

*** Concepts mentioned that have not been adequately explained 	   :noexport:
co-routines; maybe show how to implement co-routines in pthreads?



** Parallel Discrete Event Simulation(or)
The previous section has made evident that the reference implementation of the SystemC kernel assumes sequential execution and therefore can not utilize modern massively parallel host platforms. 
The most logical step in achieving faster simulations is to realize concurrency, from interleaved process execution to simultaneous/parallel execution.
By assigning each process to a different processing unit of the host platform (core or hardware thread) we enter the domain of *Parallel Discrete Event Simulation (PDES)*.
After making the strategical decision that for improving a DE simulator's performance one must orchestrate parallel execution, 
the first tactical decision encountered
is whether to keep a single simulated time perspective, 
or distribute it among processes.

For PDES implementations that enforce global simulation time, the term *Synchronous PDES* has been coined in \cite{Schumacher2010}.
In Synchronous PDES, parallel execution of processes is performed within a delta cycle. 
With respect to Alg \ref{alg:kernel}, we can say that a Synchronous PDES parallelizes the execution of the innermost loop (line 4).
However, as we will see in later sections, this approach will bare no fruits in the simulation of TLM Loosely Timed simulations, since delta cycles are never triggered \cite{Chen2012}.
Therefore, we switch our interest in *Out-of-Order PDES (OoO PDES)* \cite{Chen2015}; 
allowing each process to have its own perception of simulated time, determined by the last event it received.




*** Specify "later sections" :noexport:




** Causality and Synchronization 
The distribution of simulation time opens Pandora's box. 
Protecting the OoO PDES from *causality errors* demands certain assumptions and the addition of complex implementation mechanisms.

The first source of causality errors arises when the system's state variables are not distributed, in a disjoint way, among the processes \cite{Fujimoto1990}.
A trivial realization of the above scenario is depicted in figure [[fig:causality_shared_state]]. Processes $P_1$ and $P_2$ are executing simultaneously, while sharing the system's state variable $x$.
Events $E_1$ and $E_2$ are executed by $P_1$ and $P_2$ respectively. If we assume that in real time $E_2$ is executed before $E_1$, then we have implicitly broken causality, since $E_1$ might be influenced
by the value of $x$ that the execution of $E_2$ might have modified. Furthermore, one must observe that this kind of implicit interaction between $P_1$ and $P_2$ can not be expressed in a DE MoC. 
This is a meta-implication of the host platform's shared memory architecture.

#+CAPTION: Causality error caused by the sharing of the system's state variable $x$ by $P_1$ and $P_2$.
#+NAME: fig:causality_shared_state
[[file:Figures/causality_shared_state.png]]

The second and most difficult to deal with source of causality errors is depicted in figure [[fig:causality_safe_events]]. 
Event $E_1$ affects $E_2$ by scheduling a third event $E_3$ which, for the sake of argument, modifies the state of $P_2$. 
This scenario necessitates sequential execution of all three events. 
Thus the fundamental problem in PDES, in the context of this scenario, becomes the question: how can we deduce that it is safe to execute $E_2$ in parallel with $E_1$, without actually executing $E_1$ \cite{Fujimoto1990}?
However, one must notice that the kind of interaction that yields this problematic situation is explicitly stated in the model.

#+CAPTION: Causality error caused by the unsafe execution of event $E_2$ (adopted from \cite{Fujimoto1990}).
#+NAME: fig:causality_safe_events
[[file:Figures/causality_safe_events.png]]

The last example makes evident the fact that the daunting task of preserving causality in the simulation is all about *process synchronization*.
For example, each process must be able to communicate to each of its peers (processes that is linked with) the message: 
"I will not send you any event before $t_1$, so you can proceed with processing any event you have with time-stamp $t_2$ where $t_2 < t_1$".

OoO PDES synchronization algorithms, with respect to how they deal with causality errors, have been classified into two categories: *conservative* and *optimistic* \cite{Fujimoto2015}.
Conservative mechanisms strictly avoid the possibility of any causality error ever occurring by means of model introspection and static analysis.
On the other hand, optimistic/speculative approaches use a detection and recovery approach: when causality errors are detected a rollback mechanism is invoked to restore the system.
An optimistic compared to a conservative approach will theoretically yield better performance in models where communication, thus the probability of causality errors, is below a certain threshold \cite{Fujimoto1990}.

Both groups present severe implementation difficulties.
For conservative algorithms, model introspection and static analysis tools might be very difficult to develop,
while the rollback mechanism of an optimistic algorithm may require complex entities, such as a hardware/software transactional memory \cite{Anane2015} .


** Problem statement
The prime concern of this project can now be stated;
an evaluation of the efficiency of existing conservative process synchronization algorithms when applied to the parallel simulation
of Loosely-Timed Transaction Level Models.


** Objectives
If the timing constraints stretched beyond the scope of a Master Thesis, 
the project's self-actualization would require the development/production of the following components (sorted in descending significance order):
1. At least two OoO PDE simulation mechanisms implementing proposed conservative synchronization algorithms.
2. A proof of concept application of the proposed mechanism, on a sufficiently parallel TLM model.
3. A static analysis/introspection tool for parsing the SystemC description of the model and extracting a pure representation in XML.
4. A code generation tool for realizing the model outside SystemC.
For the critical task of analyzing the model, identifying the processes and the links between them, we will follow ForSyDe SystemC's approach \cite{Hosein2012}.
Using SystemC's well defined API for module hierarchy (e.g. \texttt{get\_child\_objects()}), along with the introduction of meta objects, the system's structure can be
serialized at runtime, in the pre simulation phase of elaboration.

Given the time constraints, the primary focus falls on the first two objectives.
The automation and generality the tools could deliver will be emulated by manual and ad-hoc solutions.

_COMMENT:_ Your thesis' value (to external parties) depends highly on delivering point 4.

\clearpage


* Out of Order PDES with MPI
The goal of this chapter is to present two conservative process synchronization algorithms and give their implementation using the MPI API.

In units [[The Chandy/Misra/Bryant synchronization algorithm]] and [[On Demand Synchronization]] we present the conservative synchronization algorithms that will be evaluated.
In unit [[Semantics of point-to-point communication in MPI]] and [[MPI Communication Modes]] we present the semantics of the Message Passing Interface (MPI) communication primitives.
In unit [[MPI Realization of CMB]] we provide pseudo code for the realization of the CMB using the MPI communication primitives.
In unit [[Existing PDES]] we give an overview of prior art in the field of PDES in ESLD.


** The Chandy/Misra/Bryant synchronization algorithm
The first conservative synchronization algorithm that will be examined originate from the work of *Chandy/Misra/Bryant (CMB)* \cite{Bryant} \cite{Chandy1979}.
Listing \ref{alg:kernel} demonstrates how the algorithm deals with the fundamental dilemma presented in section [[Problem statement]], figure [[fig:causality_safe_events]].
Events arriving on each incoming link can be stored in a first-in-first-out (FIFO) queue.

#+BEGIN_LATEX
\begin{algorithm}
\caption{Process event loop, adopted from \cite{Fujimoto1999}}
\label{alg:initial_CMB}
\begin{algorithmic}[2]

   \While{process clock < some T}  
      \State \textbf{Block} until each incoming link queue contains at least one event
      \State remove event M with the smallest time-stamp from its queue.
      \State set clock = time-stamp(M)
      \State process event M
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX

This naive realization of the individual process' event loop, however, leads to deadlock situations like the one depicted in figure [[fig:deadlock]].
The queues placed along the red loop are empty, thus simulation has halted, even though there are pending events (across the blue loop).

#+CAPTION: adopted from \cite{Fujimoto1999}
#+NAME: fig:deadlock
[[file:Figures/Deadlock.png]]

The deadlock avoidance mechanism that lies in the core of the CMB algorithm can be demonstrated with the following example:
Let us assume that $P_3$ is at time 5.
Furthermore, let us assume that we have the *a priori* knowledge that $P_3$ has a minimum event processing time of 3 (simulated).
We will call this knowledge *lookahead*.
$P_3$ could create a *null event*, with no data value, but with a time-stamp $t$(8) = clock(5) + lookahead(3) and place it on its outgoing links.
A null event is still an event, so $P_2$ by processing it would advance its clock to 8.
In the same fashion, let us assume that $P_2$ has a lookahead of 2 and upon processing $P_3$'s null event, 
it will generate a null event for $P_1$ with time-stamp 10. 
Eventually $P_1$ can now safely process the actual event with time-stamp 9, thus unfreezing the simulation.

Thus, the modified, for deadlock avoidance, algorithm is described in listing \ref{alg:null-message}.
The important points one must notice with this deadlock avoidance mechanism are that:
- Null events are created when a process updates its clock, that is upon processing an event.
- Each process propagates null events on all of its outgoing links.
- The efficiency of this mechanism is highly dependent on the designer's ability to determine sufficiently large lookaheads. The lookahead is not necessary a fixed value. It can be a function of the process' state and/or the simulation time.

#+BEGIN_LATEX
\begin{algorithm}
\caption{Process event loop, with deadlock avoidance, adopted from \cite{Fujimoto1999}}
\label{alg:null-message}
\begin{algorithmic}[2]

   \While{process clock < some T}  
      \State \textbf{Block} until each incoming link queue contains at least one event
      \State remove event M with the smallest time-stamp from its queue.
      \State set clock = time-stamp(M)
      \State process event M
      \State send either a null or meaningful event to each outgoing link L with time-stamp = clock + Lookahead(clock,L,...)
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX


_COMMENT:_ This is a rather big unit. You should consider restructuring the material in a couple of shorter units. Are there any formal proofs about the properties (deadlock free, causality) of this algorithm? 



** On Demand Synchronization
The principal disadvantage of the CMB algorithm is that a large number of null events can be generated, particularly if the lookahead is small \cite{Fujimoto1999}.
An alternative approach to sending a null event after processing each event is a demand-driven approach.
Whenever a process is about to become blocked because an incoming link is empty, it requests an event (null or otherwise) from the process on the sending side of the link.
The process resumes execution when the response to this request is achieved.

_COMMENT:_ The description of this algorithm is not complete. 


** Semantics of point-to-point Communication in MPI
The framework chosen for implementing the PDES is the *Message Passing Interface* 3.0 (MPI).
Events are modeled as structured messages, while event diffusion/communication as message passing.
MPI is a message passing library interface specification, standardized and maintained by the Message Passing Interface Forum \cite{citation}.
It is currently available for C/C++, FORTRAN and Java from multiple vendors (Intel, IBM, OpenMPI) \cite{citation}.
MPI addresses primarily the message passing parallel programming model, 
in which data is moved from the address space of one process to that of another process through cooperative operations on each process \cite{MessagePassingInterfaceForum2012}.

The basic communication primitives are the functions \texttt{MPI\_Send(...)} and \texttt{MPI\_Recv(...)}.
Their arguments specify, among others things, a data buffer and the peer process' or processes' unique id assigned by the MPI runtime.
By default, message reception is blocking, while message transmission may or may not block.
One can think of message transfer as consisting of the following three phases
1. Data is pulled out of the send buffer and a message is assembled
2. A message is transferred from sender to receiver
3. Data is pulled from the incoming message and disassembled into the receive buffer

*Order:*
Messages are non-overtaking.
If a sender sends two messages in succession to the same destination, 
and both match the same receive (a call to \texttt{MPI\_Recv}), 
then this operation cannot receive the second message if the first one is still pending. 
If a receiver posts two receives in succession,
and both match the same message, 
then the second receive operation cannot be satisfied by this message, if the first one is still pending. 
This requirement facilitates matching of sends to receives and also guarantees that message passing code is deterministic.

*Fairness:*
MPI makes no guarantee of fairness in the handling of communication. 
Suppose that a send is posted. 
Then it is possible that the destination process repeatedly posts a receive that matches this send, 
yet the message is never received, 
because it is each time overtaken by another message, 
sent from another source. 
It is the programmer’s responsibility to prevent starvation in such situations.

_COMMENT:_ Why did you choose MPI?


** MPI Communication Modes
The MPI API contains a number of variants, or modes, for the basic communication primitives.
They are distinguished by a single letter prefix (e.g. \texttt{MPI\_Isend(...)}, \texttt{MPI\_Irecv(...)}).
As dictated by the MPI version 3.0, the following communication modes are supported \cite{MessagePassingInterfaceForum2012}:

*No-prefix for standard mode: \texttt{MPI\_Send(...)}*
In this mode, it is up to MPI to decide whether outgoing messages will be buffered. 
MPI may buffer outgoing messages. 
In such a case, the send call may complete before a matching receive is invoked. 
On the other hand, buffer space may be unavailable, or MPI may choose not to buffer outgoing messages, for performance reasons. 
In this case, the send call will not complete, blocking the transmitting process, until a matching receive has been posted, and the data has been moved to the receiver.

*B for buffered mode: \texttt{MPI\_Bsend(...)}* 
A buffered mode send operation can be started whether or not a matching receive has been posted. 
It may complete before a matching receive is posted. 
However, unlike the standard send, this operation is local, and its completion does not depend on the occurrence of a matching receive. 
Thus, if a send is executed and no matching receive is posted, then MPI must buffer the outgoing message, so as to allow the send call to complete. 
A buffered send operation that cannot complete because of a lack of buffer space is erroneous. 
When such a situation is detected, an error is signaled that may cause the program to terminate abnormally. 
On the other hand, a standard send operation that cannot complete because of lack of buffer space will merely block, 
waiting for buffer space to become available or for a matching receive to be posted. 
This behavior is preferable in many situations. 
Consider a situation where a producer repeatedly produces new values and sends them to a consumer. 
Assume that the producer produces new values faster than the consumer can consume them. 
If buffered sends are used, then a buffer overflow will result. 
Additional synchronization has to be added to the program so as to prevent this from occurring. 

*S for synchronous mode: \texttt{MPI\_Ssend(...)}*
A send that uses the synchronous mode can be started whether or not a matching receive was posted. 
However, the send will complete successfully only if a matching receive is posted, and the receive operation has started to receive the message sent by the synchronous send.
Thus, the completion of a synchronous send not only indicates that the send buffer can be reused, 
but it also indicates that the receiver has reached a certain point in its execution, 
namely that it has started executing the matching receive. 
If both sends and receives are blocking operations then the use of the synchronous mode provides synchronous communication semantics: 
a communication does not complete at either end before both processes *rendezvous* at the communication point.

*R for ready mode: \texttt{MPI\_Rsend(...)}*
A send that uses the ready communication mode may be started only if the matching receive is already posted. 
Otherwise, the operation is erroneous and its outcome is undefined.
Ready sends are an optimization when it can be guaranteed that a matching receive has already been posted at the destination.
On some systems, this allows the removal of a hand-shake operation that is otherwise required and results in improved performance. 
A send operation that uses the ready mode has the same semantics as a standard send operation, or a synchronous send operation; 
it is merely that the sender provides additional information to the system (namely that a matching receive is already posted), that can save some overhead. 

*I for non-blocking mode: \texttt{MPI\_Isend(...)}, \texttt{MPI\_Ibsend(...)}, \texttt{MPI\_Issend(...)} and \texttt{MPI\_Irecv(...)*
Non-blocking message passing calls return control immediately (hence the prefix I), 
but it is the user's responsibility to ensure that communication is complete, 
before modifying/using the content of the data buffer.
It is a complementary communication mode that works en tandem with all the previous.
The MPI API contains special functions for testing whether a communication is complete, or even explicitly waiting until it is finished.




** MPI realization of CMB
Listing \ref{alg:CMB_mpi} is a pseudo code, sketching out the CMB process event loop, using MPI's communication primitives.
#+BEGIN_LATEX
\begin{algorithm}
\caption{CMB Process event loop in MPI}
\label{alg:CMB_mpi}
\begin{algorithmic}[2]

   \While{process clock < some T}  
      \State post a MPI\_Irecv on each incoming peer process
      \State post a MPI\_Wait: block until every receive has been completed
      \State save each message received in a separate, per incoming link, FIFO.
      \State identify message M with the smallest time-stamp
      \State set clock = time-stamp(M)
      \State process message M
      \State post a MPI\_Issend to each outgoing link L with time-stamp = clock + Lookahead(clock,L,...)
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX


** Evaluation Metrics
The first evaluation metric of the proposed PDES implementation will be its performance against the reference SystemC kernel.
It will be measured by experimentation on the project's use case.

The simulation's size can be easily related to the duration of the simulation (in simulated time).
Another way of describing the simulation's size is through the conception of a formula involving the number of system processes, the number of links, the system's topology and the amount of events generated.

The accuracy of the simulation can be measured by the aggregate number of causality errors.
The detection of causality errors must be facilitated in a per process level and the aggregation shall be performed at the end of the simulation.
A concrete realization of the accuracy metric comes in the form of a counter each process increments whenever it executes an event with a time-stamp lower than its clock (the time-stamp of the last processed event).
Ideally, if the synchronization algorithms have been realized correctly, no causality errors should be detected.

_COMMENT:_ This section will become more concrete when we start experimentation.


** Existing PDES
The most important:
RISC: Recoding infrastructure for SystemC \cite{Liu2015}.

Miscellaneous:
SystemC-SMP \cite{Mello2010}
SpecC \cite{Domer2011}, although the latter is not meant for SystemC.
sc\_during \cite{Moy}

_COMMENT:_ This section is incomplete that should not be incomplete in an Intermediate report. 
Are you reinventing the wheel? 
Did you try at least one of these tools?
\clearpage


* SystemC TLM 2.0
It is beyond the scope of this project to provide a comprehensive guide to system-level modeling in SystemC TLM 2.0.
However, at the time of writing and to the best of our knowledge, we can not verify the existence of a comprehensive guide about system-level modeling with SystemC TLM 2.0.
Hence, we fill obliged to provide a quick introduction into the SystemC TLM 2.0 Loosely-Timed (LT) coding style, by means of a simple example.
The chapter assumes a basic understanding of C++ and SystemC.

In unit [[Overview of SystemC TLM 2.0 API]] we enumerate the features of the SystemC TLM 2.0 API.
In units [[Transactions, Sockets, Initiators and Targets]] and [[Generic Payload]] we have a look at the fundamental notions of transaction, initiator and target components, socket and generic payload.
In unit [[Coding Styles]] we present the two coding styles (Loosely Timed and Approximately Timed) and give their typical use cases.
In unit [[An Example]] we provide the implementation of a simple initiator, interconnect and target model.
In unit [[Criticism]] we present the dominant source of criticism for TLM 2.0.
Finally, in unit [[Simics and TLM 2.0]] we provide a comparison between the dominant industry frameworks for ESLD, Simics and SystemC TLM.

** Overview of SystemC TLM 2.0 API
As stated in unit [[Transaction Level Model]], a Transaction Level Model is considered a virtual platform where an application can/is mapped.
A *virtual platform* is a fully functional software model of a complete system, typically used for software development in the absence of hardware, or prior to hardware being available. 
To be suitable for productive software development it needs to be fast, booting operating systems in seconds, and accurate enough such that code developed using standard tools on the virtual platform will run unmodified on real hardware. \cite{Leupers2010}.

The TLM 2.0 API enhances SystemC's expressiveness in order to facilitate the description and fast simulation of virtual platforms.
TLM 2.0 allows *IP interoperability* for the rapid development of fast virtual platforms and facilitate the simulation under a reference simulation kernel, that of SystemC.

TLM 2.0 API \cite{OpenSystemCInitiative2009} consists of the following features ([[fig:TLM_features]]):
- A set of core interfaces
  - A Blocking interface which is coupled with the *Loosely-Timed (LT)* coding style.
  - A non-blocking interface, which is coupled with the *Approximately-Timed (AT)* coding style.
  - The *Direct Memory Interface (DMI)* to enable an initiator to have direct access to a target's memory, bypassing the usual path through the interconnect components used by the transport interfaces. 
  - The *Debug transport interface* to allow an non-intrusive inspection of the system's state.
- The *global quantum* used by the *temporal decoupling* mechanism of the LT coding style, which facilitates faster simulations by reducing the number of context switches performed by the kernel. 
- Initiator and target *sockets* to denote the links (causal dependencies) between processes.
- The *generic payload* which supports the abstract modeling of memory-mapped buses.
- A set of *utilities*, in the form of pre configured sockets and interconnect components, to facilitate the rapid development of models.

#+CAPTION: TLM 2.0 use cases (adopted from \cite{OpenSystemCInitiative2009}).
#+NAME: fig:TLM_features
[[file:Figures/TLM_features.png]]

*** Provide some clarification concerning the term IP :noexport:


** Transactions, Sockets, Initiators and Targets
*Transactions* are non-atomic communications, normally with bidirectional data transfer, and consist of a set of messages that are usually modeled as atomic communications.
In a transaction one can distinguish two roles;
the *initiator*, the component which initiated the communication, and the *target*, the component which is supposed to service the initiator's request.
A component is not limited to either of these two roles; it can assume both.
For example, *interconnect* components encapsulate the behavior of memory-mapped buses, being responsible for routing transactions to the correct target.
From the initiator's perspective, they act as targets and from the target's perspective they act as initiators.

Implementation-wise, communication in TLM 2.0 is reduced to method calls, 
from the initiator to the target through an arbitrary number of interconnect component, without involving any context switches from the simulation kernel.

A component's role is signified by the type of *sockets* it contains.
Initiator sockets are used to forward method calls "up and out of" a component, while target sockets are used to allow method calls "down and into" a component \cite{doulos}.
Socket binding is the act of connecting components together, thus defining the component whose method call will be eventually executed to service the transaction.
From SystemC's viewpoint, a socket is basically a convenience class, wrapping a sc_port and an sc_export.

Maybe explain in more detail SystemC's export and port mechanisms?
Maybe you need to adopt a more SystemC like terminology? For example change the word "component" to "module".


** Generic Payload
The basic argument that is passed, by reference, in communicative method calls is called the *generic payload*.
It is a structure that contains all the necessary information about the transaction.
It supports the abstract modeling of memory-mapped buses, together with an extension mechanism to support the modeling of specific bus protocols whilst maximizing interoperability.

The main features/fields of the generic payload are:
- Command 
  Is it read or write?
- Address
  What is the address, who is supposed to serve the transaction.
- Data
  A pointer to the physical data as an array of bytes.
- Phase
  Since a transaction is a non-atomic operation, this indicates the stage of the transaction. It is used for a detailed modeling of communication protocols.  
- Response
  An enumeration, indicating whether the transaction was successful, and if not, what is the nature of the error.




** Coding Styles
LT is suited for describing virtual platforms intended for software development.
However, where additional timing accuracy is required, typically for software performance estimation and architectural analysis use cases, the AT style is employed.
Virtual platforms typically do not contain many cycle-accurate models of complex components because of the performance impact. 

_COMMENT:_ This is a quite problematic section. You need to elaborate more, do not forget LT is on your thesis title. 


** An Example
This unit will provide a literate code listing for the model in figure [[fig:TLM_tutorial]]
#+CAPTION: A simple system-level model. The initiator, for example, could model a processor, the interconnect component a memory bus and the target a memory.
#+NAME: fig:TLM_tutorial
[[file:Figures/TLM_tutorial.png]]


** Criticism
The root problem with TLM 2.0 lies in the elimination of explicit channels, which were a key contribution in the early days of research on system-level design.
As most researchers agreed, the concept of separation of concerns was of highest importance, 
and for system-level design in particular, this meant the clear separation of computation (in behaviors or modules) and communication (in channels).
Regrettably, SystemC TLM 2.0 chose to implement communication interfaces directly as sockets in modules and this indifference between channels and modules thus breaks the assumption of communication being safely encapsulated in channels.
Without such channels, there is very little opportunity for safe parallel execution \cite{Liu2015}.

For the above reason some designers consider TLM 2.0 a step towards the wrong direction and revert back to TLM 1.0.
Do you agree with this trend? 
Maybe tell us the major difference with TLM 1.0?

This is why SystemC TLM 2.0 model needs to be *recoded* to allow parallel execution.
The recoding must reconstitute the separation of concerns between computation and communication.
A modification of just the kernel will not suffice.



** Simics and TLM 2.0
Everything you do with SystemC TLM 2.0 you can do with Simics.
Simics is the main alternative to SystemC TLM 2.0 for system-level design.
Can you briefly outline the differences between the two tools/frameworks?
Is Simics capable of PDES?
\clearpage


* Use Case
In this chapter we describe the transaction level model we are going to use for conducting our experimentation.
The purpose of the experimentation is twofold;
verify whether we achieve better faster simulation compared to the reference SystemC kernel and evaluate the proposed process synchronization algorithms.

** Platform modeling
A block diagram of the platform that will be modeled is seen in figure [[fig:Platform]].
The platform is a shared fmemory, cache-coherent, symmetric multiprocessor system based on the [[http://opencores.org/or1k/Or1ksim][OpenRisc 1000 Instruction Set Simulator]].
Cache coherence is enforced by a directory residing in the inclusive L2 cache.
Every component is/will be implemented in C/C++ and wrapped in SystemC modules using the TLM 2.0 API for communication. 
The exact number of processors is yet to be determined.

#+CAPTION: A model of a shared memory, cache-coherent, symmetric multiprocessor system
#+NAME: fig:Platform
[[file:Figures/platform.png]]


_COMMENT:_ Can you be more specific about the cache coherence protocol? Maybe provide a state diagram?


** Application modeling
We have the bare metal (newlib based) toolchain for compiling applications for the OpenRisc ISS.

_COMMENT;_ What kind of application am I going to run on this platform?
I see that most of the papers out there do some kind of mpeg2 decoding. That seems complex.






* References
\renewcommand\refname{}
\bibliography{References}
\bibliographystyle{myIEEEtran}












* Computer Science Cheatsheet 					   :noexport:
_Semantics:_ As a necessary propery of a modeling language whose models are meant to undergo
             synthesis and refinement. In order to have well-defined semantics, we need to
             introduce some form of formalism to models and modeling languages.

_NP problem:_ Non-deterministic Polynomial
              NP problems run in polynomial time on non-deterministic Turing machines
              A decision problem for which a "yes-answer" can be verified in polynomial time (by a deterministic Turing machine)

_NP hard problem:_    (With respect to the class of NP problems) 
                      If every NP problem can be *reduced* to it.

_NP complete problem:_ If it is NP and NP hard.

An _Algorithm_ is a finite description of a sequence of steps to be taken to solve a problem.
Physical processes are rarely structured as a sequence of steps; rather, they are structured as _continuous interactions between concurrent components_.

_Model vs Reality:_ You will never strike oil by drilling through the map (Golomb 1971)
_Concurrency vs Parallelism:_ Consider two "living" threads. On a multicore machine they might be executed in parallel.
On a single core the instructions of each thread are arbitrarily interleaved. In both cases the execution is these two 
threads is characterized as concurrent. Concurrency does not imply simultaneity.

_Chattering Zeno model:_ A moment in the simulation where execution is happening within delta time, not allowing the simulation time to progress.

_Zeno model:_ A model (like Achilles and the Turtle) where simulation time advances slower and slower until it reaches a point where 
it can not advance further(time increment becomes lower than the resolution) and gets trapped in delta time.

_A simulation_ is defined as the execution of model revealing the behaviour of the system being modeled.
A system can be analyzed either by being formally verified or simulated.
Simulation beyond analysis, as a means of constructing a virtual platform.

_A binary file:_ a statically linked library, a dynamically linked library, an object module, a standalone executable.
All binary files contain  meta information, such as the symbol table.

_False Sharing:_ The silent performance killer.
When cores communicate using "shared memory", they are often really just communicating through the cache coherence mechanisms.
A pathological case can occur when two cores access data that happens to lie in the same cache line. 
Normally, cache coherence protocols assign one core, the one that last modifies a cache line, to be the owner of that cache line. If two cores write to the same cache line repeatedly, they fight over ownership. 
Importantly, note that this can happen even if the cores are not writing to the same part of the cache line.
Write contention on cache lines is the single most limiting factor on achieving scalability for parallel threads of execution in an SMP system. \cite{McCool2012}em

_Design Automation_ depends on the high-level modelling and specification of systems.

_Reentrancy (vs Thread Safety):_ A subroutine is called *re-entrant* if it can be interrupted in the middle of its execution and then safely called again (re-entered, for example by the ISR) before its previous invocations complete execution.
*Recursive subroutines must be re-entrant*. A thread-safe code does not necessarily have to be re-entrant.
#+BEGIN_SRC C++
void thread_safe()
{
   acquire_lock
        if interrupted here and the ISR tries to re-enter we are fucked.
   release_lock
}
#+END_SRC

_A computer language:_ can be regarded the medium of communicating an algorithm to a machine.
We want the language to be expressive (like the greek language), portable (like the english language) and efficient (like the swedish)

_Data Parallelism:_ parallelism determined implicitly by data *independence*.

_Bash & C:_ brick and mortar


* RTL Cheatsheet 						   :noexport:
_RTL modules are pin-accurate:_ This means that the ports of an RTL module directly correspond to wires in the real-world implementation of the module. 

_RTL_design:_ The basis of RTL design is that circuits can be thought of 
              as a set of registers and 
              a set of transfer functions 
              defining the datapaths between registers.

_Stages of RTL design:_
(Remeber the dot product example)
1. Identify Data Operations:
2. Determine Type & Precision:
3. Determine Constraints on Data Processing Resources:
4. Allocation and Scheduling: Allocation reffers to the mappings of data operations onto processing resources.
                              Scheduling refers to the choice of clock cycle on which an operation will be performed in a multi-cycle operation.
                              Registers must also be allocated to all values that cross over from one clock cycle to a later one.
			      The aim is to maximize the resource usage and simultaneously to minimise the registers required to store intermediate results.
                              It is now possible to design the datapath minus its controller.

5. Controller Design:         Design a controller to sequence the operations over the eight clock cycles.
                              There are three multiplexers and a register to control in this circuit.
                              *Normally the controller would be implemented as a state machine*
                              
6. Reset Mechanism Design:

#+BEGIN_SRC vhdl
library ieee;
use ieee.std_logic_1164.all, ieee.numeric_std.all;

package dot_product_types is
   subtype sig8 is signed (7 downto 0);
   type sig8_vector is array (natural range <>) of sig8;
end;

library ieee;
use ieee.std_logic_1164.all, ieee.numeric_std.all;
use work.dot_product_types.all;
entity dot_product is
   port (a, b : in sig8_vector(7 downto 0);
   ck, reset: in std_logic;
   result : out signed(15 downto 0));
end;

architecture behaviour of dot_product is
   signal i : unsigned(2 downto 0);
   signal ai, bi : signed (7 downto 0);
   signal product, add_in, sum, accumulator : signed(15 downto 0);
begin
   control: process
   begin
     wait until rising_edge(ck);
     if reset = '1' then
        i <= (others => '0');
     else
        i <= i + 1;
     end if;
   end process;

   a_mux: ai <= a(to_integer(i));
   b_mux: bi <= b(to_integer(i));
   multiply: product <= ai * bi;
   z_mux: add_in <= X"0000" when i = 0 else accumulator;
   add: sum <= product + add_in;
   
   accumulate: process
   begin
     wait until rising_edge(ck);
     accumulator <= sum;
   end process;
   output: result <= accumulator;
end;
#+END_SRC


* Electronics Cheatsheet 					   :noexport:
_UART:_ The idle, no data state is high-voltage, or powered. 
This is a historic legacy from telegraphy, in which the line is held high to show that the line and transmitter are not damaged


* C++ 								   :noexport:
** Explicit threading in C++
#+BEGIN_SRC cpp
#include <thread>
#+END_SRC


** Introspection vs Reflection
Super important to check Qt.
Although it is a GUI thing, it has a DES (maybe PDES, each QThread runs its own event loop) and a Meta Object Compiler.


** Iterators
Iterators connect algorithms to the elements in a container regardless of the type of the container.
Iterators decouple the algorithm from the data source; an algorithm has no knowledge of the container form which the data originates. 


** Named Casts
1. static_cast: converts between related types 
                such as one pointer type to another in the same class hierarchy, 
                an integral type to an enumeration, or a floating-point type to an integral type

2. reinterpret_cast: handles conversions between unrelated types 
                     such as an integer to a pointer
                     or a pointer to an unrelated pointer type

3. const_cast:  converts between types that differ only in const and volatile qualifiers

4. dynamic_cast: does run-time checked conversion of pointers and references into a class hierarchy

*** Dynamic Cast
To use derived classes as more than a convenient shorthand in declarations, 
we must solve the following problem: 

_Given a pointer of type Base*, to which derived type does the object pointed to really belong?_

There are four fundamental solutions:
1. Ensure that only objects of a single type are pointed to.
2. Place a type field in the base class for the functions to inspect.
3. Use dynamic_cast
4. Use virtual functions

Consequently, the most obvious and useful operation for inspecting the type of an object at run time
is *a type conversion operation that returns a valid pointer if the object is of the expected type and a null pointer if it isn’t.* 
The dynamic_cast operator does exactly that.


** DANGER
#+BEGIN_SRC cpp
  class Base{
      void foo(){}
  };
  
  
  class Derived : public Base{
      void bar(){}
  };
  
  
  void dangerous(Base *p, int n){
      for(int i=0; i!=n; i++)
          p[i].foo();
  };
  
  
  void initiate_chaos(){
      Derived d[10];
      dangerous(d, 10);
  }
#+END_SRC


* MPI 								   :noexport:
** What is Blocking and Non-Blocking in MPI's context
The classification is with respect to whether the buffer involved in the communication primitive
is available for re-use in case of send or use in case of receive.

The 4 communication modes still apply for both categories.

_A *nonblocking send* call indicates_
that the system may start copying data out of the send buffer. 
The sender should not modify any part of the send buffer after a nonblocking send operation is called, 
until the send completes.

The completion of a send operation indicates that the sender is now free to update the locations in the send buffer 
It does not indicate that the message has been received, rather, 
it may have been buffered by the communication subsystem.

However, if a *synchronous mode* send was used, the completion of the send operation indicates 
that a matching receive was initiated, 
and that the message will eventually be received by this matching receive.


_A *nonblocking receive* call indicates_
that the system may start writing data into the receive buffer. 
The receiver should not access any part of the receive buffer after a nonblocking receive operation is called, until the receive completes.

The completion of a receive operation indicates that the receive buffer contains the received message, 
the receiver is now free to access it, and that the status object is set. 
It does not indicate that the matching send operation has completed (but indicates, of course, that the send was initiated).

** MPI_Status
The source or tag of a received message may not be known if wildcard values were used in the receive operation. 
Also, if multiple requests are completed by a single MPI function (see Section 3.7.5), a distinct error code may need to be returned for each request.

The status argument also returns information on the length of the message received.
However, this information is not directly available as a field of the status variable and a call to MPI_GET_COUNT is required to “decode” this information.


* SystemC 							   :noexport:
** General

*** Parsing the SystemC standard for occurences of the word kernel
Clause 4 of \cite{OpenSystemCInitiative2009} "_Elaboration and simulation semantics_", defines the behavior of the SystemC kernel
and is central to an understanding of SystemC.

The _execution_ of a SystemC application consists of _elaboration_ followed by _simulation_.
Elaboration results in the creation of the module hierarchy.
Elaboration involves the execution of application code, the public shell of the implementation, and the private kernel of the implementation.
Simulation involves the execution of the scheduler, part of the kernel, which in turn may execute processes within the application.

The purpose of the process macros is to _register the associated function with the kernel such that the scheduler can call back that member function during simulation_.

When a port is bound to a channel, the kernel shall call the member function register_port of the channel.

Simulation time is initialized to zero at the start of simulation and increases monotonically during simulation.
The physical significance of the integer value representing time within the kernel is determined by the simulation time resolution.

Since process instances execute without interruption, only a single process instance can be running at any one time,
and no other process instance can execute until the currently executing process instance has yielded control to the kernel.
_A process shall not pre-empt or interrupt the execution of another process._
_This is known as co-routine semantics or co-operative multitasking_

The SystemC sc_module class provides four routines that may be overridden, and they are executed at the boundaries of simulation.
These routines provide modelers with a place to put initialization and clean-up code that has no place to live.
For example, checking the environment, reading run-time configuration information and generating summary reports at the end of simulation.
#+BEGIN_SRC cpp :exports code
void before_end_of_elaboration(void);
void end_of_elaboration(void);
void start_of_simulation(void);
void end_of_simulation(void);
#+END_SRC

A thread of clocked thread process instance is said to be resumed when the kernel causes the process to continue execution,
starting with the statement immediately following the most recent call to function wait.

If the thread or clocked thread process executes the entire function body or executes a return statement and thus returns control to the kernel,
the associated function shall not be called again for that process instance. The process instance is then said to be terminated.

The function next_trigger does not suspend the method process instance; a method process cannot be suspended but always executes to completion before
returning control to the kernel.

The distinction between _suspend/resume_ and _disable/enable_ lies in the sensitivity of the target process during the period while it is suspended or disabled.
With _suspend_ the kernel keeps track of the sensitivity of the target process while it is suspended such that a relevant event notification or time-out 
while suspended would cause the process to become runnable immediately when resume is called.
With _disable_ the sensitivity of the target process is nullified while it is suspended such that the process is not made runnable by the call to enable, but only on the next
relevant event notification or time-out subsequent to the call to enable.

If a process kills itself, the statements following the call to kill shall not be executed again during the current simulation, and control shall return to the kernel.

_STOPPED AT OCCURENCE 44_


*** Parsing the SystemC standard for occurences of the phrase set of
Set of runnable processes
Set of update requests
Set of delta notifications
Set of time-outs
Set of timed notifications


*** Parsing the SystemC standard for occurences of the phrase simulation time
43/105:
Synchronization may be strong in the sense that the sequences of communication events
is precisely determined in advance, or weak in the sense that the sequence of communication events
is partially determined by the detailed timing of the individual processes.

Strong synchronization is easily implemented in SystemC using FIFOs or semaphores, allowing a completely
untimed modeling style where in principle simulation can run without advancing simulation time.

Untimed modeling in this sense is outside the scope of TLM 2.0. On the other hand, a fast virtual
platform model allowing multiple embedded software threads to run in parallel may use either strong or weak
synchronization. In this standard, the appropriate coding style for such a model is termed loosely-timed.


*** Port vs Export
The purpose of port and export bindings is to enable a port or export to _forward interface method calls made during simulation._
A port _requires_ the services defined by an interface.
An export _provides_ the services defined by an interface.

Forward path form initiator to target.
Backward path from target back to initiator.


*** TODO Parsing the SystemC standard for occurences of the phrase update phase 











SC_THREADs are not threads. They are coroutines.

Coroutines are subroutines that allow multiple entry points for suspending and resuming execution at certain locations.

SystemC does not offer real concurrency. It simulates concurrency using ...

The SystemC kernel implements cooperative scheduling where each SC_THREAD willingly relinquishes control to allow other SC_THREADs to execute.

In order to implement that cooperative scheduling strategy using coroutines, a threading library is used.


The scheduler advances simulation time to the time of the next event, 
then runs any processes due to run at that time of sensitive to that event.

Computations that take some time are usually modeled by instantaneous computations followed by a SystemC wait.

A _scheduler_ manages the threads by use of queues, such as READY, which contains all those that are ready to execute
and WAIT which contains threads waiting for events.

_Threads_ switch between READY and WAIT during simulation subject to event notification and time advances.

Events are delivered in an inner loop called _delta-cycle_ and simulation time advances in an outer loop _time-cycle_.


** Co-routine semantics
\cite{OpenSystemCInitiative2012}
Since process instances execute without interruption, only a single process instance can be running at any one time, 
and no other process instance can execute until the currently executing process instance has yielded control to the kernel.
A process shall not pre-empt or interrupt the execution of another process.
This is known as co-routine semantics or co-operative multitasking

An implementation running on a machine that provides hardware support for concurrent processes may permit two or more processes to run concurrently
provided that the behavior appears identical to the co-routine semantics defined in this subclause.
In other words, the implementation would be obliged to analyze any dependencies between processes and to constrain their execution to match the co-routine semantics.

Software modules that interact with one another as if they were performing I/O operations. (Conway 1963)

Co-routine semantics are linked to Kahn process networks.

*** Impediments to speed
_Context switching:_
- Every time you see a SC_THREAD -> _wait_ or a SC_METHOD -> _next_trigger() return_
- Complex bus protocols and lots of processes


** Dynamic processes with sc_spawn


** sc_elab_and_sim
sc_elab_and_sim is used to simplify the invocation of SystemC from a user-defined main() function.
If you do not have your own main(), you do not need sc_elab_and_sim


** sc_simcontext::crunch
This process implements the simulator's execution of processes.
It is a while(true) thing

sc_simcontext::crunch
sc_simcontext::simulate
sc_core::start
sc_main
sc_elab_and_sim
main


** sc_export
An export gives a structured way to express the fact that
a module provides an interface whose methods may be called from outside the module.
In a sense, an export is the opposite of a port.
Whereas a port allows interface method calls "up and out of" a module, an export allows interface method calls "down and into" a module.

An export should be bound to a channel or to another export in the constructor of the module in which it is declared.
Unlike a multiport, an export cannot be bound to more than one channel.

As for ports, you could create specialized classes derived from sc_export if you wanted to, 
but unlike sc_port, there are none built in to the SystemC class library.


* TLM 2.0 							   :noexport:
** General
A standardized way to connect models described at the untimed or approximately timed transaction level.
Instead of every vendor of system-level virtual platforms having their own proprietary languages, models and tools
every major developer of these platforms is now beginning to standardize on the use of TLM 2.0 as the way in which
to interconnect models or is planning to do so within their next development cycle.

Models developed for one system will be able to work on another, meaning that the problem of model availability and
true interoperability is now being solved. 

TLM 2.0 provides communications and timing capabilities that enable modeling at various levels of timing accuracy.

This chapter also demonstrates the transition that is going on in the industry: away from proprietary systems
and interfaces toward open standards.

platform-based development approach

An example of an extension is the TLM 2.0 library which creates additional communications capabilities
that mimic bus-based semantics. 
While this still remains within the discrete event MoC, it illustrates how additional semantics can be
built upon the base.

With the introduction of TLM 2.0 another huge barrier was removed, which was model interoperability.
SystemC does not define the semantics of communications between models as it only provides the essential primitives
necessary for communications. Thus there was no agreement in the industry about how these interfaces should
be constructed.

Several EDA vendors, such as CoWare, attempted to create and proliferate communications libraries,
but these saw no uptake because of the proprietary nature of them.
Today we are seeing rapid adoption of TLM 2.0 by the industry with significant support coming from 
all of the major EDA players.

\cite{Bailey2010}




#+BEGIN_LATEX
\tikzstyle{block} = [draw, fill=blue!4!white, rectangle, minimum height=3em, minimum width=6em]
\begin{figure}
\begin{tikzpicture}[auto, node distance=2cm]

\node [block] (payload) {Generic payload};
\node [block, right of=payload] (phases)  {Phases};
\node [block, below of=payload] (sockets) {Initiator and target sockets};
\node [block, below of=sockets] (tlm)     {TLM-2 core interfaces: 
                                               \begin{itemize}
					       \item {Blocking transport interface}
					       \item {Non-blocking transport interface}
					       \item {Direct memory interface}
					       \item {Debug transport interface}
					       \end{itemize}
					       };

\draw [->] (payload) -- (sockets);
\draw [->] (phases)  -- (sockets);
\draw [->] (sockets) -- (tlm);

\end{tikzpicture}\caption{TLM 2.0 Interoperability layer for bus modeling}
\end{figure}
#+END_LATEX




** Transaction
A transaction is an abstraction of communication.
Two way communication.


** Sockets
A socket combines a port with an export.
An _initiator socket_ is derived from class sc_port and has an sc_export. It has the port for the forward path and the export for the backward path.
An _target_socket_    is derived from class sc_export and has an sc_port ([[~/pSystemC/src/tlm_core/tlm_2/tlm_sockets/tlm_target_socket.h][tlm_base_target_socket]])

Only the most derived classes *tlm_initiator_socket* and *tlm_target_socket* are typically used directly by applications. 
These two sockets are parameterized with a protocol traits class that defines the types used by the forward and backward interfaces.
Sockets can only be bound together if they have the identical protocol type.


** Generic Payload
It supports the _abstract modeling of memory-mapped buses_, 
together with an extension mechanism to support the modeling of specific bus protocols whilst maximizing interoperability.

The main features of the generic payload are:
- Command 
  Is it read or write?
- Address
  What is the address
- Data
  A pointer to the physical data as an array of bytes
- Byte Enable Mask Pointer
- Response
  An indication of whether the transaction was successful, and if not the nature of the error

*** Streaming Width
In case of *multi-beat* transactions 
the ratio of the data length over the streaming width will give the number of beats. 


*** Byte Enable Mask Pointer
The elements in the byte enable array shall be interpreted as follows.
A value of 0 shall indicate that that corresponding byte is disabled, and a value of 0xff shall indicate that
the corresponding byte is enabled.
The meaning of all other values shall be undefined. 
The value 0xff has been chosen that the byte enable array can be used directly as a mask.


** Initiators and Targets
A module's processes may act as either initiators or targets.
An initiator is responsible for creating a payload and calling the transport function to send it.
A target receives payloads from the transport function for processing and response.
In the case of non-blocking interfaces the target may create new transactions backwards in response to a transaction from an initiator.
Initiator calls are made through initiator sockets, target calls received through target sockets.
A module may implement both target and initiator sockets, allowing its threads to both generate and receive traffic.


** Blocking, Non-Blocking, Debug and Interfaces/Transport Call
_How does TLM contribute to performance boost:_ You do 1 wait, rather than many waits.

With the blocking interface you can have wat() on the target code.

Why does the nb_transport_if defines 4 phases?
- To enable


** Direct Memory Interface
_Characteristics:_
- Allows direct backdoor access into memory
- *Allows un-inhibited ISS execution:* 
  (Instead of roaming through the hierarchy of a buss system-Fast software execution)


** Socket
In order to pass transactions between initiators and targets, TLM-2.0 uses sockets.
An initiator sends transactions out through an _initiator socket_, and a target receives incoming transactions through a _target socket_.
A socket is basically a convinience class, wrapping up a port and an export.

[[file:Figures/tlm_socket.png]]






** Blocking interface
This interface allows only two timing points to be associated with each transaction, 
corresponding to the call to and return from the blocking transport function.



** Loosely Timed Coding Style
Notes from Video Lecture: [[http://videos.accellera.org/tlm20tutorial/David_Black/player.html][David Black, XtremeEDA USA: TLM Mechanics]]					   
_FAST-NOT ACCURATE_ (In terms of timing?): Less detail means faster simulation. Less context switching means also faster simulation.
A fast, loosely-timed model is typically expected to use the _blocking transport interface_ the _DMI_ and _temporal decoupling_.
_Older terminology:_ UnTimed - Programmer's View
_Use Cases:_
- Early Software Development
_Characteristics:_
- Only sufficient timing detail to _boot O/S and run multi-core systems. It can express the modeling of _timers and _interrupts_
- Processes can run ahead of simulation time (_temporal decoupling_)
- Each transaction has _2 timing points_: begin and end
- Uses direct memory interface (_DMI_)

_Temporal decoupling:_
Each process runs ahead up to quantum boundary.
sc_time_stamp() advances in multiples of the quantum.
Deterministic communication requires explicit synchronization.

_DMI:_
When combined with temporal decoupling may lead to completely crappy situations.
The language neither the simulator do not protect the designer.
It is like a hole in the legal system.


** Approximately-timed
_ACCURATE_ (In terms of timing?)
_Older terminology:_ Cycle Accurate
_Use cases:_
- Architectural Analysis, Software Performance Analysis
- Hardware Verification


** Loosely-timed coding style and temporal decoupling
*Each process is allowed to run for a certain time slice or quantum before switching to the next, or instead may yield control when it reaches an explicit synchronization point.*

The quantum value represents a tradeoff between simulation speed and accuracy.

For a fine grained model, the overhead of event scheduling and process context switching becomes the dominant factor in simulation speed.
Therefore allowing a process to run ahead of the simulation time will speed up the simulation.
Until it needs to interact with another process, for example read or update a variable belonging to another process.

The processs that runs ahead of simulation time creates a time warp

Each process is responsible for determining whether it can run ahead of simulation time without breaking the functionality of the model.
When a process encounters an external dependency it has two choices: either force synchronization, 
which means yielding to allow all other processes to run as normal until simulation time catches up, or sample or update the current value and continue.

_DMI:_
When combined with temporal decoupling may lead to completely crappy situations.
The language neither the simulator do not protect the designer.
It is like a hole in the legal system.

Individual SystemC processes are permitted to run ahead in a local "time warp" without actually advancing simulation time
until they need to synchronize with the rest of the system.
Temporal decoupling can result in very fast simulation for certain systems because it increases the data and code locality and reduces scheduling overhead of the simulator.

*Each process is allowed to run for a certain time slice or quantum before switching to the next, or instead may yield control when it reaches an explicit synchronization point.*

The quantum value represents a tradeoff between simulation speed and accuracy.

For a fine grained model, the overhead of event scheduling and process context switching becomes the dominant factor in simulation speed.
Therefore allowing a process to run ahead of the simulation time will speed up the simulation.
Until it needs to interact with another process, for example read or update a variable belonging to another process.

The processs that runs ahead of simulation time creates a time warp

Each process is responsible for determining whether it can run ahead of simulation time without breaking the functionality of the model.
When a process encounters an external dependency it has two choices: either force synchronization, 
which means yielding to allow all other processes to run as normal until simulation time catches up, or sample or update the current value and continue.


** Debuggin the AT 2 phase example
*** Call stack when calling the constructor of a module
constructor of current module
constructor of top module
sc_main
sc_elab_and_sim
main








* SAX 								   :noexport:


* Graveyard of potentially usefull phrases 			   :noexport:
Form must follow function - Le Corbusier

Activities that lie in between the time span an idea became a product is design

_This chapter delves_ into the world of hardware-software codesign

something real and tangible

praxis

An MoC for describing the application at the system-level

Like a wagnerian leitmotif

Working in tandem

Often, we use the terms A and B interchangeably and in a haphazard manner.

An implementation running on a machine that provides hardware support for concurrent processes may permit two or more processes to run concurrently
provided that the behavior appears identical to the co-routine semantics defined in this subclause.
In other words, the implementation would be obliged to analyze any dependencies between processes and to constrain their execution to match the co-routine semantics.

An important limitation of SystemC regarding performance is that the reference implementation is sequential, 
and the official semantics, just like any other Discrete Event Simulator (henceforth DES), make parallel execution difficult.
Most existing work on parallelization of SystemC targets cycle-accurate simulation,
and would be inefficient on loosely timed systems since they cannot run in parallel processes that do not execute simultaneously \cite{Moy}.

\cite{Moy}
The SystemC standard allows this, "provided that the behavior appears identical to the co-routine semantics" \cite{OpenSystemCInitiative2012}
This implies two constraints on a parallel implementation:

- It should not change the order in which processes are allowed to be executed. 
  In particular, the simulated time imposes an order on the execution of processes.
  
An optimistic approach would relax this constraint having a violation detection and rollback mechanism to correct any violations afterwards.
Although this may seem to work with VHDL, with SystemC this is chaotic, since arbitrary C++ code and system calls.

- It should not introduce new race conditions.
  For example, two SystemC processes may safely execute x++ on a shared variable, but running two such processes in parallel cannot be allowed.
  The co-routine semantics of the SystemC kernel guarantee that there will be no race conditions.
  Evaluate-update paradigm

How to realize the DE MoC on top of completely heterogeneous HPC platform 


* Companies List 						   :noexport:
_Who wants to work in these companies?_
Mentor Graphics
Cadence
Synopsys
Tensilica

I would rather work at Doulos with John Ansley


* Latex Headers 						   :noexport:
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt,twoside]
#+LATEX_HEADER: \usepackage[paper=a4paper,dvips,top=1.5cm,left=1.5cm,right=1.5cm, foot=1cm,bottom=1.5cm]{geometry}
#+LATEX_HEADER: \renewcommand{\rmdefault}{ptm} 
#+LATEX_HEADER: \usepackage[scaled=.90]{helvet}
#+LATEX_HEADER: \usepackage{courier}
#+LATEX_HEADER: \usepackage{bookmark}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \pagestyle{fancy}
#+LATEX_HEADER: \usepackage[dvipsnames*,svgnames]{xcolor} 
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,fit,positioning,calc,shapes}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[swedish,english]{babel}
#+LATEX_HEADER: \usepackage{rotating}		
#+LATEX_HEADER: \usepackage{array}		
#+LATEX_HEADER: \usepackage{graphicx}	 
#+LATEX_HEADER: \usepackage{float}	
#+LATEX_HEADER: \usepackage{color}      
#+LATEX_HEADER: \usepackage{mdwlist}
#+LATEX_HEADER: \usepackage{setspace}   
#+LATEX_HEADER: \usepackage{listings}	
#+LATEX_HEADER: \usepackage{bytefield}  
#+LATEX_HEADER: \usepackage{tabularx}	
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{algpseudocode}	
#+LATEX_HEADER: \usepackage{dcolumn}	
#+LATEX_HEADER: \usepackage{url}	
#+LATEX_HEADER: \usepackage[perpage,para,symbol]{footmisc} 
#+LATEX_HEADER: \usepackage[all]{hypcap}
#+LATEX_HEADER: \definecolor{darkblue}{rgb}{0.0,0.0,0.3} %% define a color called darkblue
#+LATEX_HEADER: \definecolor{darkred}{rgb}{0.4,0.0,0.0}
#+LATEX_HEADER: \definecolor{red}{rgb}{0.7,0.0,0.0}
#+LATEX_HEADER: \definecolor{lightgrey}{rgb}{0.8,0.8,0.8} 
#+LATEX_HEADER: \definecolor{grey}{rgb}{0.6,0.6,0.6}
#+LATEX_HEADER: \definecolor{darkgrey}{rgb}{0.4,0.4,0.4}
#+LATEX_HEADER: \hyphenpenalty=15000 
#+LATEX_HEADER: \tolerance=1000
#+LATEX_HEADER: \newcommand{\rr}{\raggedright} 
#+LATEX_HEADER: \newcommand{\rl}{\raggedleft} 
#+LATEX_HEADER: \newcommand{\tn}{\tabularnewline}
#+LATEX_HEADER: \newcommand{\colorbitbox}[3]{%
#+LATEX_HEADER: \rlap{\bitbox{#2}{\color{#1}\rule{\width}{\height}}}\bitbox{#2}{#3}}
#+LATEX_HEADER: \newcommand{\red}{\color{red}}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}{-3.25ex\@plus -1ex \@minus -.2ex}{1.5ex \@plus .2ex} {\normalfont\normalsize\bfseries}}
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \renewcommand\subparagraph{\@startsection{subparagraph}{5}{\z@}{-3.25ex\@plus -1ex \@minus -.2ex}{1.5ex \@plus .2ex} {\normalfont\normalsize\bfseries}}
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \setcounter{tocdepth}{3}
#+LATEX_HEADER: \setcounter{secnumdepth}{5}
#+LATEX_HEADER: \renewcommand{\headrulewidth}{0pt}
#+LATEX_HEADER: \lhead{Konstantinos Sotiropoulos}
#+LATEX_HEADER: \chead{Ms Thesis Intermediate Report}
#+LATEX_HEADER: \rhead{\date{\today}}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \let\ps@plain\ps@fancy 
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \setlength{\headheight}{15pt}


