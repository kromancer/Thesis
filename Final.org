#+TITLE:   Parallel Simulation of SystemC Loosely-Timed Transaction Level Models
#+AUTHOR:  Konstantinos Sotiropoulos
#+EMAIL:   kisp@kth.se
#+STARTUP: overview


* Abstract
The vision of a connected and automated society, the Internet of Things era has promised,
is depending on the industry's ability to design novel and complex electronic systems,
while maintaining a short time to market.
One of the first steps in the design of such systems is the in tandem simulation of hardware and software.
Transaction Level Models, expressed in the SystemC modeling language, can facilitate this co-simulation.
However, the sequential nature of the SystemC's Discrete Event simulation kernel is a limiting factor.
Poor simulation performance often constraints the scope and depth of the design decisions that can be evaluated.

The increase in computing power, modern processing units deliver, is only available for applications that can expose parallel operations.
The major obstacle one faces, when trying to parallelize a simulation, is the preservation of causality; simulation events need to be processed in a chronological order.

It is the main objective of this thesis' project to demonstrate the feasibility of parallelizing the simulation of Transaction Level Models, outside SystemC's reference simulation environment.
The difficult task of achieving causal, yet parallel, processing of simulation events, is accomplished by using proper process synchronization mechanisms.
Our proposed implementation does not depend on the presence of a centralized simulation moderator. 
It is implemented using the Message Passing Interface 3.0 framework.
To demonstrate our approach and evaluate different process synchronization algorithms,
we use the model of a cache-coherent, symmetric multiprocessor based on the OpenRisc 1000 Instruction Set Simulator. Our results indicate a significant speedup against the reference SystemC simulation.

*Keywords:* parallel discrete event simulation, conservative synchronization algorithms, transaction level models, SystemC TLM 2.0


* Maguire's Notes for Writing an Abstract 			   :noexport:
_1. What is the topic area?_
The vision of a connected and automated society, 
the Internet of Things era has promised,
is depending on the industry's ability 
to design novel and complex electronic systems,
while maintaining a short time to market.


_2. Short problem statement_
One of the first steps in the design of such systems is the in tandem simulation of hardware and software.
Transaction Level Models, expressed in the SystemC modeling language, can facilitate this co-simulation.
However, the sequential nature of the SystemC's Discrete Event simulation kernel is a limiting factor.
Poor simulation performance often constraints the scope and depth of the design decisions that can be evaluated.


_3. Why was this problem worth a Master's thesis project? Why no one else solved it yet?_
The increase in computing power, modern processing units deliver, is only available for applications that can expose parallel operations.
The major obstacle one faces, when trying to parallelize a simulation, is the preservation of causality; simulation events need to be processed in a chronological order.


_4. How did you solve the problem?_
It is the main objective of this thesis' project to demonstrate the feasibility of parallelizing the simulation of Transaction Level Models, outside SystemC's reference simulation environment.
The difficult task of achieving causal, yet parallel, processing of simulation events, is accomplished by using proper process synchronization mechanisms.
Our proposed implementation does not depend on the presence of a centralized simulation moderator. 
It is implemented using the Message Passing Interface 3.0 framework.



_5. Results/Conclusions/Consequences/Impact:_
   _What are your key results/conclusions?_
   _What will others do based upon your results?_
   _What can be done now that you have finished - that could not be done before your thesis project was completed?_

To demonstrate our approach and evaluate different process synchronization algorithms,
we use the model of a cache-coherent, symmetric multiprocessor based on the OpenRisc 1000 Instruction Set Simulator.
Our results indicate a significant speedup against the reference SystemC simulation.



* Acronyms 							   
| *ASIC*:  | Application Specific Integrated Circuit |
| *DE*:    | Discrete Event                          |
| *DES*:   | Discrete Event Simulator/Simulation     |
| *DMI*:   | Direct Memory Interface                 |
| *ES*:    | Electronic System                       |
| *ESLD*:  | Electronic System-Level Design          |
| *FPGA*:  | Field Programmable Gate Array           |
| *HDL*    | Hardware Description Language           |
| *HPC*:   | High Performance Computing              |
| *IC*     | Integrated Circuit                      |
| *IP*     | Intellectual Property                   |
| *MoC*:   | Model of Computation                    |
| *MPI*    | Message Passing Interface               |
| *MPSoC*: | Multicore System on Chips               |
| *OoO*:   | Out-of-Order                            |
| *PDES*:  | Parallel Discrete Event Simulation      |
| *SLDL*:  | System-Level Design Language            |
| *SMP*:   | Symmetric Multiprocessing               |
| *SoC*:   | System on Chip                          |
| *SR*:    | Synchronous Reactive                    |
| *TLM*:   | Transaction Level Modeling              |
| *CMB*:   | Chandy/Misra/Bryant algorithm           |
\clearpage





* Preface 							
This is a Master's Thesis project that will be carried out in Intel Sweden AB and is supervised by KTH's ICT department.
Mr. Bjorn Runaker (\texttt{bjorn.runaker@intel.com}) is the project's supervisor from the company's side, 
while professor [[https://people.kth.se/~ingo/][Ingo Sander]] (\texttt{ingo@kth.se}) and PhD student [[http://people.kth.se/~ugeorge/][George Ungureanu]] (\texttt{ugeorge@kth.se}) are the examiner and supervisor from KTH. 
The project begun on 2016-01-16 and will finish on 2016-06-30, as dictated by the contract of employment that I, Konstantinos Sotiropoulos a Master's student at the Embedded Systems program, have signed with the company.

The scope of this project has been mutually agreed on and dialectically determined between the company's needs and the institute's research agenda.
As Master's Thesis project, it will expose a scientific ground on which the engineering effort shall be rooted.
 
All the necessary equipment (software and hardware) has been kindly provided by the company.
The exact legal context that will apply to any software produced as a result of this project is yet to be determined, 
but will conform to the general context dictated by the documents already signed (documents' titles:  "Statement of Terms and Conditions of Fixed Term Employment" and "Employee Agreement") .
\clearpage


* Introduction
The aim of this chapter is to present the general context of the problem statement;
that is the engineering discipline of *Electronic System-Level Design (ESLD)*.

In unit [[The Design Process]] we provide a definition for the fundamental concepts of design, system, model and simulation.
In units [[Electronic Systems Design]] to [[Transaction-Level Model]], using Gajski and Kuhn's Y-Chart, we determine the concept of a Transaction-Level Model, as an instance in the engineering practice of Electronic System-Level Design (ESLD).
In unit [[SystemC and TLM]] we have a rudimentary look on SystemC's role in ESLD.
Unit [[Motivation]] gives the raison d'\'etre of this project.
The structure of this document is given in unit [[Document Overview]].


** The Design Process
We define the process of *designing* as the engineering art of incarnating a desired functionality into a perceivable, thus concrete, artifact.
An engineering artifact is predominantly referred to as a *system*, 
to emphasize the fact that it can be viewed as a structured collection of components and that its behavior is a product of the interaction among its components.

Conceptually, designing implies a movement from abstract to concrete, fueled by the engineer's *design decisions*, incrementally adding implementation details.
This movement is also known as the *design flow* and can be facilitated by the creation of an arbitrary number of intermediate artifacts called models.
A *model* is thus an abstract representation of the final artifact. 
The design flow can be now semi-formally defined as a process of model refinement, with the ultimate model being the final artifact itself.
We use the term semi-formal to describe the process of model refinement, because to the best of our knowledge, 
such model semantics and algebras that would establish formal transformation rules and equivalence relations are far from complete \cite{Gajski2009}.

A desired property of a model is executability that is its ability to demonstrate portions of the final artifact's desired functionality in a controlled environment.
An *executable model*, allows the engineer to form hypotheses, conduct experiments on the model and finally evaluate design decisions.
It is now evident that executable models can firmly associate the design process with the scientific method.
The execution of a model is also known as *simulation* \cite{Editor2014}.


** Electronic Systems Design
An Electronic System (ES) provides a desired functionality, by manipulating the flow of electrons.
Electronic systems are omnipotent in every aspect of human activity; 
most devices are either electronic systems or have an embedded electronic system for their cybernisis.

The prominent way for visualizing the ES design/abstraction space is by means of the Y-Chart.
The concept was first presented in 1983 \cite{Gajski1983} and has been constantly evolving to capture and steer industry practices.
Figure \ref{fig:Y-Chart} presents the form of the Y-Chart found in \cite{Gajski2009}.

#+CAPTION: The Y-Chart (adopted from \cite{Gajski2009})
#+NAME: fig:Y-Chart
[[file:Figures/y-chart.png]]


The Y-Chart quantizes the design space into four levels of abstraction; system, processor, logic and circuit, represented as the four concentric circles.
For each abstraction level, one can use different ways for describing the system; behavioral, structural and physical.
These are represented as the three axises, hence the name Y-Chart.
Models can now be identified as points in this design space.

A typical design flow for an Integrated Circuit (IC) begins with a high-level behavioral model capturing the system's specifications and 
proceeds non-monotonically to a lower level structural representation, expressed as a netlist of, still abstract, components.
From there, Electronic Design Automation (EDA) tools will pick up the the task of reducing the abstraction of a structural model by translating the netlist of abstract components to a netlist of standard cells.
The nature of the standard cells is determined by the IC's fabrication technology (FPGA, gate-array or standard-cell ASIC).
Physical dimensionality is added by place and route algorithms, 
part of an EDA framework, 
signifying the exit from the design space, 
represented in the Y-Chart by the transition from the structural to the physical axis.

We have used the adjective non-monotonic to describe the design flow, because as a movement in the abstraction space, it is iterative; 
design \rightarrow test/verify \rightarrow redesign or proceed.
This cyclic nature of the design flow is implied by the errors the human factor introduces, under the lack of formal model transformation methodologies in the upper abstraction levels.
The term *synthesis* is therefore introduced to describe a monotonic movement from a behavioral to a structural model, or the realization of an upper level structural model using finer components.
We distinguish synthesis from the general case of the design flow, to disregard the testing and verification procedures.
Therefore, the term synthesis may indicate the presence, or the desire of having, an automated design flow.
Low-level synthesis is a reality modern EDA tools achieve, while high-level synthesis is still a utopia modern tools are converging to.



*** Notes for completing this section 				   :noexport:
Explain processor, logic, circuit, structural, behavioral, etc
Physical dimensionality added by automated place and route software.
geometrical positioning of the components on the silicon wafer
The final result is a from the chosen device technology (standard cell ASIC, gate array ASIC, FPGA)
Verification and validation 
High-Level synthesis


** System-Level Design
To meet the increasing demand for functionality, ES complexity, as expressed by their heterogeneity and their size, is increasing.
Terms like Systems on Chip (SoC) and Multi Processor SoC (MPSoC), used for characterizing modern ES, indicate this trend.
With abstraction being the key mental ability for managing complexity, the initiation of the design flow has been pushed to higher abstraction levels.
In the Y-Chart the most abstract level, depicted as the outer circle, is the system level.
At this level the distinction between hardware and software is a mere design choice thus *co-simulation of hardware and software* is one of the main objectives.
Thereby the term *system-level design* is used to describe design flows that enter the design space at this level.

A common practice among modern system-level design tools/methodologies, 
like Intel's CoFluent Studio \cite{citation}, 
is for the designer to construct two intermediate models;
An application model, that is the behavioral view of the system and 
a platform model, assembled using a component database of Processing Elements (PE, processors, hardware accelerators etc) and Communication Elements (CE, buses, interfaces etc).
The final step towards *system-level synthesis*, that is the transition from a behavioral to a structural model on the system level, is called system mapping;
the partitioning of the application to the elements of the platform.



** Transaction-Level Model
A *Transaction-Level Model* (TLM) can now be defined as the point in the Y-Chart where the structural axis meets the system abstraction level.
As mentioned in the previous unit, a TLM can be thought of as a *virtual platform*, where an application can be mapped \cite{Rigo2011}.
It is a fully functional software model of a complete system that facilitates *co-simulation of hardware and software*.

There are three pragmatic reasons that stimulate the development of a transaction level model.
At first, a TLM serves as a testbed for *architectural exploration* in order to tune the overall system architecture prior to detailed design.
Secondly, software engineers must be equipped with a virtual platform they can use for *software development*, early on in the design flow, without needing to wait for the actual silicon to arrive.
The need for performing software and hardware development in parallel, is due to the facts that an increasing amount of an ES's functionality is becoming software based and ES related companies are facing the economical pressure of reducing new products' time to market.
Finally, a TLM can be a reference model for hardware *functional verification*, that is, a golden model to which an RTL implementation can be compared.


** SystemC and TLM
One fundamental question, for completing the presentation of ESLD, remains; How can executable models be expressed on the system level?
While maintaining the expressiveness of a Hardware Description Language (HDL), *SystemC* is meant to act as an *Electronic System Level Design Language* (ESLDL); a language where both RTL and system-level models can be expressed.
It is implemented as a C++ class library, thus its main concern is to provide the designer with executable rather than synthesizable models.
The language is maintained and promoted by Accellera (former Open SystemC Initiative OSCI) and has been standardized (IEEE 1666-2011 \cite{OpenSystemCInitiative2012}).

A major part of SystemC is the TLM 2.0 library, which is exactly meant for expressing TLMs.
Despite introducing different language constructs, TLM 2.0 is still a part of SystemC because it depends on the same simulation engine.
TLM 2.0 has been standardized seperately in \cite{OpenSystemCInitiative2009}.
Compared to a RTL simulation, where communication is realized through a number of pin level events trigerring context switches inside the simulator, TLM 2.0 uses a single function call, thus speeding up simulation by orders of magnitude, at the expense of accuracy.



** TODO Motivation
Faster simulation



** Document Overview
This unit be completed in the end
\clearpage



* Formulating The Problem Statement 
The aim of this chapter is to present a theoretical framework that will eventually lead to the formulation of the problem statement.
Picking up Ariadne's thread from the introduction, this chapter begins its journey by the fact that SystemC is an Electronic System-Level Design *Language* (ESLDL) for expressing system-level models.

In unit [[Models of Computation]] we link the concepts of operational semantics and Models of Computation (MoC) with that of the ESLDL.
In units [[The Discrete Event Model of Computation]] and [[The Discrete Event Simulation(or)]] the SystemC simulation engine or kernel is presented as an algorithm that realizes the operational semantics of a Discrete Event (DE) MoC.
Units [[Parallel Discrete Event Simulation(or)]] and [[Causality and Synchronization]] introduce the concept of Parallel Discrete Event Simulation (PDES) and present the fundamental causality hazards it introduces.
The prime concern of this thesis' is presented in a concise way in [[Problem statement]].
Unit [[Objectives]] introduces the objectives, that is the engineering endeavor of this project.

** Models of Computation
A *language* is a set of symbols, rules for combining them (its syntax), and rules for interpreting combinations of symbols (its semantics). 
Two approaches to semantics have evolved: denotational and operational.
*Operational semantics*, which dates back to Turing machines, gives the meaning of a language in terms of actions taken by some abstract machine. 
How the abstract machine in an operational semantics can behave is a feature of what we call the *Model of Computation (MoC)* \cite{Edwards1997}.
This definition implies that languages are not computational models themselves, but have underlying computational models \cite{Jantsch2005}.

How does the concept of a MoC fit specifically in ESLDLs?
Above all the engineer needs executable models.
Furthermore, an ESLDL describes an electronic artifact as a system; a (hierarchical) network of interacting components.
Therefore, a MoC is a collection of rules to define what constitutes a component and what are the semantics of execution, communication and concurrency of the abstract machine that will execute the model \cite{Jantsch2005} \cite{Editor2014}.
To ensure meaningful simulations, the MoC of the abstract machine that simulates a model must be equivalent with that of the abstract machine that will realize the system.

#+CAPTION: Categorization of three of the most explored MoCs: State Machine, Synchronous Dataflow and Discrete Event(adopted from \cite{Editor2014})
#+NAME: fig:MoCs
[[file:Figures/MoCs.pdf]]


** Discrete Event Model of Computation
The dominant MoC that underlies most industry standard HDLs (VHDL, Verilog, SystemC) is the *Discrete Event (DE)* MoC.
The components of a DE system are called *processes*.
In this context processes usually model the behavior and functionality of hardware entities.
The execution of processes is concurrent and the communication is achieved through *events*.
An event can be considered as a time-stamped value.

Concurrent execution does not imply parallel/simultaneous execution. 
The notion of *concurrency* is more abstract. 
Depending on a machine's computational resources, it can be realized as either parallel/simultaneous execution or as sequential interleaved execution.

Systems whose semantics are meant to be interpreted by a DE MoC, in order to be realizable, must have a *causal* behavior: they must process events in a chronological order, 
while any output events produced by a process are required to be no earlier in time than the input events that were consumed \cite{Editor2014}.
At any moment in real time, the model's time is determined by the last event processed.

In figure [[fig:MoCs]] one can observe that the DE MoC is also considered to be *Synchronous-Reactive (SR)*. 
This demonstrates the possibility of the MoC to "understand" entities with zero execution time, where output events are produced at the same time input events are consumed.
We can also extend/rephrase the previous definitions and say that Synchronous-Reactive MoCs are able to handle, in a causal way, systems where events happen at the same time, instantaneously.
The DE MoC handles the aforementioned situations by extending time-stamps(the notion of simulated time) with the introduction of delta delays (also referred to as cycles or micro-steps).
A delta delay signifies an infinitesimal unit of time and no amount of delta delays, if summed, can result in time progression.
A time-stamp is therefore represented as a tuple of values, $(t,n)$ where $t$ indicates the model time and $n$ the number of delta delays that have advanced at $t$.




** Discrete Event Simulation(or)
A realization of the DE abstract machine is called a *Discrete Event Simulator (DES)*.
SystemC's reference implementation of the DES is referred to as the *SystemC kernel* \cite{OpenSystemCInitiative2012}.

Concurrency of the system's processes is achieved through the co-routine mechanism (also known as co-operative multitasking). 
Processes execute without interruption. In a single core machine that means that only a single process can be running at any (real) time, 
and no other process instance can execute until the currently executing process instance has yielded control to the kernel.
A process shall not preempt or interrupt the execution of another process \cite{OpenSystemCInitiative2012}.

To avoid quantization errors and the non-uniform distribution of floating point values, time is expressed as an integer multiple of a real value referred to as the time resolution. 

The kernel maintains a *centralized event queue* that is sorted by time-stamp and knows which process is *running*, which are *runnable*, and which processes are waiting for events.
Runnable processes have had events to which they are sensitive triggered and are waiting for the running process to yield to the kernel so that they can be scheduled.
The kernel controls the execution order by selecting the earliest event in the event queue and making its time-stamp the current simulation time.
It then determines the process the event is destined for, and finds all other events in the event queue with the same time-stamp that are destined for the same process \cite{Black2010}.
The operation of the kernel is exemplified in listing \ref{alg:kernel}.

#+BEGIN_LATEX
\begin{algorithm}
\caption{SystemC event loop, adopted from \cite{Schumacher2010}}
\label{alg:kernel}
\begin{algorithmic}[1]

   \While{timed events to process exist}  \Comment{Simulation time progression}
      \State trigger events at that time
      \While {runnable processes exist}   \Comment{Delta cycle progression}
         \While {runnable processes exist}
	     \State run all triggered processes
             \State trigger all immediate notifications
         \EndWhile
         \State update values of changed channels
	 \State trigger all delta time events
       \EndWhile
       \State advance time to next event time
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX

*** Concepts mentioned that have not been adequately explained 	   :noexport:
co-routines; maybe show how to implement co-routines in pthreads?



** Parallel Discrete Event Simulation(or)
The previous section has made evident that the reference implementation of the SystemC kernel assumes sequential execution and therefore can not utilize modern massively parallel host platforms. 
The most logical step in achieving faster simulations is to realize concurrency, from interleaved process execution to simultaneous/parallel execution.
By assigning each process to a different processing unit of the host platform (core or hardware thread) we enter the domain of *Parallel Discrete Event Simulation (PDES)*.
After making the strategical decision that for improving a DE simulator's performance one must orchestrate parallel execution, 
the first tactical decision encountered
is whether to keep a single simulated time perspective, 
or distribute it among processes.

For PDES implementations that enforce global simulation time, the term *Synchronous PDES* has been coined in \cite{Schumacher2010}.
In Synchronous PDES, parallel execution of processes is performed within a delta cycle. 
With respect to Alg \ref{alg:kernel}, we can say that a Synchronous PDES parallelizes the execution of the innermost loop (line 4).
However, as we will see in later sections, this approach will bare no fruits in the simulation of TLM Loosely Timed simulations, since delta cycles are never triggered \cite{Chen2012}.
Therefore, we switch our interest in *Out-of-Order PDES (OoO PDES)* \cite{Chen2015}; 
allowing each process to have its own perception of simulated time, determined by the last event it received.




*** Specify "later sections" :noexport:




** Causality and Synchronization 
The distribution of simulation time opens Pandora's box. 
Protecting the OoO PDES from *causality errors* demands certain assumptions and the addition of complex implementation mechanisms.

The first source of causality errors arises when the system's state variables are not distributed, in a disjoint way, among the processes \cite{Fujimoto1990}.
A trivial realization of the above scenario is depicted in figure [[fig:causality_shared_state]]. Processes $P_1$ and $P_2$ are executing simultaneously, while sharing the system's state variable $x$.
Events $E_1$ and $E_2$ are executed by $P_1$ and $P_2$ respectively. If we assume that in real time $E_2$ is executed before $E_1$, then we have implicitly broken causality, since $E_1$ might be influenced
by the value of $x$ that the execution of $E_2$ might have modified. Furthermore, one must observe that this kind of implicit interaction between $P_1$ and $P_2$ can not be expressed in a DE MoC. 
This is a meta-implication of the host platform's shared memory architecture.

#+CAPTION: Causality error caused by the sharing of the system's state variable $x$ by $P_1$ and $P_2$.
#+NAME: fig:causality_shared_state
[[file:Figures/causality_shared_state.png]]

The second and most difficult to deal with source of causality errors is depicted in figure [[fig:causality_safe_events]]. 
Event $E_1$ affects $E_2$ by scheduling a third event $E_3$ which, for the sake of argument, modifies the state of $P_2$. 
This scenario necessitates sequential execution of all three events. 
Thus the fundamental problem in PDES, in the context of this scenario, becomes the question: how can we deduce that it is safe to execute $E_2$ in parallel with $E_1$, without actually executing $E_1$ \cite{Fujimoto1990}?
However, one must notice that the kind of interaction that yields this problematic situation is explicitly stated in the model.

#+CAPTION: Causality error caused by the unsafe execution of event $E_2$ (adopted from \cite{Fujimoto1990}).
#+NAME: fig:causality_safe_events
[[file:Figures/causality_safe_events.png]]

The last example makes evident the fact that the daunting task of preserving causality in the simulation is all about *process synchronization*.
For example, each process must be able to communicate to each of its peers (processes that is linked with) the message: 
"I will not send you any event before $t_1$, so you can proceed with processing any event you have with time-stamp $t_2$ where $t_2 < t_1$".

OoO PDES synchronization algorithms, with respect to how they deal with causality errors, have been classified into two categories: *conservative* and *optimistic* \cite{Fujimoto2015}.
Conservative mechanisms strictly avoid the possibility of any causality error ever occurring by means of model introspection and static analysis.
On the other hand, optimistic/speculative approaches use a detection and recovery approach: when causality errors are detected a rollback mechanism is invoked to restore the system.
An optimistic compared to a conservative approach will theoretically yield better performance in models where communication, thus the probability of causality errors, is below a certain threshold \cite{Fujimoto1990}.

Both groups present severe implementation difficulties.
For conservative algorithms, model introspection and static analysis tools might be very difficult to develop,
while the rollback mechanism of an optimistic algorithm may require complex entities, such as a hardware/software transactional memory \cite{Anane2015} .


** Problem statement
The prime concern of this project can now be stated;
an evaluation of the efficiency of existing conservative process synchronization algorithms when applied to the parallel simulation
of Loosely-Timed Transaction Level Models.


** Objectives
If the timing constraints stretched beyond the scope of a Master Thesis, 
the project's self-actualization would require the development/production of the following components (sorted in descending significance order):
1. At least two OoO PDE simulation mechanisms implementing proposed conservative synchronization algorithms.
2. A proof of concept application of the proposed mechanism, on a sufficiently parallel TLM model.
3. A static analysis/introspection tool for parsing the SystemC description of the model and extracting a pure representation in XML.
4. A code generation tool for realizing the model outside SystemC.
For the critical task of analyzing the model, identifying the processes and the links between them, we will follow ForSyDe SystemC's approach \cite{Hosein2012}.
Using SystemC's well defined API for module hierarchy (e.g. \texttt{get\_child\_objects()}), along with the introduction of meta objects, the system's structure can be
serialized at runtime, in the pre simulation phase of elaboration.

Given the time constraints, the primary focus falls on the first two objectives.
The automation and generality the tools could deliver will be emulated by manual and ad-hoc solutions.

_COMMENT:_ Your thesis' value (to external parties) depends highly on delivering point 4.

\clearpage


* Out of Order PDES with MPI
The goal of this chapter is to present two conservative process synchronization algorithms and give their implementation using the MPI API.

In units [[The Chandy/Misra/Bryant synchronization algorithm]] and [[On Demand Synchronization]] we present the conservative synchronization algorithms that will be evaluated.
In unit [[Semantics of point-to-point communication in MPI]] and [[MPI Communication Modes]] we present the semantics of the Message Passing Interface (MPI) communication primitives.
In unit [[MPI Realization of CMB]] we provide pseudo code for the realization of the CMB using the MPI communication primitives.
In unit [[Existing PDES]] we give an overview of prior art in the field of PDES in ESLD.


** The Chandy/Misra/Bryant synchronization algorithm
The first conservative synchronization algorithm that will be examined originates from the work of *Chandy/Misra/Bryant (CMB)* \cite{Bryant} \cite{Chandy1979}.
Listing \ref{alg:kernel} demonstrates how the algorithm deals with the fundamental dilemma presented in section [[Problem statement]], figure [[fig:causality_safe_events]].
Events arriving on each incoming link can are stored in a first-in-first-out (FIFO) queue.

#+BEGIN_LATEX
\begin{algorithm}
\caption{Process event loop, adopted from \cite{Fujimoto1999}}
\label{alg:initial_CMB}
\begin{algorithmic}[2]

   \While{process clock < some T}  
      \State \textbf{Block} until each incoming link queue contains at least one event
      \State remove event M with the smallest time-stamp from its queue.
      \State set clock = time-stamp(M)
      \State process event M
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX

This naive realization of the individual process' event loop, however, leads to deadlock situations like the one depicted in figure [[fig:deadlock]].
The queues placed along the red loop are empty, thus simulation has halted, even though there are pending events (across the blue loop).

#+BEGIN_SRC ditaa  :file Figures/deadlock.png :cmdline -S --font "Times New Roman"
+--------------+
|Process 1     |
|              |----------------------+
|              |                      |
|             9|<------------------+  |
|   ?          |                   |  |
+--------------+                   |  |
    ^  |                           |  |
    |  |                           |  |
    |  |                           |  |
    |  |                           |  |
    |  |                           |  |
    |  |                           |  |
    |  |                           |  :
    :  v                           |  v
+--------------+              +--------------+
|     |15|     |              |---+   5      |
|     |--|     |------------->|9|8|          |
|     |10|     |              |---+          |
|     +--+    ?|<=------------|              |
|Process 2     |              |Process 3     |
+--------------+              +--------------+
#+END_SRC

#+RESULTS:
[[file:Figures/deadlock.png]]

#+CAPTION: adopted from \cite{Fujimoto1999}
#+NAME: fig:deadlock
[[file:Figures/Deadlock.png]]

The deadlock avoidance mechanism that lies in the core of the CMB algorithm can be demonstrated with the following example:
Let us assume that $P_3$ is at time 5.
Furthermore, let us assume that we have the *a priori* knowledge that $P_3$ has a minimum event processing time of 3 (simulated).
We will call this knowledge *lookahead*.
$P_3$ could create a *null event*, with no data value, but with a time-stamp $t$(8) = clock(5) + lookahead(3) and place it on its outgoing links.
A null event is still an event, so $P_2$ by processing it would advance its clock to 8.
In the same fashion, let us assume that $P_2$ has a lookahead of 2 and upon processing $P_3$'s null event, 
it will generate a null event for $P_1$ with time-stamp 10. 
Eventually $P_1$ can now safely process the actual event with time-stamp 9, thus unfreezing the simulation.

Thus, the modified, for deadlock avoidance, algorithm is described in listing \ref{alg:null-message}.
The important points one must notice with this deadlock avoidance mechanism are that:
- Null events are created when a process updates its clock, that is upon processing an event.
- Each process propagates null events on all of its outgoing links.
- The efficiency of this mechanism is highly dependent on the designer's ability to determine sufficiently large lookaheads. The lookahead is not necessary a fixed value. It can be a function of the process's state and/or the simulation time.

#+BEGIN_LATEX
\begin{algorithm}
\caption{Process event loop, with deadlock avoidance, adopted from \cite{Fujimoto1999}}
\label{alg:null-message}
\begin{algorithmic}[2]

   \While{process clock < some T}  
      \State \textbf{Block} until each incoming link queue contains at least one event
      \State remove event M with the smallest time-stamp from its queue.
      \State set clock = time-stamp(M)
      \State process event M
      \State send either a null or meaningful event to each outgoing link L with time-stamp = clock + Lookahead(clock,L,...)
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX


_COMMENT:_ This is a rather big unit. You should consider restructuring the material in a couple of shorter units. Are there any formal proofs about the properties (deadlock free, causality) of this algorithm? 



** On Demand Synchronization
The principal disadvantage of the CMB algorithm is that a large number of null events can be generated, particularly if the lookahead is small \cite{Fujimoto1999}.
An alternative approach to sending a null event after processing each event is a demand-driven approach.
Whenever a process is about to become blocked because an incoming link is empty, it requests an event (null or otherwise) from the process on the sending side of the link.
The process resumes execution when the response to this request is achieved.

_COMMENT:_ The description of this algorithm is not complete. 


** Semantics of point-to-point Communication in MPI
_There is a problem here: There are two sections. Semantics of Nonblocking and Blocking communications in the MPI manual_

The framework chosen for implementing the PDES is the *Message Passing Interface* 3.0 (MPI).
Events are modeled as structured messages, while event diffusion/communication as message passing.
MPI is a message passing library interface specification, standardized and maintained by the Message Passing Interface Forum \cite{citation}.
It is currently available for C/C++, FORTRAN and Java from multiple vendors (Intel, IBM, OpenMPI) \cite{citation}.
MPI addresses primarily the message passing parallel programming model, 
in which data is moved from the address space of one process to that of another process through cooperative operations on each process \cite{MessagePassingInterfaceForum2012}.

The basic communication primitives are the functions \texttt{MPI\_Send(...)} and \texttt{MPI\_Recv(...)}.
Their arguments specify, among others things, a data buffer and the peer process' or processes' unique id assigned by the MPI runtime.
By default, message reception is blocking, while message transmission may or may not block.
One can think of message transfer as consisting of the following three phases
1. Data is pulled out of the send buffer and a message is assembled
2. A message is transferred from sender to receiver
3. Data is pulled from the incoming message and disassembled into the receive buffer

*Order:*
Messages are non-overtaking.
If a sender sends two messages in succession to the same destination, 
and both match the same receive (a call to \texttt{MPI\_Recv}), 
then this operation cannot receive the second message if the first one is still pending. 
If a receiver posts two receives in succession,
and both match the same message, 
then the second receive operation cannot be satisfied by this message, if the first one is still pending. 
This requirement facilitates matching of sends to receives and also guarantees that message passing code is deterministic.

*Fairness:*
MPI makes no guarantee of fairness in the handling of communication. 
Suppose that a send is posted. 
Then it is possible that the destination process repeatedly posts a receive that matches this send, 
yet the message is never received, 
because it is each time overtaken by another message, 
sent from another source. 
It is the programmerâ€™s responsibility to prevent starvation in such situations.

_COMMENT:_ Why did you choose MPI?


** MPI Communication Modes
The MPI API contains a number of variants, or *modes*, for the basic communication primitives.
They are distinguished by a single letter prefix (e.g. \texttt{MPI\_Isend(...)}, \texttt{MPI\_Irecv(...)}).
As dictated by the MPI version 3.0, the following communication modes are supported \cite{MessagePassingInterfaceForum2012}:

*No-prefix for standard mode: \texttt{MPI\_Send(...)}*
In this mode, it is up to MPI to decide whether outgoing messages will be buffered. 
MPI may buffer outgoing messages. 
In such a case, the send call may complete before a matching receive is invoked. 
On the other hand, buffer space may be unavailable, or MPI may choose not to buffer outgoing messages, for performance reasons. 
In this case, the send call will not complete, blocking the transmitting process, until a matching receive has been posted, and the data has been moved to the receiver.

*B for buffered mode: \texttt{MPI\_Bsend(...)}* 
A buffered mode send operation can be started whether or not a matching receive has been posted. 
It may complete before a matching receive is posted. 
However, unlike the standard send, this operation is local, and its completion does not depend on the occurrence of a matching receive. 
Thus, if a send is executed and no matching receive is posted, then MPI must buffer the outgoing message, so as to allow the send call to complete. 
A buffered send operation that cannot complete because of a lack of buffer space is erroneous. 
When such a situation is detected, an error is signaled that may cause the program to terminate abnormally. 
On the other hand, a standard send operation that cannot complete because of lack of buffer space will merely block, 
waiting for buffer space to become available or for a matching receive to be posted. 
This behavior is preferable in many situations. 
Consider a situation where a producer repeatedly produces new values and sends them to a consumer. 
Assume that the producer produces new values faster than the consumer can consume them. 
If buffered sends are used, then a buffer overflow will result. 
Additional synchronization has to be added to the program so as to prevent this from occurring. 

*S for synchronous mode: \texttt{MPI\_Ssend(...)}*
A send that uses the synchronous mode can be started whether or not a matching receive was posted. 
However, the send will complete successfully only if a matching receive is posted, and the receive operation has started to receive the message sent by the synchronous send.
Thus, the completion of a synchronous send not only indicates that the send buffer can be reused, 
but it also indicates that the receiver has reached a certain point in its execution, 
namely that it has started executing the matching receive. 
If both sends and receives are blocking operations then the use of the synchronous mode provides synchronous communication semantics: 
a communication does not complete at either end before both processes *rendezvous* at the communication point.

*R for ready mode: \texttt{MPI\_Rsend(...)}*
A send that uses the ready communication mode may be started only if the matching receive is already posted. 
Otherwise, the operation is erroneous and its outcome is undefined.
Ready sends are an optimization when it can be guaranteed that a matching receive has already been posted at the destination.
On some systems, this allows the removal of a hand-shake operation that is otherwise required and results in improved performance. 
A send operation that uses the ready mode has the same semantics as a standard send operation, or a synchronous send operation; 
it is merely that the sender provides additional information to the system (namely that a matching receive is already posted), that can save some overhead. 

Maybe you should consider non-blocking communication not as a *mode*.

*I for non-blocking mode: \texttt{MPI\_Isend(...)}, \texttt{MPI\_Ibsend(...)}, \texttt{MPI\_Issend(...)} and \texttt{MPI\_Irecv(...)*
Non-blocking message passing calls return control immediately (hence the prefix I), 
but it is the user's responsibility to ensure that communication is complete, 
before modifying/using the content of the data buffer.
It is a complementary communication mode that works en tandem with all the previous.
The MPI API contains special functions for testing whether a communication is complete, or even explicitly waiting until it is finished.




** MPI realization of CMB
Listing \ref{alg:CMB_mpi} is a pseudo code, sketching out the CMB process event loop, using MPI's communication primitives.
#+BEGIN_LATEX
\begin{algorithm}
\caption{CMB Process event loop in MPI}
\label{alg:CMB_mpi}
\begin{algorithmic}[2]

   \While{process clock < some T}  
      \State post a MPI\_Irecv on each incoming peer process
      \State post a MPI\_Wait: block until every receive has been completed
      \State save each message received in a separate, per incoming link, FIFO.
      \State identify message M with the smallest time-stamp
      \State set clock = time-stamp(M)
      \State process message M
      \State post a MPI\_Issend to each outgoing link L with time-stamp = clock + Lookahead(clock,L,...)
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX

Applications have specific communication patterns

Also provides information about the application's communication behavior to the MPI implementation.

*Topology mapping*
_One of the major features of MPI's topology interface is that it can easily be used to adapt the MPI proces layout to the underlying network and system topology._

non cartesian topologies

What is the implementation type of the event?
Let us custom pack them in one 64 bit integer.
Extract them by mapping.

Since you always send an event to your neighbors, either a meaningfull one or a null, why not broadcast?

** Evaluation Metrics
The first evaluation metric of the proposed PDES implementation will be its performance against the reference SystemC kernel.
It will be measured by experimentation on the project's use case.

The simulation's size can be easily related to the duration of the simulation (in simulated time).
Another way of describing the simulation's size is through the conception of a formula involving the number of system processes, the number of links, the system's topology and the amount of events generated.

The accuracy of the simulation can be measured by the aggregate number of causality errors.
The detection of causality errors must be facilitated in a per process level and the aggregation shall be performed at the end of the simulation.
A concrete realization of the accuracy metric comes in the form of a counter each process increments whenever it executes an event with a time-stamp lower than its clock (the time-stamp of the last processed event).
Ideally, if the synchronization algorithms have been realized correctly, no causality errors should be detected.

_COMMENT:_ This section will become more concrete when we start experimentation.


** Existing PDES
The most important:
RISC: Recoding infrastructure for SystemC \cite{Liu2015}.

Miscellaneous:
SystemC-SMP \cite{Mello2010}
SpecC \cite{Domer2011}, although the latter is not meant for SystemC.
sc\_during \cite{Moy}

_COMMENT:_ This section is incomplete that should not be incomplete in an Intermediate report. 
Are you reinventing the wheel? 
Did you try at least one of these tools?
\clearpage


* SystemC TLM 2.0
At the time of writing and to the best of our knowledge, we can not verify the existence of a comprehensive guide about system-level modeling with SystemC TLM 2.0.
Common practice among engineers that want to learn system-level modeling with SystemC TLM 2.0 is to attend courses offered by training companies.
Hence, we fill obliged to provide a quick introduction into the SystemC TLM 2.0 Loosely-Timed (LT) coding style, by means of a simple example.
The chapter assumes familiarity with C++ and SystemC.

In unit [[Overview of SystemC TLM 2.0 API]] we enumerate the features of the SystemC TLM 2.0 API.
In unit nomenclature
In units [[Transactions, Sockets, Initiators and Targets]] and [[Generic Payload]] we have a look at the fundamental notions of transaction, initiator and target components, socket and generic payload.
In unit [[Coding Styles]] we present the two coding styles (Loosely Timed and Approximately Timed) and give their typical use cases.
In unit [[An Example]] we provide the implementation of a simple initiator, interconnect and target model.
In unit [[Criticism]] we present the dominant source of criticism for TLM 2.0.
Finally, in unit [[Simics and TLM 2.0]] we provide a comparison between the dominant industry frameworks for ESLD, Simics and SystemC TLM.

** The role of SystemC TLM 2.0
As stated in unit [[Transaction Level Model]], a Transaction Level Model is considered a virtual platform where a software application can be mapped.
The TLM 2.0 API enhances SystemC's expressiveness in order to facilitate the *modular description* and *fast simulation* of virtual platforms.
As a language, unlike VHDL or SystemC, it is not meant for describing individual functional/architectural/system blocks/modules/components (henceforth *Intellectual Properperty (IP) blocks/modules/components*).
Its role is to make these individual blocks communicate with each other, as demonstrated in figure [[fig:systemc_as_wrapper]].

#+BEGIN_SRC ditaa :file Figures/tlm_as_wrapper.png :cmdline -S --font "Times New Roman"
                                                                           +-------------------------------+
                                                                           |                               |
 ------------------------------------------------------------------------> | Native SystemC module for bus |
         |                         |                          |            |                               |
         v                         v                          v            +-------------------------------+
+--------+----------+     +--------+----------+      +--------+----------+
|    TLM Wrapper    |     |    TLM Wrapper    |      |    TLM Wrapper    |
|                   |     |                   |      |                   |
|  +-------------+  |     |  +-------------+  |      |  +-------------+  |
|  |    ISS      |  |     |  |             |  |      |  |             |  |
|  |             |  |     |  |  Algorithm  |  |      |  |             |  |
|  |+-----------+|  |     |  |             |  |      |  |    VHDL     |  |
|  ||Object Code||  |     |  |    in C     |  |      |  |             |  |
|  |+-----------+|  |     |  |             |  |      |  |             |  |
|  +-------------+  |     |  +-------------+  |      |  +-------------+  |
+-------------------+     +-------------------+      +-------------------+
#+END_SRC

#+CAPTION: TLM 2.0 as a mixed language simulation technology
#+NAME: fig:tlm_as_wrapper
[[file:Figures/tlm_as_wrapper.png]]

System modularity is equivalent to individual IP block *interoperability*, enabling the reuse of IP components in a "plug and play" fashion.
TLM is relevant at every interface where an IP block needs to be plugged into a bus.
Having a library of verified IP blocks at his disposal, the engineer is able to create new virtual platforms fast and with a minimal effort. 

To be suitable for productive software development, a virtual platform needs to be fast, booting operating systems in seconds.
It also needs to be accurate enough such that code developed using standard tools on the virtual platform will run unmodified on real hardware. \cite{Leupers2010}
Compared to a standard RTL simulation, a TLM achieves a significant speed up by replacing communication through pin-level events with a single function call. 
TLM uses the simulation engine available with SystemC.


** TLM 2.0 terminology

TLM 2.0 classifies IP blocks as initiators, targets and interconnect components.
The terms initiator and target come forth as a replacement for the anachronistic terms master and slave.


An *initiator* is a component that initiates new transactions.
It is the initiator's duty to allocate memory for the transaction object or *payload*.
Payloads are always passed by reference.


A *target* component acts as the end point of a transaction. 
As such, it is responsible for providing a response to the initiator.
Request and response are combined into a single transaction object.
Thus, the target responds by modifying certain fields in the payload.


An *interconnect* component is responsible for routing a transaction on its way from initiator to target.
The route of a transaction is not pre-defined.
Routing is dynamic; it depends on the attributes of the payload, mainly its address field.
There is no limitiation on the number of interconnect components participating in a transaction. 
An initiator can also be directly connected to a target.
Since an interconnect can be connected to multiple initiator and target components, it must be able to perform *arbitration* in case transactions "collide".


The role of a component is not statically defined and it is not limited to one.
It is determined on a transactions basis. 
For example, it may function as an interconnect component for some transactions, and as a target for other transactions.


Transactions are sent through initiator *sockets*, and received through target sockets.
It goes without saying that an initiator component must have at least one initiator socket, a target component at least on target socket and a interconnet must possess both.
_Each initiator-to-target socket connection supports both a forward and a backward path by which interface methods can be called in either direction._

All the above terms are illustrated in figure [[fig:tlm_termilogy.png]]


#+BEGIN_SRC ditaa :file Figures/tlm_terminology.png :cmdline -S -E --font "Times New Roman"
+-----------+ Initiator        +--------------+           Target +-----------+
|           | socket           |              |           socket |           |
|           +---+          +---+              +---+          +---+           |
| Initiator | > |--------->| > | Interconnect | > |--------->| > |  Target   |
|           +---+          +---+              +---+          +---+           |
|           |                  |              |                  |           |
+-----------+                  +--------------+                  +-----------+
      :                                ^                               ^
      |                                |                               |
      |                                |                               |
      v                                |                               |
+------------+                         |                               |
|            |                         :                               :
|  Payload   |-------------------------+-------------------------------/
|            |
+------------+
#+END_SRC

#+CAPTION: A basic TLM system
#+NAME: fig:tlm_terminology
#+RESULTS:
[[file:Figures/tlm_terminology.png]]
















TLM 2.0 API \cite{OpenSystemCInitiative2009} consists of the following features ([[fig:TLM_features]]):
- A set of core interfaces
  - A Blocking interface which is coupled with the *Loosely-Timed (LT)* coding style.
  - A non-blocking interface, which is coupled with the *Approximately-Timed (AT)* coding style.
  - The *Direct Memory Interface (DMI)* to enable an initiator to have direct access to a target's memory, bypassing the usual path through the interconnect components used by the transport interfaces. 
  - The *Debug transport interface* to allow an non-intrusive inspection of the system's state.
- The *global quantum* used by the *temporal decoupling* mechanism of the LT coding style, which facilitates faster simulations by reducing the number of context switches performed by the kernel. 
- Initiator and target *sockets* to denote the links (causal dependencies) between processes.
- The *generic payload* which supports the abstract modeling of memory-mapped buses.
- A set of *utilities*, in the form of pre configured sockets and interconnect components, to facilitate the rapid development of models.

#+CAPTION: TLM 2.0 use cases (adopted from \cite{OpenSystemCInitiative2009}).
#+NAME: fig:TLM_features
[[file:Figures/TLM_features.png]]


** Transactions, Sockets, Initiators and Targets
*Transactions* are non-atomic communications, normally with bidirectional data transfer, and consist of a set of messages that are usually modeled as atomic communications.
In a transaction one can distinguish two roles;
the *initiator*, the component which initiated the communication, and the *target*, the component which is supposed to service the initiator's request.
A component is not limited to either of these two roles; it can assume both.
For example, *interconnect* components encapsulate the behavior of memory-mapped buses, being responsible for routing transactions to the correct target.
From the initiator's perspective, they act as targets and from the target's perspective they act as initiators.

Implementation-wise, communication in TLM 2.0 is reduced to method calls, 
from the initiator to the target through an arbitrary number of interconnect component, without involving any context switches from the simulation kernel.

A component's role is signified by the type of *sockets* it contains.
Initiator sockets are used to forward method calls "up and out of" a component, while target sockets are used to allow method calls "down and into" a component \cite{doulos}.
Socket binding is the act of connecting components together, thus defining the component whose method call will be eventually executed to service the transaction.
From SystemC's viewpoint, a socket is basically a convenience class, wrapping a sc_port and an sc_export.





** Coding Styles and Transport Interfaces
LT is suited for describing virtual platforms intended for software development.
However, where additional timing accuracy is required, typically for software performance estimation and architectural analysis use cases, the AT style is employed.
Virtual platforms typically do not contain many cycle-accurate models of complex components because of the performance impact. 

_COMMENT:_ This is a quite problematic section. You need to elaborate more, do not forget LT is on your thesis title. 



** Generic Payload
The basic argument that is passed, by reference, in communicative method calls is called the *generic payload*.
It is a *structure* that encapsulates generic attributes relevant to a generic memory-mapped bus communication.
The structure possesses an extensions mechanism the designer can use to define more specific.

An *interoperable* TLM 2.0 component must depend only on the generic attributes of the generic payload.
The presence of attributes through the extension mechansim can be ignored without breaking the functionality of the model.
In such a case, the extensions mechanism carries simulation meta-data like pointers to module internal data structures or timestamps.


| Attribute           | Type                                | Modifiable        | Description                                                                                                                                                                                                                                                                  |
|---------------------+-------------------------------------+-------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Command             | \texttt{tlm_command} (enum)         | No                | Set by the initiator to either \texttt{TLM_READ} for read, \texttt{TLM_WRITE} for write or TLM_IGNORE to indicate that the command is set in the extensions mechanism.                                                                                                       |
| Address             | \texttt{uint64}                     | Interconnect only | Can be modified by interconnects since by definition an interconnect must bridge different address spaces.                                                                                                                                                                   |
| Data pointer        | \texttt{unsigned char*}             | No                | A pointer to the actual data being transfered.                                                                                                                                                                                                                               |
| Data length         | \texttt{unsigned int}               | No                | Related to the data pointer, indicates the number of bytes that are being transfered                                                                                                                                                                                         |
| Byte enable pointer | \texttt{unsigned char*}             | No                | A pointer to a byte enable mask that can be applied on the data (0xFF for data byte enabled, 0X00 for disabled)                                                                                                                                                              |
| Byte enable length  | \texttt{unsigned int}               | No                | Only relevant when the byte enable pointer is not null. If this number is less than the data length, the byte enable mask is applied repeatedly.                                                                                                                             |
| Streaming width     | \texttt{unsigned int}               | No                | Must be greated than 0. Largest address implied by the transaction is (address + streaming width - 1). Refer to *figure* for an example                                                                                                                                      |
| DMI hint            | \texttt{bool}                       | Yes               | A hint given to the initiator of whether he can bypass the transport interface and access a target's memory directly through a pointer.                                                                                                                                      |
| Response status     | \texttt{tlm_response_status} (enum) | Target only       | The initiator must set it to \texttt{TLM_INCOMPLETE_RESPONSE} prior to initiating the transaction. The target will set it to an appropriate value indicating the outcome of the transaction. For example for a successfull transaction the value is \texttt{TLM_OK_RESPONSE} |
| Extensions          | \texttt{tlm_extension_base*)[]      | Yes               | The mechanism for allowing the generic payload to carry protocol specific attributes                                                                                                                                                                                         |
  





** An Example
This unit will provide a literate code listing for the model in figure [[fig:TLM_tutorial]]
#+CAPTION: A simple system-level model. The initiator, for example, could model a processor, the interconnect component a memory bus and the target a memory.
#+NAME: fig:TLM_tutorial
[[file:Figures/TLM_tutorial.png]]


** Criticism
The root problem with TLM 2.0 lies in the elimination of explicit channels, which were a key contribution in the early days of research on system-level design.
As most researchers agreed, the concept of separation of concerns was of highest importance, 
and for system-level design in particular, this meant the clear separation of computation (in behaviors or modules) and communication (in channels).
Regrettably, SystemC TLM 2.0 chose to implement communication interfaces directly as sockets in modules and this indifference between channels and modules thus breaks the assumption of communication being safely encapsulated in channels.
Without such channels, there is very little opportunity for safe parallel execution \cite{Liu2015}.

For the above reason some designers consider TLM 2.0 a step towards the wrong direction and revert back to TLM 1.0.
Do you agree with this trend? 
Maybe tell us the major difference with TLM 1.0?

This is why SystemC TLM 2.0 model needs to be *recoded* to allow parallel execution.
The recoding must reconstitute the separation of concerns between computation and communication.
A modification of just the kernel will not suffice.



** Simics and TLM 2.0
Everything you do with SystemC TLM 2.0 you can do with Simics.
Simics is the main alternative to SystemC TLM 2.0 for system-level design.
Can you briefly outline the differences between the two tools/frameworks?
Is Simics capable of PDES?
\clearpage


* Use Case
In this chapter we describe the transaction level model we are going to use for conducting our experimentation.
The purpose of the experimentation is twofold;
verify whether we achieve better faster simulation compared to the reference SystemC kernel and evaluate the proposed process synchronization algorithms.

** Platform modeling
A block diagram of the platform that will be modeled is seen in figure [[fig:Platform]].
The platform is a shared fmemory, cache-coherent, symmetric multiprocessor system based on the [[http://opencores.org/or1k/Or1ksim][OpenRisc 1000 Instruction Set Simulator]].
Cache coherence is enforced by a directory residing in the inclusive L2 cache.
Every component is/will be implemented in C/C++ and wrapped in SystemC modules using the TLM 2.0 API for communication. 
The exact number of processors is yet to be determined.

#+CAPTION: A model of a shared memory, cache-coherent, symmetric multiprocessor system
#+NAME: fig:Platform
[[file:Figures/platform.png]]


_COMMENT:_ Can you be more specific about the cache coherence protocol? Maybe provide a state diagram?


** Application modeling
We have the bare metal (newlib based) toolchain for compiling applications for the OpenRisc ISS.

_COMMENT;_ What kind of application am I going to run on this platform?
I see that most of the papers out there do some kind of mpeg2 decoding. That seems complex.






* References
\renewcommand\refname{}
\bibliography{References}
\bibliographystyle{myIEEEtran}












* Caches

** Directory Based Cache Coherence
Avoid broadcast.

The absence of any centralized data structure that tracks the state of the caches is both the fundamental
advantage of a snooping-based scheme, since it allows it to be inexpensive, as well as its Achille's heel
when it comes to scalability.

The sharing status of a particular block of physical memory is kept in one location,
called the *directory*.
There are two very different types of directory-based cache coherence.
In an *SMP*, we can use one centralized directory, 
associated with the memory or some other _single serialization point_, such as the outermost cache in a multicore.

In a *DSM*, it makes no sense to have a single directory, since that would create a single point of contention
and make it difficult to scale to many multicore chips given the memory demands of multicores with eight or more cores.

A directory keeps the state of every block that may be cached.
Information in the directory includes:
    1. which caches (or collections of caches) have copies of the block
    2. whether it is dirty, and so on.

Within a multicore with a shared outermost cache (say, L3), it is *easy* to implement a directory scheme.
_Simply keep a bit vector of the size equal to the number of cores for each L3 block._
The bit vector indicates which private caches may have copies of a blockin L3, and invalidations are only sent to those caches.

This works perfectly for a single multicore if *L3 is inclusive*, 
and _this scheme is the one used in the Intel i7_.


*** Basics

Just as with snooping protocol, there are two primary operations that a directory protocol must implement:
    1. handling a read miss
    2. handling a write to a shared ( thus clean) cache block.

To implement these operatations, a directory must track the state of each cache block.
In a simple protocol, these states could be the following:
   1. *Shared:* One of more nodes have the block cached, and the value in memory is up to date (as well as in all the caches)
   2. *Uncached:* No node has a copy of the cache block.
   3. *Modified:* Exactly one node has a copy of the cache block, 
       and it has written the block, so the memory copy is out of date. 

In addition to tracking the state of each potentially shared memory block, 
we must track which nodes have copies of that block, 
since those copies will need to be invalidated on a write.

_The simplest way to do this is to keep a bit vector for each memory block._

We can also use the bit vector to keep track of the owner of the block when the block is in the exclusive state.
_For efficiency reasons, we also track the state of each cache block at the individual caches._

The states and transitions for the state machine at each cache are identical to what we used for the snooping cache,
although the actions on a transition are slightly different.
The processes of invalidating and locating an exclusive copy of a data item are different,
since the both involve communication between the requesting node and the directory 
and between the directory an one or more remote nodes.

In a snooping protocol, these two steps are combined through the use of a broadcast to all the nodes.



*** Coherence Messages

A catalog of the message types that may be sent between the processors and the directories
for the purpose of handling misses and maintaining coherence.

| Message Type        | Source         | Destination    | Message contents | Function of this message                                                                                            |
|---------------------+----------------+----------------+------------------+---------------------------------------------------------------------------------------------------------------------|
| 1. Read Miss        | Local cache    | Home directory | P, A             | Node P has a read miss at address A; request data and make P a read sharer                                          |
| 2. Write Miss       | Local cache    | Home directory | P, A             | Node P has a write miss at address A; request data and make P the exclusive owner                                   |
| 3. Invalidate       | Local cache    | Home directory | A                | Request to send invalidates to all remote caches that are caching the block at address A                            |
| 4. Invalidate       | Home directory | Remote cache   | A                | Invalidate a shared copy of data at address A                                                                       |
| 5. Fetch            | Home directory | Remote cache   | A                | Fetch the block at address A and sent it to its home directory; change the state of A in the remote cache to shared |
| 6. Fetch/invalidate | Home directory | Remote cache   | A                | Fetch the block at address A and send it to its home directory; invalidate the block in the cache                   |
| 7. Data value reply | Home directory | Local cache    | D                | Return a data value from the home memory                                                                            |
| 8. Data write-back  | Remote cache   | Home directory | A, D             | Write-back a data value for address A                                                                               |

- The first 3 messages are requests sent by the local node to the home.
- The 4 through 6 messages are messages sent to a remote node by the home 
  when the home needs the data to satisfy a read or write miss request.
- Data value replies are used to send a value from the home node back to the requesting node.
- Data value write-backs occur for two reasons: 
     a. When a block is replaced in a cache and must be written back to its home memory
     b. In reply to fetch or fetch/invalidate messages from the home.

_Writing back the data value whenever the block becomes shared_
simplifies the number of states in the protocol since 
     a. Any dirty block must be exclusive 
     b. Any shared block is always available in the home memory.
  

*** Protocol from the Cache's Side

The basic states of a cache block in a directory-based protocol are exactly like those in a snooping protocol.
Thus, we can start with simple state diagrams that show 
    1. The state transitions for *an individual cache block*
    2. The state for the *directory entry* corresponding to each block in memory.



*** Protocol from the Diretory's Side

A message sent to a directory  causes two different types of actions:
  1. Updating the directory state.
  2. Send additional messages to satisfy the request.  

The memory block may be 
  1. Uncached by any node, 
  2. Cached in multiple nodes and readable (shared).
  3. Cached exclusively and writable in exactly one node.

In addition to the state of each block, the directory must track the set of nodes that
have a copy of a block; we use a set called Sharers to perform this function.
_Directory requests need to update the set Sharers and also read the set to perform invalidations._

The directory receives three different requests: read miss, write miss, and data write-back.

_Our simplified protocol assumes that some actions are atomic, such as requesting a value and sending it to another node;
a realistic implementation cannot use this assumption_


** MESI
Adds the state *Exclusive* to the basic MSI protocol
to indicate when a cache block is resident only in a single cache but is clean.

If a block is in the *E* state, it can be written without generating any invalidates,
which optimizes the case where a block is read by a single cache before being written by that same cache.

Of course, when a *read miss* to a block in the *E* state occurs, the block must be changed
to the *S* state to maintain coherence.

Because all sugsequent accesses are snooped, it is possible to maintain the accuracy of this state.
In particular, if another processor issues a read miss, the state is changed from exclusive to shared.
The advantage of adding this state is that a subsequent write to a block in the exclusive state
by the same core need not acquire bus access or generate any invalidate, since the block is known to be
exclusively in this local cache; the processor merely changes the state to modified.

This state is easily added by using the bit that encodes the coherent state as an exclusive state
and using the dirty bit to indicate that a block is modified.

The Intel i7 uses a variant of a MESI protocol, called MESIF, which adds a state (Forward) to designate
which sharing processor should respond to a request. It is designed to enhance performance in distributed
memory organizations.


* Or1ksim

Unfortunately the library of the or1ksim is not reentrant and thus does not allow multiple instances of the core
simulator to be executed in one address space. Historically all data is stored in global variables.

** OpenRISC GNU tool chain
From http://opencores.org/or1k/OpenRISC_GNU_tool_chain

The toolchain is available in several forms, depending on which C standard library they use:

1. or1k-elf for bare metal use, based on the *newlib* library

2. or1k-linux-uclibc for Linux application use, based on the *uClibc* library.

3. or1k-linux-musl for Linux application use, based on the *musl* library.
      

















* Computer Science Cheatsheet 					   :noexport:
_Semantics:_ As a necessary propery of a modeling language whose models are meant to undergo
             synthesis and refinement. In order to have well-defined semantics, we need to
             introduce some form of formalism to models and modeling languages.

_NP problem:_ Non-deterministic Polynomial
              NP problems run in polynomial time on non-deterministic Turing machines
              A decision problem for which a "yes-answer" can be verified in polynomial time (by a deterministic Turing machine)

_NP hard problem:_    (With respect to the class of NP problems) 
                      If every NP problem can be *reduced* to it.

_NP complete problem:_ If it is NP and NP hard.

An _Algorithm_ is a finite description of a sequence of steps to be taken to solve a problem.
Physical processes are rarely structured as a sequence of steps; rather, they are structured as _continuous interactions between concurrent components_.

_Model vs Reality:_ You will never strike oil by drilling through the map (Golomb 1971)
_Concurrency vs Parallelism:_ Consider two "living" threads. On a multicore machine they might be executed in parallel.
On a single core the instructions of each thread are arbitrarily interleaved. In both cases the execution is these two 
threads is characterized as concurrent. Concurrency does not imply simultaneity.

_Chattering Zeno model:_ A moment in the simulation where execution is happening within delta time, not allowing the simulation time to progress.

_Zeno model:_ A model (like Achilles and the Turtle) where simulation time advances slower and slower until it reaches a point where 
it can not advance further(time increment becomes lower than the resolution) and gets trapped in delta time.

_A simulation_ is defined as the execution of model revealing the behaviour of the system being modeled.
A system can be analyzed either by being formally verified or simulated.
Simulation beyond analysis, as a means of constructing a virtual platform.

_A binary file:_ a statically linked library, a dynamically linked library, an object module, a standalone executable.
All binary files contain  meta information, such as the symbol table.

_False Sharing:_ The silent performance killer.
When cores communicate using "shared memory", they are often really just communicating through the cache coherence mechanisms.
A pathological case can occur when two cores access data that happens to lie in the same cache line. 
Normally, cache coherence protocols assign one core, the one that last modifies a cache line, to be the owner of that cache line. If two cores write to the same cache line repeatedly, they fight over ownership. 
Importantly, note that this can happen even if the cores are not writing to the same part of the cache line.
Write contention on cache lines is the single most limiting factor on achieving scalability for parallel threads of execution in an SMP system. \cite{McCool2012}em

_Design Automation_ depends on the high-level modelling and specification of systems.

_Reentrancy (vs Thread Safety):_ A subroutine is called *re-entrant* if it can be interrupted in the middle of its execution and then safely called again (re-entered, for example by the ISR) before its previous invocations complete execution.
*Recursive subroutines must be re-entrant*. A thread-safe code does not necessarily have to be re-entrant.
#+BEGIN_SRC C++
void thread_safe()
{
   acquire_lock
        if interrupted here and the ISR tries to re-enter we are fucked.
   release_lock
}
#+END_SRC

_A computer language:_ can be regarded the medium of communicating an algorithm to a machine.
We want the language to be expressive (like the greek language), portable (like the english language) and efficient (like the swedish)

_Data Parallelism:_ parallelism determined implicitly by data *independence*.

_Bash & C:_ brick and mortar


* RTL Cheatsheet 						   :noexport:
_RTL modules are pin-accurate:_ This means that the ports of an RTL module directly correspond to wires in the real-world implementation of the module. 

_RTL_design:_ The basis of RTL design is that circuits can be thought of 
              as a set of registers and 
              a set of transfer functions 
              defining the datapaths between registers.

_Stages of RTL design:_
(Remeber the dot product example)
1. Identify Data Operations:
2. Determine Type & Precision:
3. Determine Constraints on Data Processing Resources:
4. Allocation and Scheduling: Allocation reffers to the mappings of data operations onto processing resources.
                              Scheduling refers to the choice of clock cycle on which an operation will be performed in a multi-cycle operation.
                              Registers must also be allocated to all values that cross over from one clock cycle to a later one.
			      The aim is to maximize the resource usage and simultaneously to minimise the registers required to store intermediate results.
                              It is now possible to design the datapath minus its controller.

5. Controller Design:         Design a controller to sequence the operations over the eight clock cycles.
                              There are three multiplexers and a register to control in this circuit.
                              *Normally the controller would be implemented as a state machine*
                              
6. Reset Mechanism Design:

#+BEGIN_SRC vhdl
library ieee;
use     ieee.std_logic_1164.all, ieee.numeric_std.all;

package dot_product_types is
   subtype sig8 is signed (7 downto 0);
   type sig8_vector is array (natural range <>) of sig8;
end;

library ieee;
use ieee.std_logic_1164.all, ieee.numeric_std.all;
use work.dot_product_types.all;
entity dot_product is
   port (a, b : in sig8_vector(7 downto 0);
   ck, reset: in std_logic;
   result : out signed(15 downto 0));
end;

architecture behaviour of dot_product is
   signal i : unsigned(2 downto 0);
   signal ai, bi : signed (7 downto 0);
   signal product, add_in, sum, accumulator : signed(15 downto 0);
begin
   control: process
   begin
     wait until rising_edge(ck);
     if reset = '1' then
        i <= (others => '0');
     else
        i <= i + 1;
     end if;
   end process;

   a_mux: ai <= a(to_integer(i));
   b_mux: bi <= b(to_integer(i));
   multiply: product <= ai * bi;
   z_mux: add_in <= X"0000" when i = 0 else accumulator;
   add: sum <= product + add_in;
   
   accumulate: process
   begin
     wait until rising_edge(ck);
     accumulator <= sum;
   end process;
   output: result <= accumulator;
end;
#+END_SRC


* More SystemC 							   :noexport:
_UART:_ The idle, no data state is high-voltage, or powered. 
This is a historic legacy from telegraphy, in which the line is held high to show that the line and transmitter are not damaged

By distinguishing the declaration of an interface from the implementation of its methods, 
SystemC promotes a coding style in which communication is separated from behaviour, 
a key feature to promote refinement from one level of abstraction to another.


* C++ 								   :noexport:

** Explicit threading in C++
#+BEGIN_SRC cpp
#include <thread>
#+END_SRC


** Introspection vs Reflection
Super important to check Qt.
Although it is a GUI thing, it has a DES (maybe PDES, each QThread runs its own event loop) and a Meta Object Compiler.


** Iterators
Iterators connect algorithms to the elements in a container regardless of the type of the container.
Iterators decouple the algorithm from the data source; an algorithm has no knowledge of the container form which the data originates. 


** Named Casts
1. static_cast: converts between related types 
                such as one pointer type to another in the same class hierarchy, 
                an integral type to an enumeration, or a floating-point type to an integral type

2. reinterpret_cast: handles conversions between unrelated types 
                     such as an integer to a pointer
                     or a pointer to an unrelated pointer type

3. const_cast:  converts between types that differ only in const and volatile qualifiers

4. dynamic_cast: does run-time checked conversion of pointers and references into a class hierarchy

*** Dynamic Cast
To use derived classes as more than a convenient shorthand in declarations, 
we must solve the following problem: 

_Given a pointer of type Base*, to which derived type does the object pointed to really belong?_

There are four fundamental solutions:
1. Ensure that only objects of a single type are pointed to.
2. Place a type field in the base class for the functions to inspect.
3. Use dynamic_cast
4. Use virtual functions

Consequently, the most obvious and useful operation for inspecting the type of an object at run time
is *a type conversion operation that returns a valid pointer if the object is of the expected type and a null pointer if it isnâ€™t.* 
The dynamic_cast operator does exactly that.


** DANGER
#+BEGIN_SRC cpp
  class Base{
      void foo(){}
  };
  
  
  class Derived : public Base{
      void bar(){}
  };
  
  
  void dangerous(Base *p, int n){
      for(int i=0; i!=n; i++)
          p[i].foo();
  };
  
  
  void initiate_chaos(){
      Derived d[10];
      dangerous(d, 10);
  }
#+END_SRC


** Inline

*** A questionable interpretation from StackOverflow
**** Keyword
Functions declared in the header must be marked *inline*,
otherwise,
every *translation unit* which includes the header will contain a definition of the function,
and
the linker will complain about multiple definitions (a violation of the One Definition Rule).
_The inline keyword suppresses this, allowing multiple translation units to contain identical definitions._


**** Optimization
A C++ compiler is free to apply the inlining optimization any time it likes,
as long as it doesn't alter the observable behavior of the program.

The inline keyword makes it easier for the compiler to apply this optimization, 
by allowing the function definition to be visible in multiple translation units,
but _using the keyword doesn't mean the compiler has to inline the function_, 
and _not using the keyword doesn't forbid the compiler from inlining the function._


*** Semantics in C++
The inline specifier is a hint to the compiler.


** Generic Programming
Generic Programming seeks to explicitly seperate the notion of "algorithm" from that of a "data-structure".
The motivation is to: promote component-based development, boost productivity, and reduce configuration management.
As an example, if you wanted to support four data structures (array, binary tree, linked list, and hash table)
and three algorithms (sort, find and merge), a traditional approach would require four times three permutations to develop
and maintain. Whereas, a generic programming approach would only require four plus three configuration items.


* MPI 								   :noexport:
** Nonblocking Communication
One can improve performance on many systems by overlapping communication and computation.
This is especially true on systems where communication can be executed autonomously by an intelligent communication controller.

*Light-weight* threads are one mechanism for achieving such overlap.
An alternative mechanism that often leads to better performance is to use *nonblocking communication*.

A *non-blocking send start* call initiates the send operation, but does not complete it.
The send start call can return before the message was *copied out* of the send buffer.
A seperate send complete call is needed to complete the communication, i.e., to verify that the data has been copied out of the send buffer.
With suitable hardware, the transfer of data out of the sender memory may proceed concurrently with computations done at the sender after the send was initiated and before
it completed.

Similarly, a *non-blocking receive start* call initiates the receive operation, but does not complete it.
The call can return before a message is stored into the receive buffer.
A separate receive complete call is needed to complete the receive operation and verify that the data has been recived into the receive buffer.
With suitable hardware, the transfer of data into the receiver memory may proceed concurrently with computations done after the receive was initiated and before it completed.
_The use of nonblocking receives may also avoid system buffering and memory-to-memory copying, as information is provided early on the location of the receive buffer._

_Nonblocking send start calls can use the same four modes as blocking sends: standard, buffered, synchronous and ready._
These carry the same meaning.

Sends of all modes, ready excepted, can be started whether a matching receive has been posted or not;
a nonblocking *ready* send can be started only if a matching receive is posted.
In all cases, the send start call is local: it returns immediately, irrespective of the status of other processes.
If the call causes some system resource to be exhausted, then it will fail and return an error code.
Quality implementations of MPI should ensure that this happens only in "pathological" cases.
That is, an MPI implementation should be able to support a large number of pending nonblocking operations.
The send-complete call returns when data has been copied out of the send buffer.
It may carry additional meaning, depending on the send mode.

If the send mode is *synchronous*, then the send can complete only if a matching receive has started.
That is, a receive has been posted, and has been matched with the send.
_In this case, the send-complete call is non-local_
Note that a synchronous, nonblocking send may complete, if matched by a nonblocking receive, before the receive complete call occurs.
(It can complete as soon as the sender knows the transfer will complete, but before the receiver knwos the transfer will complete.)

If the send mode is *buffered* then the message must be buffered if there is no pending receive.
In this case, the send-complete call is local, and must succeed irrespective of the status of a matching receive.

If the send mode is *standard* then the send-complete call _may_ return before a matching receive is posted, if the message is buffered.
On the other hand, the receive-complete may not complete until a matching receive is posted, and the message was copied into the receive buffer.

_Nonblocking sends can be matched with blocking receives, and vice-versa._

The completion of a send operation may be delayed, for standard mode, and must be delayed, for synchronous mode, until a matching receive is posted.
The use of nonblocking sends in these two cases allows the sender to proceed ahead of the receiver, so that the computation is more tolerant of fluctuations in the speeds of the two processes.

Nonblocking sends in the buffered and ready modes have a more limited impact, 
e.g., the blocking version of buffered send is capable of completing regardless of when a matching receive call is made.
However, separating the start from the completion of these sends still gives some opportunity for optimization within the MPI library.
For example, starting a buffered send gives an implementation more flexibility in determining if and how the message is buffered.
There are also advantages for both nonblocking buffered and ready modes when data copying can be done concurrently with computation.

The message-passing model implies that communication is initiated by the sender.
_The communication will generally have lower overhead if a receive is already posted whene the sender initiates the communication_
_(data can be moved directly to the receive buffer, and there is no need to queue a pending send request)_
However, a receive operation can complete only after the matching send has occured.
The use of nonblocking receives allows one to achieve lower communication overheads without blocking the receiver while it waits for the send.


** MPI_Request
Nonblocking communications use opaque request objects to identify communication operations 
and match the operation that initiates the communication with the operation that terminates it.
These are system objects that are accessed via a handle.
A request object identifies various properties of a communication operation, 
such as the *send mode*, 
the *communication buffer* that is associated with it,
its *context*,
the *tag* and *destinationa* arguments to be used for a send,
or the *tag* and *source* arguments to be used for a receive.
In addition, this object stores information about the status of the pending communication operation.

A *null* handle is a handle with value *MPI_REQUEST_NULL*
A persistent request and the handle to it are *inactive* if the request is not associated with any ongoing communication.
A handle is *active* if it is neither null or inactive.


** MPI_Status
An *empty* status is a status which is set to return 
tag=MPI_ANY_TAG, 
source=MPI_ANY_SOURCE, 
error=MPI_SUCCESS, and is also internally configured so that calls to MPI_GET_COUNT, MPI_GET_ELEMENTS, and MPI_GET_ELEMENTS_X return count = 0 and MPI_TEST_CANCELLED returns false.
We set a status variable to empty when the value returned by it is not significant.
Status is set in this way so as to prevent errors due to accesses of stale information.


** MPI_{TEST|WAIT}{ALL|SOME|ANY}
#+BEGIN_SRC cpp
  MPI_Wait(
      MPI_Request *request,
      MPI_Status  *status
      );
#+END_SRC
A call to MPI_WAIT returns when the operation identified by request is complete.
If the request is an active persistent request, it is marked inactive.
Any other type of request is and the request handle is set to MPI_REQUEST_NULL.
MPI_WAIT is a non-local operation.

The call returns, in *status* information on the completed operation.

One is allowed to call MPI_WAIT with a null or inactive request argument.
In this case the operation returns immediately with empty status.

Successful return of MPI_WAIT after a MPI_IBSEND implies that the used send buffer can be reused.
(data has been sent out or copied into a buffer attached with MPI_BUFFER_ATTACH)
Note that, at this point, we can no longer cancel the send.
If a matching receive is never posted, then the buffer cannot be freed.
This runs somewhat counter to the stated goal of MPI_CANCEL
(always being able to free program space that was committed to the communication subsystem).


** MPI_Status
The source or tag of a received message may not be known if wildcard values were used in the receive operation. 
Also, if multiple requests are completed by a single MPI function (see Section 3.7.5), a distinct error code may need to be returned for each request.

The status argument also returns information on the length of the message received.
However, this information is not directly available as a field of the status variable and a call to MPI_GET_COUNT is required to â€œdecodeâ€ this information.


** MPI_Irecv
#+BEGIN_SRC cpp
  MPI_Irecv(
      void *buf,
      int   count,
      MPI_Datatype datatype,
      int   source,
      int   tag,
      MPI_Comm comm,
      MPI_Request *request
  )
#+END_SRC


* SystemC 							   :noexport:
** General

*** Parsing the SystemC standard for occurences of the word kernel
Clause 4 of \cite{OpenSystemCInitiative2009} "_Elaboration and simulation semantics_", defines the behavior of the SystemC kernel
and is central to an understanding of SystemC.

The _execution_ of a SystemC application consists of _elaboration_ followed by _simulation_.
Elaboration results in the creation of the module hierarchy.
Elaboration involves the execution of application code, the public shell of the implementation, and the private kernel of the implementation.
Simulation involves the execution of the scheduler, part of the kernel, which in turn may execute processes within the application.

The purpose of the process macros is to _register the associated function with the kernel such that the scheduler can call back that member function during simulation_.

When a port is bound to a channel, the kernel shall call the member function register_port of the channel.

Simulation time is initialized to zero at the start of simulation and increases monotonically during simulation.
The physical significance of the integer value representing time within the kernel is determined by the simulation time resolution.

Since process instances execute without interruption, only a single process instance can be running at any one time,
and no other process instance can execute until the currently executing process instance has yielded control to the kernel.
_A process shall not pre-empt or interrupt the execution of another process._
_This is known as co-routine semantics or co-operative multitasking_

The SystemC sc_module class provides four routines that may be overridden, and they are executed at the boundaries of simulation.
These routines provide modelers with a place to put initialization and clean-up code that has no place to live.
For example, checking the environment, reading run-time configuration information and generating summary reports at the end of simulation.
#+BEGIN_SRC cpp :exports code
void before_end_of_elaboration(void);
void end_of_elaboration(void);
void start_of_simulation(void);
void end_of_simulation(void);
#+END_SRC

A thread of clocked thread process instance is said to be resumed when the kernel causes the process to continue execution,
starting with the statement immediately following the most recent call to function wait.

If the thread or clocked thread process executes the entire function body or executes a return statement and thus returns control to the kernel,
the associated function shall not be called again for that process instance. The process instance is then said to be terminated.

The function next_trigger does not suspend the method process instance; a method process cannot be suspended but always executes to completion before
returning control to the kernel.

The distinction between _suspend/resume_ and _disable/enable_ lies in the sensitivity of the target process during the period while it is suspended or disabled.
With _suspend_ the kernel keeps track of the sensitivity of the target process while it is suspended such that a relevant event notification or time-out 
while suspended would cause the process to become runnable immediately when resume is called.
With _disable_ the sensitivity of the target process is nullified while it is suspended such that the process is not made runnable by the call to enable, but only on the next
relevant event notification or time-out subsequent to the call to enable.

If a process kills itself, the statements following the call to kill shall not be executed again during the current simulation, and control shall return to the kernel.

_STOPPED AT OCCURENCE 44_


*** Parsing the SystemC standard for occurences of the phrase set of
Set of runnable processes
Set of update requests
Set of delta notifications
Set of time-outs
Set of timed notifications


*** Parsing the SystemC standard for occurences of the phrase simulation time
43/105:
Synchronization may be strong in the sense that the sequences of communication events
is precisely determined in advance, or weak in the sense that the sequence of communication events
is partially determined by the detailed timing of the individual processes.

Strong synchronization is easily implemented in SystemC using FIFOs or semaphores, allowing a completely
untimed modeling style where in principle simulation can run without advancing simulation time.

Untimed modeling in this sense is outside the scope of TLM 2.0. On the other hand, a fast virtual
platform model allowing multiple embedded software threads to run in parallel may use either strong or weak
synchronization. In this standard, the appropriate coding style for such a model is termed loosely-timed.


*** Port vs Export
The purpose of port and export bindings is to enable a port or export to _forward interface method calls made during simulation._
A port _requires_ the services defined by an interface.
An export _provides_ the services defined by an interface.

Forward path form initiator to target.
Backward path from target back to initiator.


*** TODO Parsing the SystemC standard for occurences of the phrase update phase 











SC_THREADs are not threads. They are coroutines.

Coroutines are subroutines that allow multiple entry points for suspending and resuming execution at certain locations.

SystemC does not offer real concurrency. It simulates concurrency using ...

The SystemC kernel implements cooperative scheduling where each SC_THREAD willingly relinquishes control to allow other SC_THREADs to execute.

In order to implement that cooperative scheduling strategy using coroutines, a threading library is used.


The scheduler advances simulation time to the time of the next event, 
then runs any processes due to run at that time of sensitive to that event.

Computations that take some time are usually modeled by instantaneous computations followed by a SystemC wait.

A _scheduler_ manages the threads by use of queues, such as READY, which contains all those that are ready to execute
and WAIT which contains threads waiting for events.

_Threads_ switch between READY and WAIT during simulation subject to event notification and time advances.

Events are delivered in an inner loop called _delta-cycle_ and simulation time advances in an outer loop _time-cycle_.


** Co-routine semantics
\cite{OpenSystemCInitiative2012}
Since process instances execute without interruption, only a single process instance can be running at any one time, 
and no other process instance can execute until the currently executing process instance has yielded control to the kernel.
A process shall not pre-empt or interrupt the execution of another process.
This is known as co-routine semantics or co-operative multitasking

An implementation running on a machine that provides hardware support for concurrent processes may permit two or more processes to run concurrently
provided that the behavior appears identical to the co-routine semantics defined in this subclause.
In other words, the implementation would be obliged to analyze any dependencies between processes and to constrain their execution to match the co-routine semantics.

Software modules that interact with one another as if they were performing I/O operations. (Conway 1963)

Co-routine semantics are linked to Kahn process networks.

*** Impediments to speed
_Context switching:_
- Every time you see a SC_THREAD -> _wait_ or a SC_METHOD -> _next_trigger() return_
- Complex bus protocols and lots of processes


** Dynamic processes with sc_spawn


** sc_elab_and_sim
sc_elab_and_sim is used to simplify the invocation of SystemC from a user-defined main() function.
If you do not have your own main(), you do not need sc_elab_and_sim


** sc_simcontext::crunch
This process implements the simulator's execution of processes.
It is a while(true) thing

sc_simcontext::crunch
sc_simcontext::simulate
sc_core::start
sc_main
sc_elab_and_sim
main


** sc_export
An export gives a structured way to express the fact that
a module provides an interface whose methods may be called from outside the module.
In a sense, an export is the opposite of a port.
Whereas a port allows interface method calls "up and out of" a module, an export allows interface method calls "down and into" a module.

An export should be bound to a channel or to another export in the constructor of the module in which it is declared.
Unlike a multiport, an export cannot be bound to more than one channel.

As for ports, you could create specialized classes derived from sc_export if you wanted to, 
but unlike sc_port, there are none built in to the SystemC class library.


* TLM 2.0 							   :noexport:

** General
A standardized way to connect models described at the untimed or approximately timed transaction level.
Instead of every vendor of system-level virtual platforms having their own proprietary languages, models and tools
every major developer of these platforms is now beginning to standardize on the use of TLM 2.0 as the way in which
to interconnect models or is planning to do so within their next development cycle.

Models developed for one system will be able to work on another, meaning that the problem of model availability and
true interoperability is now being solved. 

TLM 2.0 provides communications and timing capabilities that enable modeling at various levels of timing accuracy.

This chapter also demonstrates the transition that is going on in the industry: away from proprietary systems
and interfaces toward open standards.

platform-based development approach

An example of an extension is the TLM 2.0 library which creates additional communications capabilities
that mimic bus-based semantics. 
While this still remains within the discrete event MoC, it illustrates how additional semantics can be
built upon the base.

With the introduction of TLM 2.0 another huge barrier was removed, which was model interoperability.
SystemC does not define the semantics of communications between models as it only provides the essential primitives
necessary for communications. Thus there was no agreement in the industry about how these interfaces should
be constructed.

Several EDA vendors, such as CoWare, attempted to create and proliferate communications libraries,
but these saw no uptake because of the proprietary nature of them.
Today we are seeing rapid adoption of TLM 2.0 by the industry with significant support coming from 
all of the major EDA players.

\cite{Bailey2010}




#+BEGIN_LATEX
\tikzstyle{block} = [draw, fill=blue!4!white, rectangle, minimum height=3em, minimum width=6em]
\begin{figure}
\begin{tikzpicture}[auto, node distance=2cm]

\node [block] (payload) {Generic payload};
\node [block, right of=payload] (phases)  {Phases};
\node [block, below of=payload] (sockets) {Initiator and target sockets};
\node [block, below of=sockets] (tlm)     {TLM-2 core interfaces: 
                                               \begin{itemize}
					       \item {Blocking transport interface}
					       \item {Non-blocking transport interface}
					       \item {Direct memory interface}
					       \item {Debug transport interface}
					       \end{itemize}
					       };

\draw [->] (payload) -- (sockets);
\draw [->] (phases)  -- (sockets);
\draw [->] (sockets) -- (tlm);

\end{tikzpicture}\caption{TLM 2.0 Interoperability layer for bus modeling}
\end{figure}
#+END_LATEX
** Transaction Object

*** Blocking transport interface

1. The lifetime of a given transaction object may extend beyond the return from nb_transport such
   that _a series of calls to nb_transport may pass a single transaction object_ forward and backward between initiators, interconnect components, and targets.

2. If there are multiple calls to nb_transport associated with a given transaction instance, one and the same transaction object shall be passed as an argument to every such call.
   _In other words, a given transaction instance shall be represented by a single transaction object._

3. _An initiator may re-use a given transaction object_ to represent more than one transaction instance, or across calls to the transport interfaces, DMI, and the debug transport interface.

4. Since the lifetime of the transaction object may extend over several calls to nb_transport, either the caller or the callee may modify or update the transaction object, 
   subject to any constraints imposed by the transaction class TRANS.
   For example, for the generic payload, the target may update the data array of the transaction object in the case of a read command, but shall not update the command field.

** Sockets
A socket combines a port with an export.
An _initiator socket_ is derived from class sc_port and has an sc_export. It has the port for the forward path and the export for the backward path.
An _target_socket_    is derived from class sc_export and has an sc_port ([[~/pSystemC/src/tlm_core/tlm_2/tlm_sockets/tlm_target_socket.h][tlm_base_target_socket]])

Only the most derived classes *tlm_initiator_socket* and *tlm_target_socket* are typically used directly by applications. 
These two sockets are parameterized with a protocol traits class that defines the types used by the forward and backward interfaces.
Sockets can only be bound together if they have the identical protocol type.

** Generic Payload

*** Introduction

A specific transation type.
For *maximum interoperability*, applications should use the default transaction type *tlm_generic_payload* with the base protocol and the default phase type *tlm_phase*.
Sockets that use interfaces specialized with different transaction types cannot be bound together, providing compile-time checking but restricting interoperability.

It supports the abstract modeling of memory-mapped buses, 
together with an extension mechanism to support the modeling of specific bus protocols whilst maximizing interoperability.

The main features of the generic payload are:
- Command 
  Is it read or write?
- Address
  What is the address
- Data
  A pointer to the physical data as an array of bytes
- Byte Enable Mask Pointer
- Response
  An indication of whether the transaction was successful, and if not the nature of the error

The generic payload is the class type offered by the TLM-2.0 standard for transaction objects passed through the core interfaces.
The generic payload is closely related to the the base protocol, which itself defines further rules to ensure interoperability when using the generic payload.


*** Streaming Width
In case of *multi-beat* transactions 
the ratio of the data length over the streaming width will give the number of beats. 


*** Byte Enable Mask Pointer
The elements in the byte enable array shall be interpreted as follows.
A value of 0 shall indicate that that corresponding byte is disabled, and a value of 0xff shall indicate that
the corresponding byte is enabled.
The meaning of all other values shall be undefined. 
The value 0xff has been chosen that the byte enable array can be used directly as a mask.


*** Generic Payload Memory Management
From section 7.5 of the TLM 2.0.1 LRM

1. The initiator shall be responsible for setting the data pointer and byte enable pointer atrributes to existing
   storage, which could be static, automatic (stack) or dynamically allocated (new storage). 
   The initiator shall not delete this storage before the lifetime of the transaction is complete.
   _The generic payload destructor does not delete these two arrayes._

3. The generic payload supports two distinct approaches to memory management;
   reference counting with an explicit memory manager and ad hoc memory management by the initiator.
   The two approaches can be combined.
   Any memory management approach should manage both the transaction object itself and any extensions to the transaction object.

4. The construction and destruction of objects of type tlm_generic_payload is expected to be expensive in terms of CPU time due to the implementation of the extension array.
   As a consequence, repeated construction and destruction of generic payload objects should be avoided.
   There are two recommended strategies; either use a memory manager that implements a pool of transaction objects, 
   or if using ad hoc memory management, re-use the very same generic payload object across successive calls to *b_transport*
   (effectively a transaction pool with a size of one).
   _Having a generic payload object constructed and destructed once per call to transport would be prohibitively slow and should be avoided_

5. A memory manager is a user-defined class that implements at least the *free* method of the abstract base class tlm_mm_interface.
   The intent is that a memory manager would provide a method to allocate a generic payload transaction object from a pool of transactions,
   would implement the free method to return a transaction object to that same pool, and would implement a destructor to delete the entire pool.
   The *free* method is called by the *release* method of class tlm_generic_payload when the reference count of a transaction object reaches 0.
   The free method of class *tlm_mm_interface* would typically call the reset method of class *tlm_generic_payload* in order to delete any extensions marked for automatic deletion.

6. The methods *set_mm*, *acquire*, *release*, *get_ref_count* and *reset* of the generic payload shall only be used in the presence of a memory manager.
   By default, a generic payload object does not have a memory manager set.

7. Ad hoc memory management by the initiator without a memory manager requires the initiator to allocate memory for the transaction object before the TLM-2.0
   core interface call, and delete or pool the transaction object and any extension objects after the call.

** Initiators and Targets
A module's processes may act as either initiators or targets.
An initiator is responsible for creating a payload and calling the transport function to send it.
A target receives payloads from the transport function for processing and response.
In the case of non-blocking interfaces the target may create new transactions backwards in response to a transaction from an initiator.
Initiator calls are made through initiator sockets, target calls received through target sockets.
A module may implement both target and initiator sockets, allowing its threads to both generate and receive traffic.

** Blocking, Non-Blocking, Debug and Interfaces/Transport Call
_How does TLM contribute to performance boost:_ You do 1 wait, rather than many waits.

With the blocking interface you can have wat() on the target code.

Why does the nb_transport_if defines 4 phases?
- To enable

** Direct Memory Interface
Characteristics:
- Allows direct backdoor access into memory
- Allows un-inhibited ISS execution:
  (Instead of roaming through the hierarchy of a buss system-Fast software execution)

** Socket
In order to pass transactions between initiators and targets, TLM-2.0 uses sockets.
An initiator sends transactions out through an _initiator socket_, and a target receives incoming transactions through a _target socket_.
A socket is basically a convinience class, wrapping up a port and an export.

[[file:Figures/tlm_socket.png]]





** Blocking interface
This interface allows only two timing points to be associated with each transaction, 
corresponding to the call to and return from the blocking transport function.

The b_transport method has a timing annotation argument.
This single argument is used on both the call to and the return from b_transport to indicate the time of
the start and end of the transaction, respectively, relative to the current simulation time.


*** Class Definition
#+BEGIN_SRC cpp
  namespace tlm {
      template<typename TRANS=tlm_generic_payload>
      class tlm_blocking_transport_if: public virtual sc_core::sc_interface{
      public:
          virtual void b_transport(TRANS& trans, sc_core::sc_time& t)=0;
      };
  }
#+END_SRC



*** Rules
1. The b_transport may call wait, directly or indirectly
2. The b_transport method shall not be called from a method process.
3. The initiator may *re-use* a transaction object from one cal to the next and across calls to the transport interfaces, DMI, and the debug transport interface
4. *The call to b_transport marks the first timing point of the transaction. The return from b_transport marks the final timing point of the transaction.*
5. The timing annotation argument allows the timing points to be offset from the simulation times (value returned by sc_time_stamp()) at which the function call and return are executed.
6. The callee may modify or update the transaction object, subject to any constraints imposed by the transaction class TRANS.
7. It is recommmended that the transaction object should not contain timing information. Timing should be annotated using the sc_time argument to b_transport.
8. Typically, an interconnect component should pass the b_transport call along the forward path from initiator to target. In other words, the implementation of b_transport for the target socket of the interconnect component may call the b_transport method of an initiator socket.


** Loosely Timed Coding Style
Notes from Video Lecture: [[http://videos.accellera.org/tlm20tutorial/David_Black/player.html][David Black, XtremeEDA USA: TLM Mechanics]]					   
_FAST-NOT ACCURATE_ (In terms of timing?): Less detail means faster simulation. Less context switching means also faster simulation.
A fast, loosely-timed model is typically expected to use the _blocking transport interface_ the _DMI_ and _temporal decoupling_.
_Older terminology:_ UnTimed - Programmer's View
_Use Cases:_
- Early Software Development
_Characteristics:_
- Only sufficient timing detail to _boot O/S and run multi-core systems. It can express the modeling of _timers and _interrupts_
- Processes can run ahead of simulation time (_temporal decoupling_)
- Each transaction has _2 timing points_: begin and end
- Uses direct memory interface (_DMI_)

_Temporal decoupling:_
Each process runs ahead up to quantum boundary.
sc_time_stamp() advances in multiples of the quantum.
Deterministic communication requires explicit synchronization.

_DMI:_
When combined with temporal decoupling may lead to completely crappy situations.
The language neither the simulator do not protect the designer.
It is like a hole in the legal system.


** Approximately-timed
_ACCURATE_ (In terms of timing?)
_Older terminology:_ Cycle Accurate
_Use cases:_
- Architectural Analysis, Software Performance Analysis
- Hardware Verification


** Loosely-timed coding style and temporal decoupling
*Each process is allowed to run for a certain time slice or quantum before switching to the next, or instead may yield control when it reaches an explicit synchronization point.*

The quantum value represents a tradeoff between simulation speed and accuracy.

For a fine grained model, the overhead of event scheduling and process context switching becomes the dominant factor in simulation speed.
Therefore allowing a process to run ahead of the simulation time will speed up the simulation.
Until it needs to interact with another process, for example read or update a variable belonging to another process.

The processs that runs ahead of simulation time creates a time warp

Each process is responsible for determining whether it can run ahead of simulation time without breaking the functionality of the model.
When a process encounters an external dependency it has two choices: either force synchronization, 
which means yielding to allow all other processes to run as normal until simulation time catches up, or sample or update the current value and continue.

_DMI:_
When combined with temporal decoupling may lead to completely crappy situations.
The language neither the simulator do not protect the designer.
It is like a hole in the legal system.

Individual SystemC processes are permitted to run ahead in a local "time warp" without actually advancing simulation time
until they need to synchronize with the rest of the system.
Temporal decoupling can result in very fast simulation for certain systems because it increases the data and code locality and reduces scheduling overhead of the simulator.

*Each process is allowed to run for a certain time slice or quantum before switching to the next, or instead may yield control when it reaches an explicit synchronization point.*

The quantum value represents a tradeoff between simulation speed and accuracy.

For a fine grained model, the overhead of event scheduling and process context switching becomes the dominant factor in simulation speed.
Therefore allowing a process to run ahead of the simulation time will speed up the simulation.
Until it needs to interact with another process, for example read or update a variable belonging to another process.

The processs that runs ahead of simulation time creates a time warp

Each process is responsible for determining whether it can run ahead of simulation time without breaking the functionality of the model.
When a process encounters an external dependency it has two choices: either force synchronization, 
which means yielding to allow all other processes to run as normal until simulation time catches up, or sample or update the current value and continue.


** Debuggin the AT 2 phase example
*** Call stack when calling the constructor of a module
constructor of current module
constructor of top module
sc_main
sc_elab_and_sim
main








* Design Patterns 						   :noexport:

Check this website and maybe buy the book
https://sourcemaking.com/design_patterns


* Graveyard of potentially usefull phrases 			   :noexport:
Form must follow function - Le Corbusier

Activities that lie in between the time span an idea became a product is design

_This chapter delves_ into the world of hardware-software codesign

something real and tangible

praxis

An MoC for describing the application at the system-level

Like a wagnerian leitmotif

Working in tandem

Often, we use the terms A and B interchangeably and in a haphazard manner.

An implementation running on a machine that provides hardware support for concurrent processes may permit two or more processes to run concurrently
provided that the behavior appears identical to the co-routine semantics defined in this subclause.
In other words, the implementation would be obliged to analyze any dependencies between processes and to constrain their execution to match the co-routine semantics.

An important limitation of SystemC regarding performance is that the reference implementation is sequential, 
and the official semantics, just like any other Discrete Event Simulator (henceforth DES), make parallel execution difficult.
Most existing work on parallelization of SystemC targets cycle-accurate simulation,
and would be inefficient on loosely timed systems since they cannot run in parallel processes that do not execute simultaneously \cite{Moy}.

\cite{Moy}
The SystemC standard allows this, "provided that the behavior appears identical to the co-routine semantics" \cite{OpenSystemCInitiative2012}
This implies two constraints on a parallel implementation:

- It should not change the order in which processes are allowed to be executed. 
  In particular, the simulated time imposes an order on the execution of processes.
  
An optimistic approach would relax this constraint having a violation detection and rollback mechanism to correct any violations afterwards.
Although this may seem to work with VHDL, with SystemC this is chaotic, since arbitrary C++ code and system calls.

- It should not introduce new race conditions.
  For example, two SystemC processes may safely execute x++ on a shared variable, but running two such processes in parallel cannot be allowed.
  The co-routine semantics of the SystemC kernel guarantee that there will be no race conditions.
  Evaluate-update paradigm

How to realize the DE MoC on top of completely heterogeneous HPC platform 


* Companies List 						   :noexport:
Mentor Graphics
Cadence
Synopsys
Tensilica


* Latex Headers 						   :noexport:
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt,twoside]
#+LATEX_HEADER: \usepackage[paper=a4paper,dvips,top=1.5cm,left=1.5cm,right=1.5cm, foot=1cm,bottom=1.5cm]{geometry}
#+LATEX_HEADER: \renewcommand{\rmdefault}{ptm} 
#+LATEX_HEADER: \usepackage[scaled=.90]{helvet}
#+LATEX_HEADER: \usepackage{courier}
#+LATEX_HEADER: \usepackage{bookmark}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \pagestyle{fancy}
#+LATEX_HEADER: \usepackage[dvipsnames*,svgnames]{xcolor} 
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,fit,positioning,calc,shapes}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[swedish,english]{babel}
#+LATEX_HEADER: \usepackage{rotating}		
#+LATEX_HEADER: \usepackage{array}		
#+LATEX_HEADER: \usepackage{graphicx}	 
#+LATEX_HEADER: \usepackage{float}	
#+LATEX_HEADER: \usepackage{color}      
#+LATEX_HEADER: \usepackage{mdwlist}
#+LATEX_HEADER: \usepackage{setspace}   
#+LATEX_HEADER: \usepackage{listings}	
#+LATEX_HEADER: \usepackage{bytefield}  
#+LATEX_HEADER: \usepackage{tabularx}	
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{algpseudocode}	
#+LATEX_HEADER: \usepackage{dcolumn}	
#+LATEX_HEADER: \usepackage{url}	
#+LATEX_HEADER: \usepackage[perpage,para,symbol]{footmisc} 
#+LATEX_HEADER: \usepackage[all]{hypcap}
#+LATEX_HEADER: \definecolor{darkblue}{rgb}{0.0,0.0,0.3} %% define a color called darkblue
#+LATEX_HEADER: \definecolor{darkred}{rgb}{0.4,0.0,0.0}
#+LATEX_HEADER: \definecolor{red}{rgb}{0.7,0.0,0.0}
#+LATEX_HEADER: \definecolor{lightgrey}{rgb}{0.8,0.8,0.8} 
#+LATEX_HEADER: \definecolor{grey}{rgb}{0.6,0.6,0.6}
#+LATEX_HEADER: \definecolor{darkgrey}{rgb}{0.4,0.4,0.4}
#+LATEX_HEADER: \hyphenpenalty=15000 
#+LATEX_HEADER: \tolerance=1000
#+LATEX_HEADER: \newcommand{\rr}{\raggedright} 
#+LATEX_HEADER: \newcommand{\rl}{\raggedleft} 
#+LATEX_HEADER: \newcommand{\tn}{\tabularnewline}
#+LATEX_HEADER: \newcommand{\colorbitbox}[3]{%
#+LATEX_HEADER: \rlap{\bitbox{#2}{\color{#1}\rule{\width}{\height}}}\bitbox{#2}{#3}}
#+LATEX_HEADER: \newcommand{\red}{\color{red}}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}{-3.25ex\@plus -1ex \@minus -.2ex}{1.5ex \@plus .2ex} {\normalfont\normalsize\bfseries}}
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \renewcommand\subparagraph{\@startsection{subparagraph}{5}{\z@}{-3.25ex\@plus -1ex \@minus -.2ex}{1.5ex \@plus .2ex} {\normalfont\normalsize\bfseries}}
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \setcounter{tocdepth}{3}
#+LATEX_HEADER: \setcounter{secnumdepth}{5}
#+LATEX_HEADER: \renewcommand{\headrulewidth}{0pt}
#+LATEX_HEADER: \lhead{Konstantinos Sotiropoulos}
#+LATEX_HEADER: \chead{Ms Thesis Intermediate Report}
#+LATEX_HEADER: \rhead{\date{\today}}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \let\ps@plain\ps@fancy 
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \setlength{\headheight}{15pt}


