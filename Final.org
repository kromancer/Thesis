#+TITLE: Parallel Simulation of SystemC Loosely-Timed Transaction Level Models
#+AUTHOR:Konstantinos Sotiropoulos
#+EMAIL: kisp@kth.se
#+STARTUP: overview
#+KEYWORDS: parallel discrete event simulation, conservative synchronization algorithms, transaction level models, SystemC TLM 2.0
#+OPTIONS: toc:nil title:nil date:nil creator:nil email:nil author:nil broken-links:mark tasks:nil




* Latex Preamble                                                     :ignore:
#+LATEX_HEADER: \documentclass[11pt,a4paper,oneside,openright,abstractoff,titlepage,final,BCOR10mm]{scrreprt}
#+LATEX_HEADER: \usepackage[margin=25mm]{geometry}
#+LATEX_HEADER: \usepackage[margin=25mm]{geometry}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage{inputenc}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{lastpage}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage[table]{xcolor}
#+LATEX_HEADER: \usepackage{tabularx}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{rotating} 
#+LATEX_HEADER: \usepackage{lmodern} 
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{microtype}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage[colorlinks]{hyperref}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \usepackage{pdfpages} 
#+LATEX_HEADER: \usepackage{glossaries} 
#+LATEX_HEADER: \usepackage[intoc]{nomencl}
#+LATEX_HEADER: \usepackage{verse}
#+LATEX_HEADER: \newcommand{\attrib}[1]{\nopagebreak{\raggedcenter \footnotesize #1\par}}
#+LATEX_HEADER: \renewcommand{\poemtitlefont}{\raggedright\normalfont\large\bfseries\hspace{\leftmargin}}
#+LATEX_HEADER: \hypersetup{ colorlinks = true, urlcolor = cyan, linkcolor = blue, citecolor = red }
#+LATEX_HEADER: \usepackage{dsfont}


* Titlepage                                                          :ignore:
#+BEGIN_EXPORT latex
\begin{titlepage}
\thispagestyle{empty}
\begin{center}
  
  \vspace{5cm}
  
  \huge{Parallel Simulation of SystemC Loosely-Timed Transaction Level Models}
  \vspace{5cm} 
  
  \Large Master of Science Thesis\\
  \vspace{2cm}
  
  \today
  \vspace{6cm}
  
  \begin{tabular}{ll} 
  \noindent Author: 	 		& Konstantinos Sotiropoulos \\
  \noindent Supervisor: 		& Björn Runåker (Intel Sweden AB) \\ 

  \noindent Examiner:  	 		& Prof. Ingo Sander (KTH)\\ 
  \noindent Academic adviser: 	        & PhD student George Ungureanu (KTH)
  \end{tabular}
  \vspace{2.5cm}

  \small
  \begin{tabular}{l}
  \textsc{KTH Royal Institute of Technology}\\
          School of Information and Communication Technology\\
          Department of Electronic Systems\\
	  Stockholm, Sweden
  \end{tabular}
  
\end{center} 
\end{titlepage}
\clearpage
#+END_EXPORT


* Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:

\pagenumbering{roman}

\addcontentsline{toc}{section}{Abstract}
Parallelizing the development cycles of hardware and software is becoming the industry's norm for reducing electronic devices time to market.
In the absence of hardware, software development is based on a virtual platform; 
a fully functional software model of a system under development, able to execute unmodified code.

A Transaction Level Model, expressed with the SystemC TLM 2.0 language, is one of the many possible ways for constructing a virtual platform.
Under SystemC's simulation engine, hardware and software is being co-simulated.
However, the sequential nature of the reference implementation of the SystemC's simulation kernel, is a limiting factor.
Poor simulation performance often constraints the scope and depth of the design decisions that can be evaluated.

It is the main objective of this thesis' project to demonstrate the feasibility of parallelizing the co-simulation of hardware and software using Transaction Level Models, outside SystemC's reference simulation environment.
The major obstacle identified is the preservation of causal relations between simulation events.
The solution is obtained by using the process synchronization mechanism known as the Chandy/Misra/Bryantt algorithm.

To demonstrate our approach and evaluate under which conditions a speedup can be achieved, we use the model of a cache-coherent, symmetric multiprocessor executing a synthetic application. 
Two versions of the model are used for the comparison; the parallel version, based on the Message Passing Interface 3.0, which incorporates the synchronization algorithm and an equivalent sequential model based on SystemC TLM 2.0.
Our results indicate that by adjusting the parameters of the synthetic application, a certain threshold is reached, above which a significant speedup against the sequential SystemC simulation is observed.
Although performed manualy, the transformation of a SystemC TLM 2.0 model into a parallel MPI application is deemed feasible.

*Keywords:* parallel discrete event simulation, conservative synchronization algorithms, transaction level models, SystemC TLM 2.0



\clearpage





* Acknowledgement
:PROPERTIES:
:UNNUMBERED: t
:END:

\addcontentsline{toc}{section}{Acknowledgement}

My Master's Thesis project was sponsored by Intel Sweden AB and was supervised by KTH's ICT department.
Most of the work was carried out in Intel's offices in Kista, where I was kindly provided with all the necessary experimentation infastructure.\\

Björn Runåker was the project's supervisor from the company's side.
I would like to thank you Björn for placing your trust in me for carrying out this challenging task.
Furthermore, I would also like to thank Magnus Karlsson for his valuable feedback.\\

Professor Ingo Sander and PhD student George Ungureanu were the examiner and academic advisor from the university's side. 
I blame you for my intellectual Odyssey in the vast ocean of mathematical abstractions.
I am now a sailor, on course for an Ithaka I may never reach.
And I am most grateful for this beautiful journey.
May our ForSyDe come true: the day when the conceptual wall between software and hardware collapses.
\textit{Let there be computation}.\\

Mother and father you shall be acknowledged, I owe my existence to you.
Maria, I want to express my gratitude for your tolerance and support.
Finally, Spandan, my comrade, you must always remember the price of intellect.
Social responsibillity and chronic insomnia.

\clearpage

#+BEGIN_LATEX
\begin{verse}[\linewidth]
\itshape  As you set out for Ithaka \\
          hope the voyage is a long one, \\
          full of adventure, full of discovery. \\!

          But do not hurry the journey at all. \\
          Better if it lasts for years, \\
          so you are old by the time you reach the island, \\
          wealthy with all you have gained on the way, \\
          not expecting Ithaka to make you rich. \\!

          Ithaka gave you the marvelous journey. \\
          Without her you would not have set out. \\
          She has nothing left to give you now. \\!
 
          And if you find her poor, Ithaka won’t have fooled you. \\
          Wise as you will have become, so full of experience, \\
          you will have understood by then what these Ithakas mean. \\!
	  
	  \attrib{ Konstantinos Kavafis, Ithaka }
\end{verse}
\clearpage
#+END_LATEX


* Table of Contents                                                  :ignore:
#+TOC: headlines 3
\addcontentsline{toc}{section}{Contents}
\clearpage


* List of Acronyms and Abbreviations
:PROPERTIES:
:UNNUMBERED: t
:END:

#+ATTR_LATEX: :center nil
| *ASIC*:  | Application Specific Integrated Circuit |
| *DE*:    | Discrete Event                          |
| *DES*:   | Discrete Event Simulator/Simulation     |
| *DMI*:   | Direct Memory Interface                 |
| *ES*:    | Electronic System                       |
| *ESLD*:  | Electronic System-Level Design          |
| *FPGA*:  | Field Programmable Gate Array           |
| *FSM*    | Finite State Machine                    |
| *HDL*    | Hardware Description Language           |
| *HPC*:   | High Performance Computing              |
| *IC*     | Integrated Circuit                      |
| *IP*     | Intellectual Property                   |
| *MoC*:   | Model of Computation                    |
| *MPI*    | Message Passing Interface               |
| *MPSoC*: | Multiprocessor System on Chip           |
| *OoO*:   | Out-of-Order                            |
| *PDES*:  | Parallel Discrete Event Simulation      |
| *SLDL*:  | System-Level Design Language            |
| *SMP*:   | Symmetric Multiprocessing               |
| *SoC*:   | System on Chip                          |
| *SR*:    | Synchronous Reactive                    |
| *TLM*:   | Transaction Level Modeling              |
| *CMB*:   | Chandy/Misra/Bryant algorithm           |
\addcontentsline{toc}{section}{List of Acronyms and Abbreviations}
\clearpage


* List of Figures                                                    :ignore:
#+BEGIN_EXPORT latex
\listoffigures
\addcontentsline{toc}{section}{List of Figures}
\clearpage
#+END_EXPORT 


* Introduction
\pagenumbering{arabic}
Section [[Overview]], provides an insight to the pragmatics of the project; 
without disclosing any commercially sensitive information, the reader is exposed to the use case, which became the raison d'être of this project.
The problem definition is then presented in Section [[Problem Definition]].
Section [[Hypothesis]] presents the hypothesis; an optimistic assumption that motivated this work.
Section [[Purpose]] attempts to provide a general answer to the cui bono question. For a specific answer, the reader is encouraged to jump to section [[Reflections]].
Following the classification presented in \cite{Hakansson2013}, Section [[Research Methodology]] describes the research methodology applied.
Section [[Objectives]] and [[Delimitations]] clarify the software engineering extend; what artifacts need to be constructed, in order to address the problem statement.
A synopsys of this document can be found in [[Structure of this thesis]]


** Overview
This project follows the work of Björn Runåker \cite{Runaker2015} on his effort to parallelize the simulation of the next generation (5G) of radio base stations.
Telecom radio base stations are indeed a very heterogeneous system.
To say the least, a virtual platform describing the system consists of a Network Processing Unit (NPU), Field Programmable Gate Array (FPGA) logic and a group of Digital Signal Procesors (DSP).
For a more pictorial exposition of the situation the reader is encouraged to refer to the work of Björn.

The approach followed was defined as "coarse-grained";
parallelism is achieved through multiple instations of SystemC's simulation engine, one per major component.
However, a question is left open;
the feasibilty and merits of a "fine-grained" treatment, where parallelsim is achieved within a single instance of the simulation engine.


** TODO Problem Definition


** TODO Hypothesis
Given the fact that we are dealing with multiprocessor systems
ample opportunities for parallelization 
to leverage process synchronization overhead and yield a simulation speedup, 
compared to 


** Purpose
An increasing amount of an Electronic System's (ES) expected use value is becoming software based.
Companies which neglect this fact face catastrophic results.
A well identified narrative, for example in \cite{Surowiecki2013}, 
is how Nokia was marginalized in the "smartphone" market, 
despite possesing the technological know-how for producing superior hardware.

If an ES company is to withstand the economical pressure a competitive market introduces, the need for performing software and hardware development in parallel is imperative.
Established ways of designing ESs, that delay software development until hardware is available, are therefore obsolete.
The de facto standard of dealing with this situation has become the development of virtual platforms.
It is obvious, that if a virtual platform is to be used for software development, it must be able to complete execution in the same order of magnitute as the actual hardware.
Poor simulation performance often constraints the scope and depth of the design decisions that can be evaluated.


** TODO Research Methodology
Twofold identify concepts, how these concepts are interpreted in sdes, how these concepts could be re interpreted for pdes
mathematical concepts can be the base for software engineering
no more ontological oo design
"From Mathematics to Generic Programming"
maybe put a concept map


** TODO Objectives
In order to reach The answer of requires the development of the following software applications:

+ An MPI realization of a process synchronization algorithm in the context of an event-based 


** Delimitations
The following list demonstrates a number of artifacts that are not to be expected from this work, mainly due to their implementation complexity, given the limited time scope of a thesis project.
However, one must keep in mind that the term "implementation complexity" often conceals the more fundamental question of feasibility.

+ A modified version of the reference SystemC simulation kernel, capable of orchestrating a parallel simulation.
 
+ A compiler for translating SystemC TLM 2.0 models into parallel applications. In fact, the previous statement should be generalized, for the shake of brevity:
  this thesis will not produce any sort of tool or utility.

+ Any form of quantitative comparison between the proposed and existing attempts to parallelize SystemC TLM 2.0 simulations.


** Structure of this thesis
\clearpage



* Background
This chapter wishes to inform the reader about the theoretical constituents of this project.
[[Electronic System-Level Design]] presents the outermost context; that is the engineering discipline of *Electronic System-Level Design (ESLD)* and how SystemC TLM 2.0 fits into the whole picture.
Section [[The Discrete Event Model of Computation]] hopes to help the reader understand why *Electronic System-Level Design Language* (ESLDL) models can be executed.
In Section [[SystemC's Discrete Event Simulator]], SystemC's simulation engine is presented. This section is complemented by the code example found in Appendix \ref{AppendixA}.
Before proceeding, the reader is adviced to abandon momentarily any preconceptions about design, system, model, computation, time, concurrency and causality.

** Electronic System-Level Design
Section [[The Design Process]] defines the fundamental concepts of design, system, model and simulation.
In Sections [[Electronic Systems Design]] to [[Transaction-Level Model]], using Gajski and Kuhn's Y-Chart, the concept of a Transaction-Level Model is determined, as an instance in the engineering practice of Electronic System-Level Design (ESLD).
Section [[SystemC and TLM]] a rudimentary look on SystemC's role in ESLD.


*** The Design Process
We define the process of *designing* as the engineering art of incarnating a desired functionality into a perceivable, thus concrete, artifact.
An engineering artifact is predominantly referred to as a *system*, 
to emphasize the fact that it can be viewed as a structured collection of components and that its behavior is a product of the interaction among its components.

Conceptually, designing implies a movement from abstract to concrete, fueled by the engineer's *design decisions*, incrementally adding implementation details.
This movement is also known as the *design flow* and can be facilitated by the creation of an arbitrary number of intermediate artifacts called models.
A *model* is thus an abstract representation of the final artifact in some form of a language.
The design flow can be now semi-formally defined as a process of model refinement, with the ultimate model being the final artifact itself.
We use the term semi-formal to describe the process of model refinement, because to the best of our knowledge, 
such model semantics and algebras that would establish formal transformation rules and equivalence relations are far from complete \cite{Gajski2009}.

A desired property of a model is executability that is its ability to demonstrate portions of the final artifact's desired functionality in a controlled environment.
An *executable model*, allows the engineer to form hypotheses, conduct experiments on the model and finally evaluate design decisions.
It is now evident that executable models can firmly associate the design process with the scientific method.
The execution of a model is also known as *simulation* \cite{Editor2014}.



*** Electronic Systems Design
An Electronic System (ES) provides a desired functionality, by manipulating the flow of electrons.
Electronic systems are omnipotent in every aspect of human activity; 
most devices are either electronic systems or have an embedded electronic system for their cybernisis.

The prominent way for visualizing the ES design/abstraction space is by means of the Y-Chart.
The concept was first presented in 1983 \cite{Gajski1983} and has been constantly evolving to capture and steer industry practices.
Figure \ref{fig:Y-Chart} presents the form of the Y-Chart found in \cite{Gajski2009}.

#+BEGIN_EXPORT latex
\begin{figure}[htpb]
  \centering
  \begin{tikzpicture}[>=stealth',join=bevel,font=\sffamily,auto,on grid,decoration={markings, mark=at position .5 with \arrow{>}}]

    \coordinate (behaviouralNode) at (135:4cm);
    \coordinate (structuralNode) at (45:4cm);
    \coordinate (physicalNode) at (270:4cm);
    \coordinate (originNode) at (0:0cm);

    \node [above=1em] at (behaviouralNode) {\textbf{Behavioural Domain}};
    \node [above=1em] at (structuralNode) {\textbf{Structural Domain}};
    \node [below=1em] at (physicalNode) {\textbf{Physical Domain}};

    \draw[-, very thick] (behaviouralNode.south) -- (0,0) node[left,pos=0]{System Requirements} node[left,pos=0.2]{} node[left,pos=0.4]{} node[left,pos=0.6]{} node[left,pos=0.8]{Transfer Functions};

    \draw[-, very thick] (structuralNode.south) -- (0,0) node[pos=0]{Model of Computation} node[pos=0.2]{} node[pos=0.4]{} node[pos=0.6]{} node[pos=0.8]{Transistors};

    \draw[-, very thick] (physicalNode.south) -- (0,0) node[right,pos=0]{Virtual Platform} node[right,pos=0.2]{} node[right,pos=0.4]{} node[right,pos=0.6]{} node[right,pos=0.8]{Transistor layout};

    \draw[fill] (barycentric cs:behaviouralNode=1.0,originNode=0) circle (2pt);
    \draw[fill] (barycentric cs:behaviouralNode=0.8,originNode=0.2) circle (2pt);
    \draw[fill] (barycentric cs:behaviouralNode=0.6,originNode=0.4) circle (2pt);
    \draw[fill] (barycentric cs:behaviouralNode=0.4,originNode=0.6) circle (2pt);
    \draw[fill] (barycentric cs:behaviouralNode=0.2,originNode=0.8) circle (2pt);

    \draw[fill] (barycentric cs:structuralNode=1.0,originNode=0) circle (2pt);
    \draw[fill] (barycentric cs:structuralNode=0.8,originNode=0.2) circle (2pt);
    \draw[fill] (barycentric cs:structuralNode=0.6,originNode=0.4) circle (2pt);
    \draw[fill] (barycentric cs:structuralNode=0.4,originNode=0.6) circle (2pt);
    \draw[fill] (barycentric cs:structuralNode=0.2,originNode=0.8) circle (2pt);

    \draw[fill] (barycentric cs:physicalNode=1.0,originNode=0) circle (2pt);
    \draw[fill] (barycentric cs:physicalNode=0.8,originNode=0.2) circle (2pt);
    \draw[fill] (barycentric cs:physicalNode=0.6,originNode=0.4) circle (2pt);
    \draw[fill] (barycentric cs:physicalNode=0.4,originNode=0.6) circle (2pt);
    \draw[fill] (barycentric cs:physicalNode=0.2,originNode=0.8) circle (2pt);

    \draw[black!50] (0,0) circle (4.0cm);
    \draw[black!50] (0,0) circle (3.2cm);
    \draw[black!50] (0,0) circle (2.4cm);
    \draw[black!50] (0,0) circle (1.6cm);
    \draw[black!50] (0,0) circle (0.8cm);

  \end{tikzpicture}
  \caption{Gajski-Kuhn \index{Gajski-Kuhn Y-chart}Y-chart} 
  \label{fig:Y-Chart}
\end{figure}
#+END_EXPORT

The Y-Chart quantizes the design space into four levels of abstraction; system, processor, logic and circuit, represented as the four concentric circles.
For each abstraction level, one can use different ways for describing the system: behavioral, structural and physical.
These are represented as the three axises, hence the name Y-Chart.
Models can now be identified as points in this design space.

A typical design flow for an Integrated Circuit (IC) begins with a high-level behavioral model capturing the system's specifications and proceeds non-monotonically to a lower level structural representation, expressed as a netlist of, still abstract, components.
From there, Electronic Design Automation (EDA) tools will pick up the the task of reducing the abstraction of a structural model by translating the netlist of abstract components to a netlist of standard cells.
The nature of the standard cells is determined by the IC's fabrication technology (FPGA, gate-array or standard-cell ASIC).
Physical dimensionality is added by place and route algorithms, part of an EDA framework, signifying the exit from the design space, represented in the Y-Chart by the the "lowest" point of the physical axis.

The adjective non-monotonic is used to describe the design flow, because as a movement in the abstraction space, it is iterative:
design \rightarrow test/verify \rightarrow redesign.
This cyclic nature of the design flow is implied by the errors the human factor introduces, under the lack of formal model transformation methodologies in the upper abstraction levels.
The term *synthesis* is also introduced to describe a variety of monotonic movements in the design space: from a behavioral to a less-equally abstract structural model, from a structural to a less-equally abstract physical model, or for movement to less abstract models on the same axis.
Synthesis is distinguished from the general case of the design flow, in order to disregard the testing and verification procedures.
Therefore, the term synthesis may indicate the presence, or the desire of having, an automated design flow.
Low-level synthesis is a reality modern EDA tools achieve, while high-level synthesis is still a utopia modern tools are converging to.





*** System-Level Design
To meet the increasing demand for functionality, ES complexity, as expressed by their heterogeneity and their size, is increasing.
Terms like Systems on Chip (SoC) and Multi Processor SoC (MPSoC), used for characterizing modern ES, indicate this trend.
With abstraction being the key mental ability for managing complexity, the initiation of the design flow has been pushed to higher abstraction levels.
In the Y-Chart the most abstract level, depicted as the outer circle, is the system level.
At this level the distinction between hardware and software is a mere design choice thus *co-simulation of hardware and software* is one of the main objectives.
Thereby the term *system-level design* is used to describe design activity at this level.



*** Transaction-Level Model
A *Transaction-Level Model* (TLM) can now be defined as the point in the Y-Chart where the physical axis meets the system abstraction level.
As mentioned in the previous unit, a TLM can be thought of as a *Virtual Platform* (VP), where an application can be mapped \cite{Rigo2011}.
Another way of perceiving the relationship between these three terms (TLM, VP and application) is to say the following:
An application "animates" the virtual platform by making its components communicate through transactions.
A TLM It is a fully functional software model of a complete system that facilitates *co-simulation of hardware and software*.

There are three pragmatic reasons that stimulate the development of a transaction level model.
At first, as already mentioned, software engineers must be equipped with a virtual platform they can use for *software development*, early on in the design flow, without needing to wait for the actual silicon to arrive.
Secondly, a TLM serves as a testbed for *architectural exploration* in order to tune the overall system architecture, with software in mind, prior to detailed design.
Finally, a TLM can be a reference model for hardware *functional verification*, that is, a golden model to which an RTL implementation can be compared.




*** SystemC and TLM
One fundamental question, for completing the presentation of ESLD, remains; How can models be expressed on the system level?
While maintaining the expressiveness of a Hardware Description Language (HDL), *SystemC* is meant to act as an *Electronic System Level Design Language* (ESLDL).
It is implemented as a C++ class library, thus its main concern is to provide the designer with executable rather than synthesizable models.
The language is maintained and promoted by Accellera (former Open SystemC Initiative OSCI) and has been standardized (IEEE 1666-2011 \cite{OpenSystemCInitiative2012}).
A major part of SystemC is the TLM 2.0 library, which is exactly meant for expressing TLMs.
Despite introducing different language constructs, TLM 2.0 is still a part of SystemC because it depends on the same simulation engine.
TLM 2.0 has been standardized seperately in \cite{OpenSystemCInitiative2009}.
\clearpage


** The Discrete Event Model of Computation
With Section [[Models of Computation]] the reader will be able to understand why a linguistic artifact, such as a model, can be "animated".
In Sections [[Discrete Event Model of Computation]] we present the *Discrete Event Model of Computation* (DE MoC).
As with any MoC, the section presents what constitutes a component and what actions it can perform.
Sections [[Causality and Concurrency]] and [[Time and Determinism]] define the concepts of causality, concurrency, time and determinism in the theoretical framework developed in the previous section.


*** Models of Computation
A *language* is a set of symbols, rules for combining them (its syntax), and rules for interpreting combinations of symbols (its semantics). 
The process of resolving the semantics of a linguistic artifact is called *computation*.
Two approaches to semantics have evolved: denotational and operational.
*Operational semantics*, which dates back to Turing machines, give the meaning of a language in terms of actions taken by some abstract machine. 
The word "machine" indicates a system that can be set in "motion" through "space" and time.

With operational semantics it is implied that a language can not determine computation byitself \cite{Jantsch2005}. 
Computation is an epiphenomenon of the "motion" of the underlying abstract machine, just like time indication in a mechanical watch is a byproduct of gear motion.
Consider the language of regular expressions.
A linguistic artifact in this language describes a pattern that is either matched or not by a string of symbols.
A Finite State Machine (FSM) is the underlying abstract machine.
Computation is a byproduct of the FSM changing states; was the final state an accepting state or not.
The rules that describe an abstract machine constitute a *Model of Computation (MoC)* \cite{Edwards1997}.

All of the above painstaking narrative has been formed to reach the following conclusion: 
The dominant MoC related to an ESLDL is called the *Discrete Event (DE)* MoC, and it is the presence of the DE MoC that makes an ESLDL model executable.



*** Discrete Event Model of Computation
The components of a DE MoC are called *processes*.
Processes introduce a spatial decompostion of a system;
The system is mathematically represented as a set of variables $\mathds{V}$, and every process is related to a subset of $\mathds{V}$.
The *system's state* is a mapping from $\mathds{V}$ to a value domain $\mathds{U}$.
The system changes states in a discrete fashion; 
the set $\mathds{A}$ of all possible system states can be enumerated by natural numbers ($\|\mathds{A}\| = \aleph_0$).

A process can now be defined as a set of *events* $P_i \subseteq \mathds{E}$ where $i\in\mathbb{N}$.
$\mathds{E}$ is a universal set on which processes $P_i$ define a partition: $\bigcup_{i=1}^{n} P_i = \mathds{E}$ and $P_i \cap P_j = \emptyset$ where $i,j,n \in \mathbb{N}$ and $n$ is the number of processes.
An event denotes a system state change; from the system's perspective, it can be regarding as a mapping $\mathds{A} \rightarrow \mathds{A}$.

$\mathds{E}$ is a partially ordered set under the relationship *"happens before"*, denoted by the symbol $\sqsubset$ \cite{Lamport1978}.
The binary relationship $\sqsubset$, apart from being antisymmetric and transitive, is irreflexive; 
an event can not "happen before" itself, it is counter intuitive to expect this when modeling physical systems.

On a process two actions are performed: communication and execution.
Both of these can be defined as functions $\mathds{E} \rightarrow \mathds{E}$.
*Execution* $f: P_i \rightarrow P_i$ is the processing of events (hence the name process to describe the entity that performs this action).
In simpler terms, execution "consumes" an event, may change the system's state and may "produce" an event that needs to be communicated.
Execution has the following property: $e_1 \sqsubset f(e_1)$ where $e_1 \in P_i$.
In other words, the sets $P_i$ are totally ordered under the $\sqsubset$ relationship.
*Communication* $g: P_i \rightarrow P_j$ is the exchange of events.
In simpler terms, communication maps an event from one process to an event in another process.
Communication has a similar property: $e_1 \sqsubset f(e_1)$ where $e_1 \in P_i$ and $f(e_1) \in P_j$.


*** Causality and Concurrency
The binary relationship *"causaly affects"*, denoted by the symbol $\propto$, is introduced.
Causality, 
as a philosophical assumption about the behaviour of the system, 
can now be interpreted by the following statement: for any two events $e_1,e_2 \in \mathds{E}$ it is true that $e_1 \propto e_2 \implies e_1 \sqsubset e_2$.
Two events $e_1,e_2 \in \mathds{E}$ are *concurrent* if neither $e_1 \sqsubset e_2$ nor $e_2 \sqsubset e_1$ holds.
It follows, that concurrent events are not causaly related.

#+BEGIN_EXPORT latex
\begin{figure}[htpb]
\centering
\begin{tikzpicture}[
arrow/.style={draw,->,>=stealth},
point/.style={circle,fill=black},
every node/.style={node distance = 10},
]

\node (p1) at (0,2) {$p_1$};
\node (p2) at (0,1) {$p_2$};
\node (p3) at (0,0) {$p_3$};

\node (p1l) at (0.2,2) {};
\node (p2l) at (0.2,1) {};
\node (p3l) at (0.2,0) {};

\node (p1r) at (8,2) {};
\node (p2r) at (8,1) {};
\node (p3r) at (8,0) {};

\path[draw] (p1l.center) edge (p3l.center);

\draw[arrow] (p1l.center) to (p1r);
\path[arrow] (p2l.center) to (p2r);
\path[arrow] (p3l.center) to (p3r);

\node[point] (a) at (1,2) {};
\node [below of = a] {a};
\node[point] (b) at (2.5,2) {};
\node [below of = b] {b};
\node[point] (c) at (1,1) {};
\node [below of = c] {c};
\node[point] (d) at (5.5,1) {};
\node [below of = d] {d};
\node[point] (e) at (1.75,0) {};
\node [below of = e] {e};
\node[point] (f) at (7,0) {};
\node [below of = f] {f};

\path[arrow] (b) edge node [right] {} (c);
\path[arrow] (d) edge node [right] {} (f);
\end{tikzpicture}
\caption{DE space-time decomposition} 
\label{fig:DE}
\end{figure}
#+END_EXPORT

Figure \ref{fig:DE} provides a visual understanding of a DE system, as a space-time diagram.
A discrete perception of space is obtained by process decomposition (y-axis), while the perception of time (x-axis) is obtained by process actions.
The horizontal arrows indicate process execution, while non-horizontal arrows indicate process communication.
Events are represented as points in this plane.
The execution and communication properties are denoted by placing the input event on the start of the arrow and the output event at its tip [fn:223].

To move forward in time, one must follow a *chain* of ordered, under the $\sqsubset$ relationship, events.
One such chain is the sequence $a,b,c,d,f$.
Event $a$ may causaly affect $f$.
Events $d,e$ are concurrent: there is no chain that contains both.
Event $d$ cannot causaly affet $e$ and vice versa.
The time axis is not resolved; a clocking mechanism for relating an event with a number, its timestamp, has not been defined.
That is why the placement of events on the plane, for example events $d,e$ is quite arbitrary, non-unique and maybe counter intuitive.

[fn:223] For execution, the reader has to imagine the presence of many intermediate arrows, between two subsequent events on the same horizontal arrow. The start is at the left event and the tip at the right.


*** Time and Determinism
When implementing a DE MoC, one needs to differentiate between two notions of time: Simulated/logic time and real/wallclock time.
*Real/Wallclock time* refers to the notion of time existing in the simulator; for example a x86 Time Stamp Counter (TSC), which counts the number of oscillation events since the reset event.
*Simulated/logic time* is defined as a the notion of time in the DE. 
Since $\mathds{E}$ is partially ordered and only the sets $P_i$ are totally ordered, one is forced to reach the conclusion that the DE MoC instigates a *relativistic notion of time*.
Simulated time may be different across processes, at any moment in real time.
In other words, there is no global perception of logic time, that would allow, for example, the time axis in Figure \ref{fig:DE} to be resolved/measured/quantized or, force a unique placement of events $d,e$ in the plane.

Logic time modeling is deferred to the implementation of the DE abstract machine.
It is highly depended on the nature of the undelying machine. 
Is it a *parallel* machine, that is a machine that preserves the spatial decomposition defined in the DE?
Or is it a *sequential* machine, where space dimensionality must be emulated.
A realization of the DE abstract machine is called a *Discrete Event Simulator (DES)*.

If a DES can infer a total ordering of $\mathds{E}$, somehow, then the simulation is said to be *deterministic*.
A total ordering of $\mathds{E}$ also infers a total ordering of the set $\mathds{S}$: the system states encountered during simulation ($\mathds{S} \subseteq \mathds{A}$).
Determinism is a very important reasoning facility, engineers seek from the simulation of the systems they construct, in order to provide any formal statement about the system's behavior.
Physicists, especially those engaged with quantum mechanics, are more tolerant to non-determinism.

For amusement purposes only, the reader can regard her/his brain as a DES.
How does the human brain handles the relativistic nature of time; it infers total orderings for the events of reality.
Alas, human intuition is biased towards a deterministic understanding of the physical world.
Intellect, though, is (hopefully) much more capable!

\clearpage


** SystemC's Discrete Event Simulator
Section [[Coroutines]] demonstrates how SystemC realizes the concept of a process.
This section is complemented by the code examples found in Appendices \ref{AppendixA} and \ref{AppendixB}.

*** Coroutines
SystemC's distribution comes with a sequential realization of the DE MoC, referred to as the reference *SystemC simulation engine* \cite{OpenSystemCInitiative2012}.
It is a sequential implementation because the spatial decomposition of the system is emulated through *coroutines* (also known as co-operative multitasking). 
Co-routines in SystemC have been counterintuively named as \texttt{SC\_METHOD}, \texttt{SC\_THREAD} or \texttt{SC\_CTHREAD}.
A coroutine is neither a function nor a thread.

Processes, realized as coroutines[fn:pthread], perform their actions (computation, communication), henceforth *run*, without interruption.
At any moment in real time only a single process can be running.
No other process can run until the running process has voluntarily *yielded*.
Furthermore, a non-running process can not preempt or interrupt the running process.

A process can be declared sensitive to a number of events (static sensitivity).
Moreover, a process can declare itself sensitive to events (dynamic sensitivity).
All of the events the process is sensitive to, form its *sensitivity list*.
A yielded process is awaiting for events in its sensitivity list to to be triggered.

Before yielding, a process saves its context and registers its identity in a global structure of coroutine handlers called the *waiting list*.
Along comes the question: to whom does a yielding process pass the baton of control flow?

[fn:pthread] The exact library that realizes co-routines in C++ is determined during the compilation of the SystemC distribution. 
             In GNU/Linux, SystemC version 2.3.1 supports QuickThreads and Posix Threads.
	     However, it is highly propable that future revisions of the C++ standard will include *resumable functions*, a concept semantically equivalent to coroutines.



*** The kernel
The *kernel* is the simulation's director \cite{Editor2014}, the maestro of a well orchestrated simulation music.
Processes yield to the kernel, a coroutine himself.
In the presence of an ill-behaved never yielding process, the kernel is powerless [fn:kernel].

The kernel is responsible for many things[fn:forward]:
1. It sorts the *global event queue* according to timestamp order.
2. It is from his perspective that a non-relativistic notion of logic time is formed: 
   it maintains a *clock* that advances according to the timestamp of the event last triggered.
3. When the list of *runnable* processes has been depleted, it is his duty to trigger the next, according to timestamp order, event.
4. When triggering an event, it must identify which processes can be moved from the waiting to the runnable list. 
   The decision is based on a process sensitivity list.
5. It is responsible for *context switching* between the running and a runnable process. 
   The selection of the running process from the list of runnable processes is implementation-defined.
   An example of such a situation can be found in Appendix \ref{AppendixB}.
6. If there are no events in the global event queue and the list of runnable processes is empty, it must *terminate* the simulation.

A spectre is haunting the previous description of the kernel: how is logic time modeled?

[fn:kernel] This is exactly the most important problem faced by early operating systems (16-bit era). 
            Their cooperative nature could not discipline poorly designed applications.
[fn:forward] Please note that many terms are forward-declared and defined either further down in the description or in upcoming sections.

*** Modeling Time
Logic time is represented as a vector [fn:dense] $(t,n) \in \mathbb{N}^2$.
Every event is associated with the moment in logic time it occured.
In other words, every event has a *timestamp*.
Ordering of events comes as a lexicographical comparison between timestamps.
Two events $e_1, e_2$ associated with the timestamps $(t_1,n_1), (t_2, n_2)$ are said to be *simultaneous* if $t_1 = t_2$.
If both $t_1 = t_2$ and $n_1 = n_2$ they are *strongly simultaneous*.

The first co-ordinate of a logic time vector is meant for modeling real time.
*Modeled real time values* are used as timing annotations the designer injects into the system in order to describe the duration of communication and execution in the physical system.
To avoid quantization errors and the non-uniform distribution of floating point values, time is internally represented as an integral multiple of an SI unit referred to as the time resolution.
The integral multiplier is limited by the underlying machine's capabilities: in a 64-bit architecture its maximum value is $2^{64}-1$.
The minimum time resolution SystemC can provide is that of a femtosecond ($10^{-15}$ seconds).

To assist in the construction of modeled real time values, SystemC provides the class \texttt{sc\_time}.
\texttt{sc\_time}'s constructor takes two arguments: (\texttt{double}, \texttt{SC\_TIME}) [fn:unit].
The designer needs to be very careful when providing time annotations: modeled real time is internally represented as an integral value, despite \texttt{sc\_time}'s constructor having a floating point argument.
The mistake of using a value of \texttt{sc\_time(0.5, SC\_FS)} can only be detected during *run-time*.
The same applies for a value of \texttt{sc\_time(1, SC\_SEC)} with a time resolution of 1 \texttt{SC\_FS}.


[fn:dense] This time modeling technique is referred to as *superdense time* in \cite{Editor2014}. 
           However, this is not consistent across literature, for example in \cite{Furia2010}.
[fn:unit] \texttt{SC\_TIME} is an enumeration: \texttt{SC\_SEC} for a second, \texttt{SC\_MS} for a millisecond etc.


*** Events


Events in SystemC are realized as instances of the class \texttt{sc\_event}.
A process explicitly "makes" an event occur by calling either of these variations of the \texttt{sc\_event.notify} method:
+ \texttt{notify()}: Immediate occurence.
+ \texttt{notify(SC\_ZERO\_TIME)}: Delayed occurence.
+ \texttt{notify(sc\_time t)}: Scheduled occurence.


Now that a metric for logic time has been established, we be more specific on a process yields.
Yielding is explicitly stated by a calling:
+ \texttt{wait()}
+ \texttt{wait(sc\_time)}
+ \texttt{wait(sc\_event)}


*** The Simulation Procedure
#+BEGIN_EXPORT latex
\begin{algorithm}
\caption{SystemC event loop, adopted from \cite{Schumacher2010}}
\label{alg:kernel}
\begin{algorithmic}[1]

   \State run all processes 
   \State order events
   \While{scheduled events exist}        \Comment{Simulation time progression}
      \State order events and trigger the event with the smallest timestamp
      \State advance simulation time
      \State make all sensitive processes runnable
      \While {runnable processes exist}   \Comment{Delta cycle progression}
         \While {runnable processes exist}
	     \State run all runnable processes
             \State trigger all immediate notifications
             \State make all sensitive processes runnable
         \EndWhile
	 \State trigger all delayed events
         \State make all sensitive processes runnable
       \EndWhile
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_EXPORT



\clearpage




** Parallel Discrete Event Simulation
Units [[Parallel Discrete Event Simulation(or)]] and [[Causality and Synchronization]] introduce the concept of Parallel Discrete Event Simulation (PDES) and identify the fundamental causality hazards.

*** Prior Art
The previous section has made evident that the reference implementation of the SystemC kernel assumes sequential execution and therefore can not utilize modern massively parallel host platforms. 
The most logical step in achieving faster simulations is to realize concurrency, from interleaved process execution to simultaneous/parallel execution.
By assigning each process to a different processing unit of the host platform (core or hardware thread) we enter the domain of *Parallel Discrete Event Simulation (PDES)*.
After making the strategical decision that for improving a DE simulator's performance one must orchestrate parallel execution, 
the first tactical decision encountered
is whether to keep a single simulated time perspective, 
or distribute it among processes.

For PDES implementations that enforce global simulation time, the term *Synchronous PDES* has been coined in \cite{Schumacher2010}.
In Synchronous PDES, parallel execution of processes is performed within a delta cycle. 
With respect to Alg \ref{alg:kernel}, we can say that a Synchronous PDES parallelizes the execution of the innermost loop (line 4).
However, as we will see in later sections, this approach will bare no fruits in the simulation of TLM Loosely Timed simulations, since delta cycles are never triggered \cite{Chen2012}.
Therefore, we switch our interest in *Out-of-Order PDES (OoO PDES)* \cite{Chen2015}; 
allowing each process to have its own perception of simulated time, determined by the last event it received.

\clearpage


*** Causality Hazards 
The distribution of simulation time opens Pandora's box. 
Protecting the OoO PDES from *causality errors* demands certain assumptions and the addition of complex implementation mechanisms.

The first source of causality errors arises when the system's state variables are not distributed, in a disjoint way, among the processes \cite{Fujimoto1990}.
A trivial realization of the above scenario is depicted in figure [[fig:causality_shared_state]]. Processes $P_1$ and $P_2$ are executing simultaneously, while sharing the system's state variable $x$.
Events $E_1$ and $E_2$ are executed by $P_1$ and $P_2$ respectively. If we assume that in real time $E_2$ is executed before $E_1$, then we have implicitly broken causality, since $E_1$ might be influenced
by the value of $x$ that the execution of $E_2$ might have modified. Furthermore, one must observe that this kind of implicit interaction between $P_1$ and $P_2$ can not be expressed in a DE MoC. 
This is a meta-implication of the host platform's shared memory architecture.

#+CAPTION: Causality error caused by the sharing of the system's state variable $x$ by $P_1$ and $P_2$.
#+NAME: fig:causality_shared_state
[[file:Figures/causality_shared_state.png]]

The second and most difficult to deal with source of causality errors is depicted in figure [[fig:causality_safe_events]]. 
Event $E_1$ affects $E_2$ by scheduling a third event $E_3$ which, for the sake of argument, modifies the state of $P_2$. 
This scenario necessitates sequential execution of all three events. 
Thus the fundamental problem in PDES, in the context of this scenario, becomes the question: how can we deduce that it is safe to execute $E_2$ in parallel with $E_1$, without actually executing $E_1$ \cite{Fujimoto1990}?
However, one must notice that the kind of interaction that yields this problematic situation is explicitly stated in the model.

#+CAPTION: Causality error caused by the unsafe execution of event $E_2$ (adopted from \cite{Fujimoto1990}).
#+NAME: fig:causality_safe_events
[[file:Figures/causality_safe_events.png]]

The last example makes evident the fact that the daunting task of preserving causality in the simulation is all about *process synchronization*.
For example, each process must be able to communicate to each of its peers (processes that is linked with) the message: 
"I will not send you any event before $t_1$, so you can proceed with processing any event you have with time-stamp $t_2$ where $t_2 < t_1$".

OoO PDES synchronization algorithms, with respect to how they deal with causality errors, have been classified into two categories: *conservative* and *optimistic* \cite{Fujimoto2015}.
Conservative mechanisms strictly avoid the possibility of any causality error ever occurring by means of model introspection and static analysis.
On the other hand, optimistic/speculative approaches use a detection and recovery approach: when causality errors are detected a rollback mechanism is invoked to restore the system.
An optimistic compared to a conservative approach will theoretically yield better performance in models where communication, thus the probability of causality errors, is below a certain threshold \cite{Fujimoto1990}.

Both groups present severe implementation difficulties.
For conservative algorithms, model introspection and static analysis tools might be very difficult to develop,
while the rollback mechanism of an optimistic algorithm may require complex entities, such as a hardware/software transactional memory \cite{Anane2015} .



* Methodology


\clearpage


* Out of Order PDES with MPI
The goal of this chapter is to present the process synchronization algorithm that will be applied and give their implementation using the MPI API.

In units [[The Chandy/Misra/Bryant synchronization algorithm]] and [[On Demand Synchronization]] we present the conservative synchronization algorithms that will be evaluated.
In unit [[Semantics of point-to-point Communication in MPI]] and [[MPI Communication Modes]] we present the semantics of the Message Passing Interface (MPI) communication primitives.
In unit [[MPI Realization of CMB]] we provide pseudo code for the realization of the CMB using the MPI communication primitives.
In unit [[Existing PDES]] we give an overview of prior art in the field of PDES in ESLD.


** The Chandy/Misra/Bryant synchronization algorithm
The synchronization algorithm at the heart of the proposed OoO PDES is known as the *Chandy/Misra/Bryant (CMB)* \cite{Bryant} \cite{Chandy1979}.
Historically, it has been the first of the family of conservative synchronization algorithms \cite{Fujimoto1990}.

According to the algorithm, the physical system to be simulated must be modeled as a number of communicating sequential *processes*.
The system's state, a set of variables, is distributed in a disjoint way, across the processes.
Computation is reactive; it is sparked by an event and produces further events and *side-effects* (changes in a subset of the system's variables).
Each process keeps its own perspective of simulated time through a *clock* variable.
The value of the clock is equal to the timestamp of the last event selected for computation.

Based on the system's state segregation, a static determination of which processes are interdependent can be established.
This is indicated by placing a *link* for each pair of dependent processes.
From a process' perspective a link can be either *outgoing*, meaning that events are sent via the link, or *incoming* meaning that events are received through it.
An incoming link must encapsulate a First-In-First-Out (FIFO) data structure for storing incoming events, in the order they are received.

The order by which events are received is *chronological*; non decreasing timestamp order.
This system-wide property is maintained by making each process select for computation the event that has the smallest timestamp.
A formal proof of how this local property *induces* a system-wide property can be found in \cite{Bryant} \cite{Chandy1979}.
Chronological reception of events is a necessary, but not sufficient, condition for ensuring *causality*.
The algorithm deals with the "is an event safe to execute" dilemma by forcing a process to *block* until each of its incoming links contains an event.
All the above are demonstrated in Listing \ref{alg:kernel}. 
The synchronization algorithm is realized as a process' main event loop.

#+BEGIN_LATEX
\begin{algorithm}
\caption{Process event loop, without deadlock avoidance}
\label{alg:initial_CMB}
\begin{algorithmic}[1]

   \While{process clock < some T}  
      \State \textbf{Block} until each incoming link FIFO contains at least one event
      \State Pop event M, with the \textbf{smallest} timestamp across all incoming links.
      \State Set process' \textbf{clock} = timestamp(M)
      \State \textbf{React} to event M
      \State \textbf{Communicate} resulting events over the appropriate links
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX


** Deadlock Avoidance
The naive realization of the process' event loop presented in Listing \ref{alg:kernel} leads to deadlock situations like the one depicted in Figure [[fig:deadlock]].
The queues placed along the outer loop are empty, thus simulation has halted, even though there are pending events (across the queues of the inner loop).
A global simulation moderator could easily detect deadlocks and allow the process, that has access to the event with the global minimum timestamp, to resume execution.
The presence of a moderator, however, would violate the distributed nature of the simulation, thus increasing the implementation complexity of the simulation environment.
Furthermore, 

For the context of this thesis, a distributed mechanism is more favorable.
What follows is a presentation of a distributed mechanism for overcoming these situations, referred to as the *null-event deadlock avoidance* \cite{Fujimoto1999}.

#+BEGIN_SRC ditaa  :file Figures/deadlock.png :cmdline -S --font "Times New Roman"
+--------------+
|ARL           |
|@2            |----------------------+
|              |                      |
|             8|<------------------+  |
|   ?          |                   |  |
+--------------+                   |  |
    ^  |                           |  |
    |  |                           |  |
    |  |                           |  |
    |  |                           |  |
    |  |                           |  |
    |  |                           |  |
    |  |                           |  :
    :  v                           |  v
+--------------+              +--------------+
|      5       |              |       ?      |
|              |------------->|6             |
|              |              |              |
|             ?|<=------------|              |
|              |              |              |
|CDG           |              |SKG           |
|@3            |              |@5            |
+--------------+              +--------------+
#+END_SRC
#+CAPTION: Deadlock scenario justifying the use of Null messages in the CMB
#+NAME: fig:deadlock
#+RESULTS:
[[file:Figures/deadlock.png]]

Figure demonstrates an air traffic simulation, where the airports (ARL, CDG and SKG) constitute the simulation processes.
The events exchanged between the airports model flights, the time unit being arbitrary.
At deadlock, every airport is at time 5.

Furthermore, it is assumed that there is an *a priori* knowledge conserning the flight time between airports.
This knowledge is referred to as the *lookahead* and takes the form of a function $lookahead:(PxP) \rightarrow time$
For example, by selecting the distance between every airport to be 3 time units, one can deduce the following:
since SKG is at 5 then ARL or CDG should not expect any event from SKG before 8.

To communicate this fact, SKG could create a special kind of event, a *null event*, with no data value, but with a timestamp 8 (clock+lookahead) and place it on its outgoing links.
A null event is still an event, so CDG would acknowledge it during the selection phase, thus being able to receive the flight from ARL.
CDG now sits at 5 and in the same fashion it could broadcast a null event with timestamp 8.
It is evident that the deadlock has been solved, at the expense of flooding the communication links with null events.


#+BEGIN_LATEX
\begin{algorithm}
\caption{Process event loop, with deadlock avoidance}
\label{alg:null-event}
\begin{algorithmic}[1]

   \While{process clock < some T}  
      \State \textbf{Block} until each incoming link FIFO contains at least one event
      \State Remove event M with the smallest timestamp from its FIFO.
      \State Set process' clock = timestamp(M)
      \State \textbf{React} to event M
      \State \textbf{Communicate} either a null or meaningful event to each outgoing link with timestamp = clock + lookahead
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX


** Criticism
The modified, for deadlock avoidance, algorithm is described in listing \ref{alg:null-event}.
The important points one must notice with this deadlock avoidance mechanism are that:
- Null events are created when a process updates its clock, that is upon processing an event.
- Each process propagates null events on all of its outgoing links.
- The efficiency of this mechanism is highly dependent on the designer's ability to determine sufficiently large lookaheads. 
- The lookahead must be a function 


** Semantics of point-to-point Communication in MPI
_There is a problem here: There are two sections. Semantics of Nonblocking and Blocking communications in the MPI manual_

The framework chosen for implementing the PDES is the *Message Passing Interface* 3.0 (MPI).
Events are modeled as structured messages, while event diffusion/communication as message passing.
MPI is a message passing library interface specification, standardized and maintained by the Message Passing Interface Forum \cite{citation}.
It is currently available for C/C++, FORTRAN and Java from multiple vendors (Intel, IBM, OpenMPI) \cite{citation}.
MPI addresses primarily the message passing parallel programming model, 
in which data is moved from the address space of one process to that of another process through cooperative operations on each process \cite{MessagePassingInterfaceForum2012}.

The basic communication primitives are the functions \texttt{MPI\_Send(...)} and \texttt{MPI\_Recv(...)}.
Their arguments specify, among others things, a data buffer and the peer process' or processes' unique id assigned by the MPI runtime.
By default, message reception is blocking, while message transmission may or may not block.
One can think of message transfer as consisting of the following three phases
1. Data is pulled out of the send buffer and a message is assembled
2. A message is transferred from sender to receiver
3. Data is pulled from the incoming message and disassembled into the receive buffer

*Order:*
Messages are non-overtaking.
If a sender sends two messages in succession to the same destination, 
and both match the same receive (a call to \texttt{MPI\_Recv}), 
then this operation cannot receive the second message if the first one is still pending. 
If a receiver posts two receives in succession,
and both match the same message, 
then the second receive operation cannot be satisfied by this message, if the first one is still pending. 
This requirement facilitates matching of sends to receives and also guarantees that message passing code is deterministic.

*Fairness:*
MPI makes no guarantee of fairness in the handling of communication. 
Suppose that a send is posted. 
Then it is possible that the destination process repeatedly posts a receive that matches this send, 
yet the message is never received, 
because it is each time overtaken by another message, 
sent from another source. 
It is the programmer’s responsibility to prevent starvation in such situations.

_COMMENT:_ Why did you choose MPI?


** MPI Communication Modes
The MPI API contains a number of variants, or *modes*, for the basic communication primitives.
They are distinguished by a single letter prefix (e.g. \texttt{MPI\_Isend(...)}, \texttt{MPI\_Irecv(...)}).
As dictated by the MPI version 3.0, the following communication modes are supported \cite{MessagePassingInterfaceForum2012}:

*No-prefix for standard mode: \texttt{MPI\_Send(...)}*
In this mode, it is up to MPI to decide whether outgoing messages will be buffered. 
MPI may buffer outgoing messages. 
In such a case, the send call may complete before a matching receive is invoked. 
On the other hand, buffer space may be unavailable, or MPI may choose not to buffer outgoing messages, for performance reasons. 
In this case, the send call will not complete, blocking the transmitting process, until a matching receive has been posted, and the data has been moved to the receiver.

*B for buffered mode: \texttt{MPI\_Bsend(...)}* 
A buffered mode send operation can be started whether or not a matching receive has been posted. 
It may complete before a matching receive is posted. 
However, unlike the standard send, this operation is local, and its completion does not depend on the occurrence of a matching receive. 
Thus, if a send is executed and no matching receive is posted, then MPI must buffer the outgoing message, so as to allow the send call to complete. 
A buffered send operation that cannot complete because of a lack of buffer space is erroneous. 
When such a situation is detected, an error is signaled that may cause the program to terminate abnormally. 
On the other hand, a standard send operation that cannot complete because of lack of buffer space will merely block, 
waiting for buffer space to become available or for a matching receive to be posted. 
This behavior is preferable in many situations. 
Consider a situation where a producer repeatedly produces new values and sends them to a consumer. 
Assume that the producer produces new values faster than the consumer can consume them. 
If buffered sends are used, then a buffer overflow will result. 
Additional synchronization has to be added to the program so as to prevent this from occurring. 

*S for synchronous mode: \texttt{MPI\_Ssend(...)}*
A send that uses the synchronous mode can be started whether or not a matching receive was posted. 
However, the send will complete successfully only if a matching receive is posted, and the receive operation has started to receive the message sent by the synchronous send.
Thus, the completion of a synchronous send not only indicates that the send buffer can be reused, 
but it also indicates that the receiver has reached a certain point in its execution, 
namely that it has started executing the matching receive. 
If both sends and receives are blocking operations then the use of the synchronous mode provides synchronous communication semantics: 
a communication does not complete at either end before both processes *rendezvous* at the communication point.

*R for ready mode: \texttt{MPI\_Rsend(...)}*
A send that uses the ready communication mode may be started only if the matching receive is already posted. 
Otherwise, the operation is erroneous and its outcome is undefined.
Ready sends are an optimization when it can be guaranteed that a matching receive has already been posted at the destination.
On some systems, this allows the removal of a hand-shake operation that is otherwise required and results in improved performance. 
A send operation that uses the ready mode has the same semantics as a standard send operation, or a synchronous send operation; 
it is merely that the sender provides additional information to the system (namely that a matching receive is already posted), that can save some overhead. 

Maybe you should consider non-blocking communication not as a *mode*.

*I for non-blocking mode: \texttt{MPI\_Isend(...)}, \texttt{MPI\_Ibsend(...)}, \texttt{MPI\_Issend(...)} and \texttt{MPI\_Irecv(...)*
Non-blocking message passing calls return control immediately (hence the prefix I), 
but it is the user's responsibility to ensure that communication is complete, 
before modifying/using the content of the data buffer.
It is a complementary communication mode that works en tandem with all the previous.
The MPI API contains special functions for testing whether a communication is complete, or even explicitly waiting until it is finished.


** MPI Realization of CMB
Listing \ref{alg:CMB_mpi} is a pseudo code, sketching out the CMB process event loop, using MPI's communication primitives.
#+BEGIN_LATEX
\begin{algorithm}
\caption{CMB Process event loop in MPI}
\label{alg:CMB_mpi}
\begin{algorithmic}[2]

   \While{process clock < some T}  
      \State post a MPI\_Irecv on each incoming peer process
      \State post a MPI\_Wait: block until every receive has been completed
      \State save each message received in a separate, per incoming link, FIFO.
      \State identify message M with the smallest time-stamp
      \State set clock = time-stamp(M)
      \State process message M
      \State post a MPI\_Issend to each outgoing link L with time-stamp = clock + Lookahead(clock,L,...)
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX

Applications have specific communication patterns

Also provides information about the application's communication behavior to the MPI implementation.

*Topology mapping*
_One of the major features of MPI's topology interface is that it can easily be used to adapt the MPI proces layout to the underlying network and system topology._

non cartesian topologies

What is the implementation type of the event?
Let us custom pack them in one 64 bit integer.
Extract them by mapping.

Since you always send an event to your neighbors, either a meaningfull one or a null, why not broadcast?


** Evaluation Metrics
The first evaluation metric of the proposed PDES implementation will be its performance against the reference SystemC kernel.
It will be measured by experimentation on the project's use case.

The simulation's size can be easily related to the duration of the simulation (in simulated time).
Another way of describing the simulation's size is through the conception of a formula involving the number of system processes, the number of links, the system's topology and the amount of events generated.

The accuracy of the simulation can be measured by the aggregate number of causality errors.
The detection of causality errors must be facilitated in a per process level and the aggregation shall be performed at the end of the simulation.
A concrete realization of the accuracy metric comes in the form of a counter each process increments whenever it executes an event with a time-stamp lower than its clock (the time-stamp of the last processed event).
Ideally, if the synchronization algorithms have been realized correctly, no causality errors should be detected.

_COMMENT:_ This section will become more concrete when we start experimentation.


** Existing PDES
The most important:
RISC: Recoding infrastructure for SystemC \cite{Liu2015}.

Miscellaneous:
SystemC-SMP \cite{Mello2010}
SpecC \cite{Domer2011}, although the latter is not meant for SystemC.
sc\_during \cite{Moy}

_COMMENT:_ This section is incomplete that should not be incomplete in an Intermediate report. 
Are you reinventing the wheel? 
Did you try at least one of these tools?
\clearpage

[]


* Analysis
\clearpage


* Conclusion and Future Work
The major contributions of this work can be found in Section [[Contributions]].
Section [[Limitations]] provides a list of actions that the author believes that should have been performed
This work is far from complete: The brave Theseus that would like to confront the minotaur can find Ariadne's thread in Section [[Future Work]]
Section [[Reflections]] revisits, in a more specific way, the cui bono question answered in Section [[Purpose]].

** Contributions
The following are the main research contributions of this work:
+ In Section [[The Discrete Event Model of Computation]] a different approach is adopted for presenting the DE MoC, 
  when compared to the reference work in MoCs by the Ptolemy Project [fn:ptolemy].
  It is the fact that time modeling is not included in the description of the DE MoC itself; 
  Time modeling is an implementation concern.
  For the abstract/mathematical description of the DE MoC, Lamport's "happens before" relationship \cite{Lamport1978} suffices in describing 
  the important concepts emanating (e.g. causality, concurrency and determinism).


** Limitations
+ The theoretical description of the DE MoC in section [[The Discrete Event Model of Computation]] is far from complete.
  It lacks of a Turing completeness proof.

+ Intel's Xeon Phi coprocessor was not used as an experimentation tool, despite this being specified as a primary objective in the project plan.
  Its Multiple Instruction Multiple Date (MIMD) architecture and its highly parametirized MPI implementation, makes it an ideal platform for performing the proposed OoO PDES simulation.
  However, we are able to report that SystemC 2.3.1 can be compiled with Intel's C++ compiler 16.0 for the Xeon Phi platform. 
  Moreover, the compiled package was verified againt the accompanying test suite. 

+ Not establishing an open communication channel with the following two scientists/engineers/researchers: Professor Rainer Dömer [fn:domer] and Dr. Jakob Engblom [fn:engblom]
  It is a researcher's ethical obligation towards society to take the initiative for disseminating his work.
  This work could be of some infinitesimal value towards the important, for the collective, work they do on ES design.
  Vice versa, their feedback would have greatly increased the quality of the work.


[fn:ptolemy] The Ptolemy Project, Center for Hybrid and Embedded Software Systems (CHESS),
             Department of Electrical Engineering and Computer Sciences, University of California at Berkeley: http://ptolemy.eecs.berkeley.edu/
[fn:domer] Professor Rainer Dömer works at the University of California Irvine, The Henry Samueli School of Engineering: http://www.cecs.uci.edu/~doemer/. 
           His current project \textit{Parallel SystemC Simulation on Many-Core computer architectures} is highly relevant to this thesis.
[fn:engblom] Dr. Jakob Engblom works as a Product Management Engineer at Intel in Uppsala: https://www.linkedin.com/in/jakobengblom.
           His academic research and professional experience with virtual platforms would be a significant source of feedback.



** Future Work
Unfortunately the library of the or1ksim is not reentrant and thus does not allow multiple instances of the core
simulator to be executed in one address space. Historically all data is stored in global variables.

In Section [[Delimitations]], the automatic compilation of a SystemC TLM 2.0 model into our proposed MPI implementation was indicated as a delimitation of this project.
However, it is the next logical step in progressing this work, since it has been deemed feasible.
Some general guidlines are:
+ For the critical task of analyzing the model (identifying the processes and the links between them),  ForSyDe SystemC's approach could be mimicked \cite{Hosein2012}.
  Using SystemC's well defined API for module hierarchy (e.g. \texttt{get\_child\_objects()}), along with the introduction of meta objects, the system's structure can be
  serialized at runtime, in the pre-simulation phase of elaboration.
+ After elaboration simulation should halt. The desirable outcome, propably in some XML format, was the serialization of the system's structure. 
  The proposed compiler can now use this abstract representation in conjunction with a library of code skeletons to generate the desired MPI implementation.

Although not relevant to the thesis, during the implementation of the cache hierarchy, the author has identified the need for an open-source framework for designing, documenting, implementing and testing FSMs.
[[http://perso.ensta-paristech.fr/~kielbasi/tikzuml/][TikZ-UML]] could serve as the front-end. 
It can express most of the UML 2.0 statechart defined concepts and produce a visual representation.
Since the syntax follows a structural manner, a compiler for the following backends could be developed:
+ [[http://nusmv.fbk.eu/][NuSMV]] for model checking by expressing requirements as temporal logic expressions.
+ [[http://www.state-machine.com/][Quantum Leaps]] can provide a well structured, easily maintained and tested C/C++ real-time implementation. 
Furthermore, [[http://orgmode.org/][Emacs' Org mode]] could be used for housing the compilation procedure, by unifying the editing of all the above representations of the FSM.
Emacs Org mode is more than a text editor: it is an ecosystem that enables the symbiosis of source code and document, in an unpresented way, that follows Donald Knuth concept of literate programming.
It is an indispensable tool when reproducibility is a desirable feature \cite{Schulte2011}.


** Reflections
On May the 3\textsuperscript{rd} 2016 the SystemC user community came together at Intel's headquarters in Munich, 
for a full-day workshop about the evolution of the various SystemC standards.
The event was called [[http://accellera.org/news/events/systemc-evolution-day-2016][SystemC Evolution Day 2016]]  [fn:PRES] and was organized by [[http://accellera.org/about][Accelera]], the organization responsible for advancing the language.
Professor Rainer Dömer gave a highly influential presentation titled \textit{"Seven Obstacles in the Way of Parallel SystemC Simulation"}, 
from where the following views can be induced:
+ A formal understanding of the DE MoC is needed.
+ The progression from sequential DES to PDES is of vital importance for the longevity of the language. 
  As Professor Dömer humorously remarks: \textit{"SystemC must embrace true parallelsim otherwise it will go down the same path as the dinosaurs"}

The fact that that this project's initiaton preceeds ($\sqsubset$) the event, can be regarded as an indication of proper alignment:
this project is organically bound to the ongoing discussion about SystemC's new major revision.

\clearpage

[fn:PRES] All presentations from the event are available at: [http://accellera.org/news/events/systemc-evolution-day-2016]


* References
\renewcommand\refname{}
\bibliography{References}
\bibliographystyle{myIEEEtran}
\clearpage


* Appendices                                                         :ignore:
#+BEGIN_EXPORT latex
\appendix
\section*{Appendices}
\addcontentsline{toc}{section}{Appendices}
\renewcommand{\thesubsection}{\Alph{subsection}}
#+END_EXPORT

** Producer Consumer :ignore:
#+BEGIN_EXPORT latex
\subsection{Producer Consumer Example in SystemC}
\label{AppendixA}
#+END_EXPORT

This example complements the presentation of the SysteC simulation engine in Section [[Sequential Discrete Event Simulation]].
It is an example of a producer-consumer system.
The producer communicates with the consumer via a fifo channel.
Its primary purpose is to demystify the way primitive channels are implemented in SystemC, by revealing their event driven nature.

These are the interfaces the channel must be able to provide to the actors: 
#+BEGIN_SRC cpp
  #include "systemc.h"
  #include <iostream>
  using namespace sc_core;

  //****************
  // FIFO Interfaces
  //****************
  // Fifo interface exposed to Producers
  class fifo_write_if: virtual public sc_interface
  {
  public:
      // blocking write
      virtual void write(char) = 0;
      // number of free entried
      virtual int numFree() const = 0;
  };
  // Fifo interface exposed to Consumers
  class fifo_read_if: virtual public sc_interface
  {
  public:
      // blocking read
      virtual char read() = 0;
      // number of available entries
      virtual int numAvailable() const = 0;
  };
#+END_SRC


Following, is the interface of the fifo channel, which internally acts like a circular buffer.
#+BEGIN_SRC cpp
  //*************
  // FIFO channel
  //*************
  class Fifo: public sc_prim_channel, public fifo_write_if, public fifo_read_if
  {
  protected:
      int   size;
      char *buf;
      int   free;
      int   ri; // read index
      int   wi; // write index
      int   numReadable, numRead, numWritten;
      // For notifying Producer and Consumer
      sc_event Ev_dataRead;
      sc_event Ev_dataWritten;
  public:
      // Constructor
      explicit Fifo(int _size=16):
	  sc_prim_channel(sc_gen_unique_name("thefifo"))
	  {
	      size = _size;
	      buf = new char[_size];
	      reset();
	  }
      // Destructor
      ~Fifo(){ delete [] buf; }
      int  numAvailable() const { return numReadable - numRead; }
      int  numFree() const { return size - numReadable; }
      void reset() { free=size; ri=0; wi=0; }
      void write(char c);
      char read();
      void update();
  };
#+END_SRC

Next we see how the channel realizes the blocking read and write interfaces.
The \texttt{read} and \texttt{write} methods are executed during the *evaluation phase*.
The co-routine that executes these methods will yield immediately if it reaches the \texttt{wait} statement.
When an event is passed as an argument to the \texttt{wait} function, the co-routine's sensitivity is said to change dynamically.
The \texttt{request\_update} method (inherited from \texttt{sc\_prim\_channel}) is a kernel callback. 
It signals the kernel that during the *update phase* he should execute the channel's \texttt{update} method.
#+BEGIN_SRC cpp
  // Blocking write implementation
  void Fifo::write(char c)
  {
      if (numFree() == 0)
	  wait( Ev_dataRead );
      numWritten++;
      buf[wi] = c;
      wi = (wi+1) % size; // Circular buffer
      free--;
      request_update();
  }
  // Blocking read implementation
  char Fifo::read()
  {
      if (numAvailable() == 0)
	  wait( Ev_dataWritten );
      numRead++;
      char temp  = buf[ri];
      ri = (ri+1) % size; // Circular buffer
      free++;
      request_update();
      return temp;
  }
#+END_SRC

Following, is the implementation of the \texttt{update} method, which is executed during the update phase by the kernel's.
A yielded (*blocked*) co-routine might end up in the *runnable* set if it has declared its sensitivity to the event being notified. 
#+BEGIN_SRC cpp
  // Update method called in the UPDATE phase of the simulation
  void Fifo::update()
  {
      if (numRead > 0)
	  Ev_dataRead.notify(SC_ZERO_TIME);
      if (numWritten > 0)
	  Ev_dataWritten.notify(SC_ZERO_TIME);
      numReadable = size - free;
      numRead = 0;
      numWritten = 0;
  }

#+END_SRC

Next we see the implementation of the producer and consumer modules.
The co-routine is declared sensitive (static sensitivity) to a clock's rising edge.
The co-routine that represents these modules executes the \texttt{run} function.
Since all co-routines are declared runnable at *elaboration*, 
they need to yield immediately after entering the function.
#+BEGIN_SRC cpp
class Producer: public sc_module
{
public:
    sc_port<fifo_write_if> out;
    sc_in<bool> clock;
    void run()
	{
	    while(1)
	    {
		wait(); // wait for clock edge
		out->write(1);
		cout << "Produced at: " << sc_time_stamp() << endl;
	    }
	}
    // Constructor
    SC_CTOR(Producer)
	{
	    SC_THREAD(run);
	    sensitive << clock.pos();
	}
};


class Consumer: public sc_module
{
public:
    sc_port<fifo_read_if> in;
    sc_in<bool> clock;
    void run()
	{
	    while(1)
	    {
		wait(); // wait for clock edge
		char temp = in->read();
		cout << "Consumed at: " << sc_time_stamp() << endl;
	    }
	}
    SC_CTOR(Consumer)
	{
	    SC_THREAD(run);
	    sensitive << clock.pos();
	}

};
#+END_SRC

Finally, the modules are linked with the fifo and their clock, and simulation is started.
#+BEGIN_SRC cpp
int sc_main(int argc, char *argv[])
{
    sc_clock clkFast("ClkFast", 1, SC_NS);
    sc_clock clkSlow("ClkSlow", 500, SC_NS);

    Fifo fifo1;

    Producer p1("p1");
    p1.out(fifo1);
    p1.clock(clkFast);

    Consumer c1("c1");
    c1.in(fifo1);
    c1.clock(clkSlow);

    sc_start(5000, SC_NS);
    
    return 0;
}
#+END_SRC




** Non-Determinism in SystemC :ignore:
#+BEGIN_EXPORT latex
\subsection{Non-Determinism in SystemC}
\label{AppendixB}
#+END_EXPORT

The following code example should in theory lead to non-deterministic behavior.
It models a race condition.
The system contains 3 processes which access a sharedVariable: 2 of them write it and 1 reads it.
At every clock pulse, all 3 processes are made *runnable*.
In practice however there is a repeatable pattern: processes are selected in the order in which their modules are instantiated.
If this holds, the one can draw the conclusion that logic time in SystemC has an implied third dimension: it is a vector $(t,n,p_{id}) \in \mathbb{N}^3$, and thus simulation events are totally ordered, which makes any simulation deterministic.
SystemC's LRM explicitly states:
\textit{"The order in which process instances are selected from the set of runnable processes is implementation-defined. However, if a specific version of a specific implementation runs a specific application using a specific input data set, the order of process execution shall not vary from run to run."}

#+BEGIN_SRC cpp
  #include "systemc.h"

  using namespace sc_core;


  std::string sharedVariable;

  SC_MODULE(chaos1)
  {
      sc_in<bool> clock;

      void run()
      {
	  while(1)
	  {
	      wait();
	      sharedVariable = "chaos";
	  }
      }

      SC_CTOR(chaos1)
      {
	  SC_THREAD(run);
	  sensitive << clock.pos(); // static sensitivity
      }
  };

  SC_MODULE(chaos2)
  {
      sc_in<bool> clock;

      void run()
      {
	  while(2)
	  {
	      wait();
	      sharedVariable = "and destruction";
	  }
      }

      SC_CTOR(chaos2)
      {
	  SC_THREAD(run);
	  sensitive << clock.pos(); // static sensitivity
      }
    
  };

  SC_MODULE(observer)
  {
      sc_in<bool> clock;

      void run()
      {
	  while(2)
	  {
	      wait();
	      cout << sharedVariable << endl;
	  }
      }

      SC_CTOR(observer)
      {
	  SC_THREAD(run);
	  sensitive << clock.pos(); // static sensitivity
      }
    
  };





  int sc_main(int argc, char *argv[])
  {
      sc_clock clock("clock", 1, SC_NS);
      chaos1 c1("c1");
      chaos2 c2("c2");
      observer ob("ob");
	
      c1.clock(clock);
      c2.clock(clock);
      ob.clock(clock);
    
      sc_start(2, SC_NS);
    
      return 0;
  }


#+END_SRC


* Use Case :noexport:
In this chapter we describe the transaction level model we are going to use for conducting our experimentation.
The purpose of the experimentation is twofold;
verify whether we achieve better faster simulation compared to the reference SystemC kernel and evaluate the proposed process synchronization algorithms.

** Cache Hierarchy Design
Caching shared data introduces a new problem because 
the view of memory held by two different processors is through their individual caches
which without any additional precautions could end up seeing two different values.

This difficulty is generally referred to as the cache coherence problem.

Notice that the coherence problem exists because we habe both a global state, defined primarily by
the main memory, and a local state, defined by the individual caches, which are private to each processor
core.

Thus in a multicore where some level of caching may be shared, while some levels are private
the coherence problem still exists and must be solved.

Informall we could say that a memory system is coherent if any read of a data item returns
the most recently written value of that data item

A program running on multiple processors will normally have copies of the same data in several caches.

The protocols to maintain coherence for multiple processors are called cache coherence protocols
Key to implementing a cache coherence protocol is tracking the state of any sharing of a data block.
There are two classes of protocols in use, each of which uses different techniques to track the sharing status.

Directory based: The sharing status of a particular block of physical memory is kept in one location, called the directory.
There are two very different types of directory-based cache coherence.
In an SMP, we can use one centralized directory, associated with the memory or some other single serialization point,
such as the outermost cache in a multicore.

A directory keeps the state of every block that may be cached.
Information in the directory includes which caches have copies of the block,
whether it is dirty and so on.

The simplest directory implementations associate an entry in the directory with each memory block.
In such implementations, the amount of information is proportional to the product of the number of memory blocks
times the number of nodes.
This overhead is not a problem for multiprocessors with less than a few hundred processors
because the directory overhead with a reasonable block size will be tolerable.

For efficiency reasons, we also track the state of each cache block at the individual caches.

The states and transitions for the state machine at each cache are identical to what we used for the snooping cache,
although the actions on a a transition are slightly different.
The processes of invalidating and locating an exclusive copy of a data item are different,
since they both involve communication between the requesting node and the directory and between
the directory and one or more remote nodes.
In a snooping protocol, these two steps are combined through the use of a broadcast to all the nodes.

We can start with simple state diagrams that show the state transitions for an individual cache block
and the examine the state diagram for the directory entry corresponding to each block in memory.

Presented as UML state machine diagram

#+BEGIN_LATEX
\begin{tikzpicture}

  \umlbasicstate[name=invalid, fill=white, anchor=north]{invalid}
  \umlbasicstate[name=shared, right=12cm of invalid-body.north, anchor=north, fill=white]{shared}
  \umlbasicstate[name=modified, below left=4cm and 4.5cm of shared-body.south, fill=white]{modified}
  \umlstateinitial[above=1cm of invalid, name=initial]

  \umltrans{initial}{invalid}

  % Invalid transition  
  \umltrans[arg={CPU\_read/}, pos=0.7, anchor1=30, anchor2=150]{invalid}{shared}
  \umlVHtrans[anchor2=150, arg={CPU\_write/}, pos=1.6]{invalid}{modified}

  % Shared transitions
  \umltrans[anchor1=170, anchor2=10, arg={invalidate/}, pos=0.7]{shared}{invalid}
  \umlVHtrans[anchor1=245, anchor2=30, arg={CPU\_write\_hit/}, pos=1.5]{shared}{modified}
  \umlVHtrans[anchor1=280, anchor2=5, arg={CPU\_write\_miss/}, pos=1.5]{shared}{modified}
  \umltrans[pos=1.2, arg={CPU\_read\_miss || CPU\_read\_hit}, recursive=90|10|3cm, recursive direction=top to right]{shared}{shared}

  % Modified transitions
  \umlVHtrans[pos=0.5, arg={CPU\_read\_miss/}, anchor1=90, anchor2=190]{modified}{shared}
  %\umlVHtrans[arg={fetch/}, anchor1=70, anchor2=210]{modified}{shared}
  \umlHVtrans[pos=0.75, anchor1=175, anchor2=245, arg={fetch\_invalidate/}]{modified}{invalid}
  \umltrans[pos=2.2, arg={CPU\_write\_miss}, recursive=-20|280|2.3cm, recursive direction=right to bottom]{modified}{modified}
  \umltrans[pos=1.5, arg={CPU\_read\_hit || CPU\_write\_hit}, recursive=260|200|4cm, recursive direction=bottom to left]{modified}{modified}

\end{tikzpicture}
#+END_LATEX

In a directory-based protocol, the directory implements the other half of the coherence protocol.
A message sent to a directory causes two different types of actions:
updating the directory state and sending additional messages to satisfy the request.
The states in the directory represent the three standard states for a block;
unlike in a snooping scheme, however, the directory state indicates the state of all the cached copies
of a memory block, rather than for a single cache block.

The memory block may be uncached by any node, cached in multiple nodes and readable (shared), or
cached exclusively and writable in exactly one node.
In addition to the state of each block, the directory must track the set of nodes that have a copy
of a block; we use a set called Sharers to perform this function.

we use a set called Sharers to perform this function. 
In multiprocessors with fewer than 64 nodes (each of which may represent four to eight times as many processors),
this set is typically kept as a bit vector.
Directory requests need to update the set Sharers and also read the set to perform invalidations.

The directory receives three different requests: read miss, write miss, and data write-back.
The messages sent in response by the directory are shown in bold,
while the updating of the set Sharers is shown in bold italics.
Because all the stimulus messages are external, all actions are shown in gray.
Our simplified protocol assumes that some actions are atomic, such as requesting a value and sending
it to another node; a realistic implementation cannot use this assumption.







** Platform modeling
A block diagram of the platform that will be modeled is seen in figure [[fig:Platform]].
The platform is a shared fmemory, cache-coherent, symmetric multiprocessor system based on the [[http://opencores.org/or1k/Or1ksim][OpenRisc 1000 Instruction Set Simulator]].
Cache coherence is enforced by a directory residing in the inclusive L2 cache.
Every component is/will be implemented in C/C++ and wrapped in SystemC modules using the TLM 2.0 API for communication. 
The exact number of processors is yet to be determined.

#+CAPTION: A model of a shared memory, cache-coherent, symmetric multiprocessor system
#+NAME: fig:Platform
[[file:Figures/platform.png]]


_COMMENT:_ Can you be more specific about the cache coherence protocol? Maybe provide a state diagram?



** Application modeling
We have the bare metal (newlib based) toolchain for compiling applications for the OpenRisc ISS.

_COMMENT;_ What kind of application am I going to run on this platform?
I see that most of the papers out there do some kind of mpeg2 decoding. That seems complex.


* SystemC TLM 2.0    :noexport:
Compared to a detailed hardware simulation, where communication is realized through a number of pin level events trigerring context switches inside the simulator, 
TLM 2.0 uses a single function call, thus speeding up simulation by orders of magnitude.

At the time of writing and to the best of our knowledge, we can not verify the existence of a comprehensive guide about system-level modeling with SystemC TLM 2.0.
Common practice among engineers that want to learn system-level modeling with SystemC TLM 2.0 is to attend courses offered by training companies.
Hence, we fill obliged to provide a quick introduction into the SystemC TLM 2.0 Loosely-Timed (LT) coding style, by means of a simple example.
The chapter assumes familiarity with C++ and SystemC.

In unit [[The Role of SystemC TLM 2.0]] we enumerate the features of the SystemC TLM 2.0 API.
In unit nomenclature
In units [[Transactions, Sockets, Initiators and Targets]] and [[Generic Payload]] we have a look at the fundamental notions of transaction, initiator and target components, socket and generic payload.
In unit [[Coding Styles and Transport Interfaces]] we present the two coding styles (Loosely Timed and Approximately Timed) and give their typical use cases.
In unit [[An Example]] we provide the implementation of a simple initiator, interconnect and target model.
In unit [[Criticism]] we present the dominant source of criticism for TLM 2.0.
Finally, in unit [[Simics and TLM 2.0]] we provide a comparison between the dominant industry frameworks for ESLD, Simics and SystemC TLM.


*** The Role of SystemC TLM 2.0
As stated in unit, a Transaction Level Model is considered a virtual platform where a software application can be mapped.
The TLM 2.0 API enhances SystemC's expressiveness in order to facilitate the *modular description* and *fast simulation* of virtual platforms.
As a language, unlike VHDL or SystemC, it is not meant for describing individual functional/architectural/system blocks/modules/components (henceforth *Intellectual Properperty (IP) blocks/modules/components*).
Its role is to make these individual blocks communicate with each other, as demonstrated in figure [[fig:tlm_as_wrapper]].

#+BEGIN_SRC ditaa :file Figures/tlm_as_wrapper.png :cmdline -S --font "Times New Roman"
                                                                           +-------------------------------+
                                                                           |                               |
 ------------------------------------------------------------------------> | Native SystemC module for bus |
         |                         |                          |            |                               |
         v                         v                          v            +-------------------------------+
+--------+----------+     +--------+----------+      +--------+----------+
|    TLM Wrapper    |     |    TLM Wrapper    |      |    TLM Wrapper    |
|                   |     |                   |      |                   |
|  +-------------+  |     |  +-------------+  |      |  +-------------+  |
|  |    ISS      |  |     |  |             |  |      |  |             |  |
|  |             |  |     |  |  Algorithm  |  |      |  |             |  |
|  |+-----------+|  |     |  |             |  |      |  |    VHDL     |  |
|  ||Object Code||  |     |  |    in C     |  |      |  |             |  |
|  |+-----------+|  |     |  |             |  |      |  |             |  |
|  +-------------+  |     |  +-------------+  |      |  +-------------+  |
+-------------------+     +-------------------+      +-------------------+
#+END_SRC

#+CAPTION: TLM 2.0 as a mixed language simulation technology
#+NAME: fig:tlm_as_wrapper
[[file:Figures/tlm_as_wrapper.png]]

System modularity is equivalent to individual IP block *interoperability*, enabling the reuse of IP components in a "plug and play" fashion.
TLM is relevant at every interface where an IP block needs to be plugged into a bus.
Having a library of verified IP blocks at his disposal, the engineer is able to create new virtual platforms fast and with a minimal effort. 

To be suitable for productive software development, a virtual platform needs to be fast, booting operating systems in seconds.
It also needs to be accurate enough such that code developed using standard tools on the virtual platform will run unmodified on real hardware. \cite{Leupers2010}
Compared to a standard RTL simulation, a TLM achieves a significant speed up by replacing communication through pin-level events with a single function call. 
TLM uses the simulation engine available with SystemC.

*** TLM 2.0 terminology
Apart from that, an LT interface looks much like any other remote function call interface.

TLM 2.0 classifies IP blocks as initiators, targets and interconnect components.
The terms initiator and target come forth as a replacement for the anachronistic terms master and slave.


An *initiator* is a component that initiates new transactions.
It is the initiator's duty to allocate memory for the transaction object or *payload*.
Payloads are always passed by reference.


A *target* component acts as the end point of a transaction. 
As such, it is responsible for providing a response to the initiator.
Request and response are combined into a single transaction object.
Thus, the target responds by modifying certain fields in the payload.


An *interconnect* component is responsible for routing a transaction on its way from initiator to target.
The route of a transaction is not pre-defined.
Routing is dynamic; it depends on the attributes of the payload, mainly its address field.
There is no limitiation on the number of interconnect components participating in a transaction. 
An initiator can also be directly connected to a target.
Since an interconnect can be connected to multiple initiator and target components, it must be able to perform *arbitration* in case transactions "collide".


The role of a component is not statically defined and it is not limited to one.
It is determined on a transactions basis. 
For example, it may function as an interconnect component for some transactions, and as a target for other transactions.


Transactions are sent through initiator *sockets*, and received through target sockets.
It goes without saying that an initiator component must have at least one initiator socket, a target component at least on target socket and a interconnet must possess both.
_Each initiator-to-target socket connection supports both a forward and a backward path by which interface methods can be called in either direction._

All the above terms are illustrated in figure [[fig:tlm_terminology]]

#+BEGIN_SRC ditaa :file Figures/tlm_terminology.png :cmdline -S -E --font "Times New Roman"
+-----------+ Initiator        +--------------+           Target +-----------+
|           | socket           |              |           socket |           |
|           +---+          +---+              +---+          +---+           |
| Initiator | > |--------->| > | Interconnect | > |--------->| > |  Target   |
|           +---+          +---+              +---+          +---+           |
|           |                  |              |                  |           |
+-----------+                  +--------------+                  +-----------+
      :                                ^                               ^
      |                                |                               |
      |                                |                               |
      v                                |                               |
+------------+                         |                               |
|            |                         :                               :
|  Payload   |-------------------------+-------------------------------/
|            |
+------------+
#+END_SRC

#+CAPTION: A basic TLM system
#+NAME: fig:tlm_terminology
#+RESULTS:
[[file:Figures/tlm_terminology.png]]

TLM 2.0 API \cite{OpenSystemCInitiative2009} consists of the following features ([[fig:TLM_features]]):
- A set of core interfaces
  - A Blocking interface which is coupled with the *Loosely-Timed (LT)* coding style.
  - A non-blocking interface, which is coupled with the *Approximately-Timed (AT)* coding style.
  - The *Direct Memory Interface (DMI)* to enable an initiator to have direct access to a target's memory, bypassing the usual path through the interconnect components used by the transport interfaces. 
  - The *Debug transport interface* to allow an non-intrusive inspection of the system's state.
- The *global quantum* used by the *temporal decoupling* mechanism of the LT coding style, which facilitates faster simulations by reducing the number of context switches performed by the kernel. 
- Initiator and target *sockets* to denote the links (causal dependencies) between processes.
- The *generic payload* which supports the abstract modeling of memory-mapped buses.
- A set of *utilities*, in the form of pre configured sockets and interconnect components, to facilitate the rapid development of models.

#+CAPTION: TLM 2.0 use cases (adopted from \cite{OpenSystemCInitiative2009}).
#+NAME: fig:TLM_features
[[file:Figures/TLM_features.png]]

*** Transactions, Sockets, Initiators and Targets
*Transactions* are non-atomic communications, normally with bidirectional data transfer, and consist of a set of messages that are usually modeled as atomic communications.
In a transaction one can distinguish two roles;
the *initiator*, the component which initiated the communication, and the *target*, the component which is supposed to service the initiator's request.
A component is not limited to either of these two roles; it can assume both.
For example, *interconnect* components encapsulate the behavior of memory-mapped buses, being responsible for routing transactions to the correct target.
From the initiator's perspective, they act as targets and from the target's perspective they act as initiators.

Implementation-wise, communication in TLM 2.0 is reduced to method calls, 
from the initiator to the target through an arbitrary number of interconnect component, without involving any context switches from the simulation kernel.

A component's role is signified by the type of *sockets* it contains.
Initiator sockets are used to forward method calls "up and out of" a component, while target sockets are used to allow method calls "down and into" a component \cite{doulos}.
Socket binding is the act of connecting components together, thus defining the component whose method call will be eventually executed to service the transaction.
From SystemC's viewpoint, a socket is basically a convenience class, wrapping a sc\_port and an sc\_export.



*** Coding Styles and Transport Interfaces
LT is suited for describing virtual platforms intended for software development.
However, where additional timing accuracy is required, typically for software performance estimation and architectural analysis use cases, the AT style is employed.
Virtual platforms typically do not contain many cycle-accurate models of complex components because of the performance impact. 

_COMMENT:_ This is a quite problematic section. You need to elaborate more, do not forget LT is on your thesis title. 

*** Generic Payload
The basic argument that is passed, by reference, in communicative method calls is called the *generic payload*.
It is a *structure* that encapsulates generic attributes relevant to a generic memory-mapped bus communication.
The structure possesses an extensions mechanism the designer can use to define more specific.

An *interoperable* TLM 2.0 component must depend only on the generic attributes of the generic payload.
The presence of attributes through the extension mechansim can be ignored without breaking the functionality of the model.
In such a case, the extensions mechanism carries simulation meta-data like pointers to module internal data structures or timestamps.


| Attribute           | Type                                | Modifiable        | Description                                                                                                                                                                                                                                                                  |
|---------------------+-------------------------------------+-------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Command             | \texttt{tlm\_command} (enum)         | No                | Set by the initiator to either \texttt{TLM\_READ} for read, \texttt{TLM\_WRITE} for write or TLM\_IGNORE to indicate that the command is set in the extensions mechanism.                                                                                                       |
| Address             | \texttt{uint64}                     | Interconnect only | Can be modified by interconnects since by definition an interconnect must bridge different address spaces.                                                                                                                                                                   |
| Data pointer        | \texttt{unsigned char*}             | No                | A pointer to the actual data being transfered.                                                                                                                                                                                                                               |
| Data length         | \texttt{unsigned int}               | No                | Related to the data pointer, indicates the number of bytes that are being transfered                                                                                                                                                                                         |
| Byte enable pointer | \texttt{unsigned char*}             | No                | A pointer to a byte enable mask that can be applied on the data (0xFF for data byte enabled, 0X00 for disabled)                                                                                                                                                              |
| Byte enable length  | \texttt{unsigned int}               | No                | Only relevant when the byte enable pointer is not null. If this number is less than the data length, the byte enable mask is applied repeatedly.                                                                                                                             |
| Streaming width     | \texttt{unsigned int}               | No                | Must be greated than 0. Largest address implied by the transaction is (address + streaming width - 1). Refer to *figure* for an example                                                                                                                                      |
| DMI hint            | \texttt{bool}                       | Yes               | A hint given to the initiator of whether he can bypass the transport interface and access a target's memory directly through a pointer.                                                                                                                                      |
| Response status     | \texttt{tlm\_response\_status} (enum) | Target only       | The initiator must set it to \texttt{TLM\_INCOMPLETE\_RESPONSE} prior to initiating the transaction. The target will set it to an appropriate value indicating the outcome of the transaction. For example for a successfull transaction the value is \texttt{TLM\_OK\_RESPONSE} |
| Extensions          | \texttt{tlm\_extension\_base*)[]      | Yes               | The mechanism for allowing the generic payload to carry protocol specific attributes                                                                                                                                                                                         |
  
*** An Example
This unit will provide a literate code listing for the model in figure [[fig:TLM_tutorial]]
#+CAPTION: A simple system-level model. The initiator, for example, could model a processor, the interconnect component a memory bus and the target a memory.
#+NAME: fig:TLM_tutorial
[[file:Figures/TLM_tutorial.png]]

*** Criticism
The root problem with TLM 2.0 lies in the elimination of explicit channels, which were a key contribution in the early days of research on system-level design.
As most researchers agreed, the concept of separation of concerns was of highest importance, 
and for system-level design in particular, this meant the clear separation of computation (in behaviors or modules) and communication (in channels).
Regrettably, SystemC TLM 2.0 chose to implement communication interfaces directly as sockets in modules and this indifference between channels and modules thus breaks the assumption of communication being safely encapsulated in channels.
Without such channels, there is very little opportunity for safe parallel execution \cite{Liu2015}.

For the above reason some designers consider TLM 2.0 a step towards the wrong direction and revert back to TLM 1.0.
Do you agree with this trend? 
Maybe tell us the major difference with TLM 1.0?

This is why SystemC TLM 2.0 model needs to be *recoded* to allow parallel execution.
The recoding must reconstitute the separation of concerns between computation and communication.
A modification of just the kernel will not suffice.

\clearpage





* Caches :noexport:

** Directory Based Cache Coherence
Avoid broadcast.

The absence of any centralized data structure that tracks the state of the caches is both the fundamental
advantage of a snooping-based scheme, since it allows it to be inexpensive, as well as its Achille's heel
when it comes to scalability.

The sharing status of a particular block of physical memory is kept in one location,
called the *directory*.
There are two very different types of directory-based cache coherence.
In an *SMP*, we can use one centralized directory, 
associated with the memory or some other _single serialization point_, such as the outermost cache in a multicore.

In a *DSM*, it makes no sense to have a single directory, since that would create a single point of contention
and make it difficult to scale to many multicore chips given the memory demands of multicores with eight or more cores.

A directory keeps the state of every block that may be cached.
Information in the directory includes:
    1. which caches (or collections of caches) have copies of the block
    2. whether it is dirty, and so on.

Within a multicore with a shared outermost cache (say, L3), it is *easy* to implement a directory scheme.
_Simply keep a bit vector of the size equal to the number of cores for each L3 block._
The bit vector indicates which private caches may have copies of a blockin L3, and invalidations are only sent to those caches.

This works perfectly for a single multicore if *L3 is inclusive*, 
and _this scheme is the one used in the Intel i7_.


*** Basics

Just as with snooping protocol, there are two primary operations that a directory protocol must implement:
    1. handling a read miss
    2. handling a write to a shared ( thus clean) cache block.

To implement these operatations, a directory must track the state of each cache block.
In a simple protocol, these states could be the following:
   1. *Shared:* One of more nodes have the block cached, and the value in memory is up to date (as well as in all the caches)
   2. *Uncached:* No node has a copy of the cache block.
   3. *Modified:* Exactly one node has a copy of the cache block, 
       and it has written the block, so the memory copy is out of date. 

In addition to tracking the state of each potentially shared memory block, 
we must track which nodes have copies of that block, 
since those copies will need to be invalidated on a write.

_The simplest way to do this is to keep a bit vector for each memory block._

We can also use the bit vector to keep track of the owner of the block when the block is in the exclusive state.
_For efficiency reasons, we also track the state of each cache block at the individual caches._

The states and transitions for the state machine at each cache are identical to what we used for the snooping cache,
although the actions on a transition are slightly different.
The processes of invalidating and locating an exclusive copy of a data item are different,
since the both involve communication between the requesting node and the directory 
and between the directory an one or more remote nodes.

In a snooping protocol, these two steps are combined through the use of a broadcast to all the nodes.



*** Coherence Messages

A catalog of the message types that may be sent between the processors and the directories
for the purpose of handling misses and maintaining coherence.

| Message Type        | Source         | Destination    | Message contents | Function of this message                                                                                            |
|---------------------+----------------+----------------+------------------+---------------------------------------------------------------------------------------------------------------------|
| 1. Read Miss        | Local cache    | Home directory | P, A             | Node P has a read miss at address A; request data and make P a read sharer                                          |
| 2. Write Miss       | Local cache    | Home directory | P, A             | Node P has a write miss at address A; request data and make P the exclusive owner                                   |
| 3. Invalidate       | Local cache    | Home directory | A                | Request to send invalidates to all remote caches that are caching the block at address A                            |
| 4. Invalidate       | Home directory | Remote cache   | A                | Invalidate a shared copy of data at address A                                                                       |
| 5. Fetch            | Home directory | Remote cache   | A                | Fetch the block at address A and sent it to its home directory; change the state of A in the remote cache to shared |
| 6. Fetch/invalidate | Home directory | Remote cache   | A                | Fetch the block at address A and send it to its home directory; invalidate the block in the cache                   |
| 7. Data value reply | Home directory | Local cache    | D                | Return a data value from the home memory                                                                            |
| 8. Data write-back  | Remote cache   | Home directory | A, D             | Write-back a data value for address A                                                                               |

- The first 3 messages are requests sent by the local node to the home.
- The 4 through 6 messages are messages sent to a remote node by the home 
  when the home needs the data to satisfy a read or write miss request.
- Data value replies are used to send a value from the home node back to the requesting node.
- Data value write-backs occur for two reasons: 
     a. When a block is replaced in a cache and must be written back to its home memory
     b. In reply to fetch or fetch/invalidate messages from the home.

_Writing back the data value whenever the block becomes shared_
simplifies the number of states in the protocol since 
     a. Any dirty block must be exclusive 
     b. Any shared block is always available in the home memory.
  

*** Protocol from the Cache's Side

The basic states of a cache block in a directory-based protocol are exactly like those in a snooping protocol.
Thus, we can start with simple state diagrams that show 
    1. The state transitions for *an individual cache block*
    2. The state for the *directory entry* corresponding to each block in memory.



*** Protocol from the Diretory's Side

A message sent to a directory  causes two different types of actions:
  1. Updating the directory state.
  2. Send additional messages to satisfy the request.  

The memory block may be 
  1. Uncached by any node, 
  2. Cached in multiple nodes and readable (shared).
  3. Cached exclusively and writable in exactly one node.

In addition to the state of each block, the directory must track the set of nodes that
have a copy of a block; we use a set called Sharers to perform this function.
_Directory requests need to update the set Sharers and also read the set to perform invalidations._

The directory receives three different requests: read miss, write miss, and data write-back.

_Our simplified protocol assumes that some actions are atomic, such as requesting a value and sending it to another node;
a realistic implementation cannot use this assumption_


** MESI
Adds the state *Exclusive* to the basic MSI protocol
to indicate when a cache block is resident only in a single cache but is clean.

If a block is in the *E* state, it can be written without generating any invalidates,
which optimizes the case where a block is read by a single cache before being written by that same cache.

Of course, when a *read miss* to a block in the *E* state occurs, the block must be changed
to the *S* state to maintain coherence.

Because all sugsequent accesses are snooped, it is possible to maintain the accuracy of this state.
In particular, if another processor issues a read miss, the state is changed from exclusive to shared.
The advantage of adding this state is that a subsequent write to a block in the exclusive state
by the same core need not acquire bus access or generate any invalidate, since the block is known to be
exclusively in this local cache; the processor merely changes the state to modified.

This state is easily added by using the bit that encodes the coherent state as an exclusive state
and using the dirty bit to indicate that a block is modified.

The Intel i7 uses a variant of a MESI protocol, called MESIF, which adds a state (Forward) to designate
which sharing processor should respond to a request. It is designed to enhance performance in distributed
memory organizations.


* Graveyard :noexport:
A common practice among modern system-level design tools/methodologies, like Intel's CoFluent Studio, is for the designer to construct two intermediate models;
An application model, that is the behavioral view of the system and 
a platform model, assembled using a component database of Processing Elements (PE, processors, hardware accelerators etc) and Communication Elements (CE, buses, interfaces etc).
The final step towards *system-level synthesis*, that is the transition from a behavioral to a structural model on the system level, is called system mapping;
the partitioning of the application to the elements of the platform.


* Refinements :noexport:
+ Chapter Titles bigger, occupying half of the page
+ Increase a spacing between lines
+ Header for every page
+ Section about FSM, put footnotes
+ The "Ingo Paradox"
