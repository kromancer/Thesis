#+TITLE: Parallel Simulation of SystemC Loosely-Timed Transaction Level Models
#+AUTHOR:Konstantinos Sotiropoulos
#+EMAIL: kisp@kth.se
#+STARTUP: overview
#+OPTIONS: broken-links:mark

* Abstract
The vision of a connected and automated society, the Internet of Things era has promised,
is depending on the industry's ability to design novel and complex electronic systems,
while maintaining a short time to market.
One of the first steps in the design of such systems is the in tandem simulation of hardware and software.
Transaction Level Models, expressed in the SystemC modeling language, can facilitate this co-simulation.
However, the sequential nature of the SystemC's Discrete Event simulation kernel is a limiting factor.
Poor simulation performance often constraints the scope and depth of the design decisions that can be evaluated.

The increase in computing power, modern processing units deliver, is only available for applications that can expose parallel operations.
The major obstacle one faces, when trying to parallelize a simulation, is the preservation of causality; simulation events need to be processed in a chronological order.

It is the main objective of this thesis' project to demonstrate the feasibility of parallelizing the simulation of Transaction Level Models, outside SystemC's reference simulation environment.
The difficult task of achieving causal, yet parallel, processing of simulation events, is accomplished by using proper process synchronization mechanisms.
Our proposed implementation does not depend on the presence of a centralized simulation moderator. 
It is implemented using the Message Passing Interface 3.0 framework.
To demonstrate our approach and evaluate different process synchronization algorithms,
we use the model of a cache-coherent, symmetric multiprocessor based on the OpenRisc 1000 Instruction Set Simulator. Our results indicate a significant speedup against the reference SystemC simulation.

*Keywords:* parallel discrete event simulation, conservative synchronization algorithms, transaction level models, SystemC TLM 2.0

* Maguire's Notes for Writing an Abstract 			   :noexport:
_1. What is the topic area?_
The vision of a connected and automated society, 
the Internet of Things era has promised,
is depending on the industry's ability 
to design novel and complex electronic systems,
while maintaining a short time to market.


_2. Short problem statement_
One of the first steps in the design of such systems is the in tandem simulation of hardware and software.
Transaction Level Models, expressed in the SystemC modeling language, can facilitate this co-simulation.
However, the sequential nature of the SystemC's Discrete Event simulation kernel is a limiting factor.
Poor simulation performance often constraints the scope and depth of the design decisions that can be evaluated.


_3. Why was this problem worth a Master's thesis project? Why no one else solved it yet?_
The increase in computing power, modern processing units deliver, is only available for applications that can expose parallel operations.
The major obstacle one faces, when trying to parallelize a simulation, is the preservation of causality; simulation events need to be processed in a chronological order.


_4. How did you solve the problem?_
It is the main objective of this thesis' project to demonstrate the feasibility of parallelizing the simulation of Transaction Level Models, outside SystemC's reference simulation environment.
The difficult task of achieving causal, yet parallel, processing of simulation events, is accomplished by using proper process synchronization mechanisms.
Our proposed implementation does not depend on the presence of a centralized simulation moderator. 
It is implemented using the Message Passing Interface 3.0 framework.



_5. Results/Conclusions/Consequences/Impact:_
   _What are your key results/conclusions?_
   _What will others do based upon your results?_
   _What can be done now that you have finished - that could not be done before your thesis project was completed?_

To demonstrate our approach and evaluate different process synchronization algorithms,
we use the model of a cache-coherent, symmetric multiprocessor based on the OpenRisc 1000 Instruction Set Simulator.
Our results indicate a significant speedup against the reference SystemC simulation.

* Acronyms 							   
| *ASIC*:  | Application Specific Integrated Circuit |
| *DE*:    | Discrete Event                          |
| *DES*:   | Discrete Event Simulator/Simulation     |
| *DMI*:   | Direct Memory Interface                 |
| *ES*:    | Electronic System                       |
| *ESLD*:  | Electronic System-Level Design          |
| *FPGA*:  | Field Programmable Gate Array           |
| *HDL*    | Hardware Description Language           |
| *HPC*:   | High Performance Computing              |
| *IC*     | Integrated Circuit                      |
| *IP*     | Intellectual Property                   |
| *MoC*:   | Model of Computation                    |
| *MPI*    | Message Passing Interface               |
| *MPSoC*: | Multicore System on Chips               |
| *OoO*:   | Out-of-Order                            |
| *PDES*:  | Parallel Discrete Event Simulation      |
| *SLDL*:  | System-Level Design Language            |
| *SMP*:   | Symmetric Multiprocessing               |
| *SoC*:   | System on Chip                          |
| *SR*:    | Synchronous Reactive                    |
| *TLM*:   | Transaction Level Modeling              |
| *CMB*:   | Chandy/Misra/Bryant algorithm           |
\clearpage


* Preface 							
This is a Master's Thesis project that will be carried out in Intel Sweden AB and is supervised by KTH's ICT department.
Mr. Bjorn Runaker (\texttt{bjorn.runaker@intel.com}) is the project's supervisor from the company's side, 
while professor [[https://people.kth.se/~ingo/][Ingo Sander]] (\texttt{ingo@kth.se}) and PhD student [[http://people.kth.se/~ugeorge/][George Ungureanu]] (\texttt{ugeorge@kth.se}) are the examiner and supervisor from KTH. 
The project begun on 2016-01-16 and will finish on 2016-06-30, as dictated by the contract of employment that I, Konstantinos Sotiropoulos a Master's student at the Embedded Systems program, have signed with the company.

The scope of this project has been mutually agreed on and dialectically determined between the company's needs and the institute's research agenda.
As Master's Thesis project, it will expose a scientific ground on which the engineering effort shall be rooted.
 
All the necessary equipment (software and hardware) has been kindly provided by the company.
The exact legal context that will apply to any software produced as a result of this project is yet to be determined, 
but will conform to the general context dictated by the documents already signed (documents' titles:  "Statement of Terms and Conditions of Fixed Term Employment" and "Employee Agreement") .
\clearpage

* Introduction
The aim of this chapter is to present the general context of the problem statement;
that is the engineering discipline of *Electronic System-Level Design (ESLD)*.

In unit [[The Design Process]] we provide a definition for the fundamental concepts of design, system, model and simulation.
In units [[Electronic Systems Design]] to [[Transaction-Level Model]], using Gajski and Kuhn's Y-Chart, we determine the concept of a Transaction-Level Model, as an instance in the engineering practice of Electronic System-Level Design (ESLD).
In unit [[SystemC and TLM]] we have a rudimentary look on SystemC's role in ESLD.
Unit [[Motivation]] gives the raison d'\'etre of this project.
The structure of this document is given in unit [[Document Overview]].


** The Design Process
We define the process of *designing* as the engineering art of incarnating a desired functionality into a perceivable, thus concrete, artifact.
An engineering artifact is predominantly referred to as a *system*, 
to emphasize the fact that it can be viewed as a structured collection of components and that its behavior is a product of the interaction among its components.

Conceptually, designing implies a movement from abstract to concrete, fueled by the engineer's *design decisions*, incrementally adding implementation details.
This movement is also known as the *design flow* and can be facilitated by the creation of an arbitrary number of intermediate artifacts called models.
A *model* is thus an abstract representation of the final artifact. 
The design flow can be now semi-formally defined as a process of model refinement, with the ultimate model being the final artifact itself.
We use the term semi-formal to describe the process of model refinement, because to the best of our knowledge, 
such model semantics and algebras that would establish formal transformation rules and equivalence relations are far from complete \cite{Gajski2009}.

A desired property of a model is executability that is its ability to demonstrate portions of the final artifact's desired functionality in a controlled environment.
An *executable model*, allows the engineer to form hypotheses, conduct experiments on the model and finally evaluate design decisions.
It is now evident that executable models can firmly associate the design process with the scientific method.
The execution of a model is also known as *simulation* \cite{Editor2014}.


** Electronic Systems Design
An Electronic System (ES) provides a desired functionality, by manipulating the flow of electrons.
Electronic systems are omnipotent in every aspect of human activity; 
most devices are either electronic systems or have an embedded electronic system for their cybernisis.

The prominent way for visualizing the ES design/abstraction space is by means of the Y-Chart.
The concept was first presented in 1983 \cite{Gajski1983} and has been constantly evolving to capture and steer industry practices.
Figure \ref{fig:Y-Chart} presents the form of the Y-Chart found in \cite{Gajski2009}.

#+CAPTION: The Y-Chart (adopted from \cite{Gajski2009})
#+NAME: fig:Y-Chart
[[file:Figures/y-chart.png]]


The Y-Chart quantizes the design space into four levels of abstraction; system, processor, logic and circuit, represented as the four concentric circles.
For each abstraction level, one can use different ways for describing the system; behavioral, structural and physical.
These are represented as the three axises, hence the name Y-Chart.
Models can now be identified as points in this design space.

A typical design flow for an Integrated Circuit (IC) begins with a high-level behavioral model capturing the system's specifications and 
proceeds non-monotonically to a lower level structural representation, expressed as a netlist of, still abstract, components.
From there, Electronic Design Automation (EDA) tools will pick up the the task of reducing the abstraction of a structural model by translating the netlist of abstract components to a netlist of standard cells.
The nature of the standard cells is determined by the IC's fabrication technology (FPGA, gate-array or standard-cell ASIC).
Physical dimensionality is added by place and route algorithms, 
part of an EDA framework, 
signifying the exit from the design space, 
represented in the Y-Chart by the transition from the structural to the physical axis.

We have used the adjective non-monotonic to describe the design flow, because as a movement in the abstraction space, it is iterative; 
design \rightarrow test/verify \rightarrow redesign or proceed.
This cyclic nature of the design flow is implied by the errors the human factor introduces, under the lack of formal model transformation methodologies in the upper abstraction levels.
The term *synthesis* is therefore introduced to describe a monotonic movement from a behavioral to a structural model, or the realization of an upper level structural model using finer components.
We distinguish synthesis from the general case of the design flow, to disregard the testing and verification procedures.
Therefore, the term synthesis may indicate the presence, or the desire of having, an automated design flow.
Low-level synthesis is a reality modern EDA tools achieve, while high-level synthesis is still a utopia modern tools are converging to.



*** Notes for completing this section 				   :noexport:
Explain processor, logic, circuit, structural, behavioral, etc
Physical dimensionality added by automated place and route software.
geometrical positioning of the components on the silicon wafer
The final result is a from the chosen device technology (standard cell ASIC, gate array ASIC, FPGA)
Verification and validation 
High-Level synthesis


** System-Level Design
To meet the increasing demand for functionality, ES complexity, as expressed by their heterogeneity and their size, is increasing.
Terms like Systems on Chip (SoC) and Multi Processor SoC (MPSoC), used for characterizing modern ES, indicate this trend.
With abstraction being the key mental ability for managing complexity, the initiation of the design flow has been pushed to higher abstraction levels.
In the Y-Chart the most abstract level, depicted as the outer circle, is the system level.
At this level the distinction between hardware and software is a mere design choice thus *co-simulation of hardware and software* is one of the main objectives.
Thereby the term *system-level design* is used to describe design flows that enter the design space at this level.

A common practice among modern system-level design tools/methodologies, 
like Intel's CoFluent Studio \cite{citation}, 
is for the designer to construct two intermediate models;
An application model, that is the behavioral view of the system and 
a platform model, assembled using a component database of Processing Elements (PE, processors, hardware accelerators etc) and Communication Elements (CE, buses, interfaces etc).
The final step towards *system-level synthesis*, that is the transition from a behavioral to a structural model on the system level, is called system mapping;
the partitioning of the application to the elements of the platform.



** Transaction-Level Model
A *Transaction-Level Model* (TLM) can now be defined as the point in the Y-Chart where the structural axis meets the system abstraction level.
As mentioned in the previous unit, a TLM can be thought of as a *virtual platform*, where an application can be mapped \cite{Rigo2011}.
It is a fully functional software model of a complete system that facilitates *co-simulation of hardware and software*.

There are three pragmatic reasons that stimulate the development of a transaction level model.
At first, a TLM serves as a testbed for *architectural exploration* in order to tune the overall system architecture prior to detailed design.
Secondly, software engineers must be equipped with a virtual platform they can use for *software development*, early on in the design flow, without needing to wait for the actual silicon to arrive.
The need for performing software and hardware development in parallel, is due to the facts that an increasing amount of an ES's functionality is becoming software based and ES related companies are facing the economical pressure of reducing new products' time to market.
Finally, a TLM can be a reference model for hardware *functional verification*, that is, a golden model to which an RTL implementation can be compared.


** SystemC and TLM
One fundamental question, for completing the presentation of ESLD, remains; How can executable models be expressed on the system level?
While maintaining the expressiveness of a Hardware Description Language (HDL), *SystemC* is meant to act as an *Electronic System Level Design Language* (ESLDL); a language where both RTL and system-level models can be expressed.
It is implemented as a C++ class library, thus its main concern is to provide the designer with executable rather than synthesizable models.
The language is maintained and promoted by Accellera (former Open SystemC Initiative OSCI) and has been standardized (IEEE 1666-2011 \cite{OpenSystemCInitiative2012}).

A major part of SystemC is the TLM 2.0 library, which is exactly meant for expressing TLMs.
Despite introducing different language constructs, TLM 2.0 is still a part of SystemC because it depends on the same simulation engine.
TLM 2.0 has been standardized seperately in \cite{OpenSystemCInitiative2009}.
Compared to a RTL simulation, where communication is realized through a number of pin level events trigerring context switches inside the simulator, TLM 2.0 uses a single function call, thus speeding up simulation by orders of magnitude, at the expense of accuracy.



** TODO Motivation
Faster simulation



** Document Overview
This unit be completed in the end
\clearpage

* Formulating The Problem Statement 
The aim of this chapter is to present a theoretical framework that will eventually lead to the formulation of the problem statement.
Picking up Ariadne's thread from the introduction, this chapter begins its journey by the fact that SystemC is an Electronic System-Level Design *Language* (ESLDL) for expressing system-level models.

In unit [[Models of Computation]] we link the concepts of operational semantics and Models of Computation (MoC) with that of the ESLDL.
In units [[The Discrete Event Model of Computation]] and [[The Discrete Event Simulation(or)]] the SystemC simulation engine or kernel is presented as an algorithm that realizes the operational semantics of a Discrete Event (DE) MoC.
Units [[Parallel Discrete Event Simulation(or)]] and [[Causality and Synchronization]] introduce the concept of Parallel Discrete Event Simulation (PDES) and present the fundamental causality hazards it introduces.
The prime concern of this thesis' is presented in a concise way in [[Problem statement]].
Unit [[Objectives]] introduces the objectives, that is the engineering endeavor of this project.

** Models of Computation
A *language* is a set of symbols, rules for combining them (its syntax), and rules for interpreting combinations of symbols (its semantics). 
Two approaches to semantics have evolved: denotational and operational.
*Operational semantics*, which dates back to Turing machines, gives the meaning of a language in terms of actions taken by some abstract machine. 
How the abstract machine in an operational semantics can behave is a feature of what we call the *Model of Computation (MoC)* \cite{Edwards1997}.
This definition implies that languages are not computational models themselves, but have underlying computational models \cite{Jantsch2005}.

How does the concept of a MoC fit specifically in ESLDLs?
Above all the engineer needs executable models.
Furthermore, an ESLDL describes an electronic artifact as a system; a (hierarchical) network of interacting components.
Therefore, a MoC is a collection of rules to define what constitutes a component and what are the semantics of execution, communication and concurrency of the abstract machine that will execute the model \cite{Jantsch2005} \cite{Editor2014}.
To ensure meaningful simulations, the MoC of the abstract machine that simulates a model must be equivalent with that of the abstract machine that will realize the system.

#+CAPTION: Categorization of three of the most explored MoCs: State Machine, Synchronous Dataflow and Discrete Event(adopted from \cite{Editor2014})
#+NAME: fig:MoCs
[[file:Figures/MoCs.pdf]]


** The Discrete Event Model of Computation
The dominant MoC that underlies most industry standard HDLs (VHDL, Verilog, SystemC) is the *Discrete Event (DE)* MoC.
The components of a DE system are called *processes*.
In this context processes usually model the behavior and functionality of hardware entities.
The execution of processes is concurrent and the communication is achieved through *events*.
An event can be considered as a time-stamped value.
Computation is reactive

Concurrent execution does not imply parallel/simultaneous execution. 
The notion of *concurrency* is more abstract. 
Depending on a machine's computational resources, it can be realized as either parallel/simultaneous execution or as sequential interleaved execution.

Systems whose semantics are meant to be interpreted by a DE MoC, in order to be realizable, must have a *causal* behavior: they must process events in a chronological order, 
while any output events produced by a process are required to be no earlier in time than the input events that were consumed \cite{Editor2014}.
At any moment in real time, the model's time is determined by the last event processed.

In figure [[fig:MoCs]] one can observe that the DE MoC is also considered to be *Synchronous-Reactive (SR)*. 
This demonstrates the possibility of the MoC to "understand" entities with zero execution time, where output events are produced at the same time input events are consumed.
We can also extend/rephrase the previous definitions and say that Synchronous-Reactive MoCs are able to handle, in a causal way, systems where events happen at the same time, instantaneously.
The DE MoC handles the aforementioned situations by extending time-stamps(the notion of simulated time) with the introduction of delta delays (also referred to as cycles or micro-steps).
A delta delay signifies an infinitesimal unit of time and no amount of delta delays, if summed, can result in time progression.
A time-stamp is therefore represented as a tuple of values, $(t,n)$ where $t$ indicates the model time and $n$ the number of delta delays that have advanced at $t$.




** The Discrete Event Simulation(or)
A realization of the DE abstract machine is called a *Discrete Event Simulator (DES)*.
SystemC's reference implementation of the DES is referred to as the *SystemC kernel* \cite{OpenSystemCInitiative2012}.

Concurrency of the system's processes is achieved through the co-routine mechanism (also known as co-operative multitasking). 
Processes execute without interruption. In a single core machine that means that only a single process can be running at any (real) time, 
and no other process instance can execute until the currently executing process instance has yielded control to the kernel.
A process shall not preempt or interrupt the execution of another process \cite{OpenSystemCInitiative2012}.

To avoid quantization errors and the non-uniform distribution of floating point values, time is expressed as an integer multiple of a real value referred to as the time resolution. 

The kernel maintains a *centralized event queue* that is sorted by time-stamp and knows which process is *running*, which are *runnable*, and which processes are waiting for events.
Runnable processes have had events to which they are sensitive triggered and are waiting for the running process to yield to the kernel so that they can be scheduled.
The kernel controls the execution order by selecting the earliest event in the event queue and making its time-stamp the current simulation time.
It then determines the process the event is destined for, and finds all other events in the event queue with the same time-stamp that are destined for the same process \cite{Black2010}.
The operation of the kernel is exemplified in listing \ref{alg:kernel}.

#+BEGIN_LATEX
\begin{algorithm}
\caption{SystemC event loop, adopted from \cite{Schumacher2010}}
\label{alg:kernel}
\begin{algorithmic}[1]

   \While{timed events to process exist}  \Comment{Simulation time progression}
      \State trigger events at that time
      \While {runnable processes exist}   \Comment{Delta cycle progression}
         \While {runnable processes exist}
	     \State run all triggered processes
             \State trigger all immediate notifications
         \EndWhile
         \State update values of changed channels
	 \State trigger all delta time events
       \EndWhile
       \State advance time to next event time
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX

*** Concepts mentioned that have not been adequately explained 	   :noexport:
co-routines; maybe show how to implement co-routines in pthreads?



** Parallel Discrete Event Simulation(or)
The previous section has made evident that the reference implementation of the SystemC kernel assumes sequential execution and therefore can not utilize modern massively parallel host platforms. 
The most logical step in achieving faster simulations is to realize concurrency, from interleaved process execution to simultaneous/parallel execution.
By assigning each process to a different processing unit of the host platform (core or hardware thread) we enter the domain of *Parallel Discrete Event Simulation (PDES)*.
After making the strategical decision that for improving a DE simulator's performance one must orchestrate parallel execution, 
the first tactical decision encountered
is whether to keep a single simulated time perspective, 
or distribute it among processes.

For PDES implementations that enforce global simulation time, the term *Synchronous PDES* has been coined in \cite{Schumacher2010}.
In Synchronous PDES, parallel execution of processes is performed within a delta cycle. 
With respect to Alg \ref{alg:kernel}, we can say that a Synchronous PDES parallelizes the execution of the innermost loop (line 4).
However, as we will see in later sections, this approach will bare no fruits in the simulation of TLM Loosely Timed simulations, since delta cycles are never triggered \cite{Chen2012}.
Therefore, we switch our interest in *Out-of-Order PDES (OoO PDES)* \cite{Chen2015}; 
allowing each process to have its own perception of simulated time, determined by the last event it received.




*** Specify "later sections" :noexport:




** Causality and Synchronization 
The distribution of simulation time opens Pandora's box. 
Protecting the OoO PDES from *causality errors* demands certain assumptions and the addition of complex implementation mechanisms.

The first source of causality errors arises when the system's state variables are not distributed, in a disjoint way, among the processes \cite{Fujimoto1990}.
A trivial realization of the above scenario is depicted in figure [[fig:causality_shared_state]]. Processes $P_1$ and $P_2$ are executing simultaneously, while sharing the system's state variable $x$.
Events $E_1$ and $E_2$ are executed by $P_1$ and $P_2$ respectively. If we assume that in real time $E_2$ is executed before $E_1$, then we have implicitly broken causality, since $E_1$ might be influenced
by the value of $x$ that the execution of $E_2$ might have modified. Furthermore, one must observe that this kind of implicit interaction between $P_1$ and $P_2$ can not be expressed in a DE MoC. 
This is a meta-implication of the host platform's shared memory architecture.

#+CAPTION: Causality error caused by the sharing of the system's state variable $x$ by $P_1$ and $P_2$.
#+NAME: fig:causality_shared_state
[[file:Figures/causality_shared_state.png]]

The second and most difficult to deal with source of causality errors is depicted in figure [[fig:causality_safe_events]]. 
Event $E_1$ affects $E_2$ by scheduling a third event $E_3$ which, for the sake of argument, modifies the state of $P_2$. 
This scenario necessitates sequential execution of all three events. 
Thus the fundamental problem in PDES, in the context of this scenario, becomes the question: how can we deduce that it is safe to execute $E_2$ in parallel with $E_1$, without actually executing $E_1$ \cite{Fujimoto1990}?
However, one must notice that the kind of interaction that yields this problematic situation is explicitly stated in the model.

#+CAPTION: Causality error caused by the unsafe execution of event $E_2$ (adopted from \cite{Fujimoto1990}).
#+NAME: fig:causality_safe_events
[[file:Figures/causality_safe_events.png]]

The last example makes evident the fact that the daunting task of preserving causality in the simulation is all about *process synchronization*.
For example, each process must be able to communicate to each of its peers (processes that is linked with) the message: 
"I will not send you any event before $t_1$, so you can proceed with processing any event you have with time-stamp $t_2$ where $t_2 < t_1$".

OoO PDES synchronization algorithms, with respect to how they deal with causality errors, have been classified into two categories: *conservative* and *optimistic* \cite{Fujimoto2015}.
Conservative mechanisms strictly avoid the possibility of any causality error ever occurring by means of model introspection and static analysis.
On the other hand, optimistic/speculative approaches use a detection and recovery approach: when causality errors are detected a rollback mechanism is invoked to restore the system.
An optimistic compared to a conservative approach will theoretically yield better performance in models where communication, thus the probability of causality errors, is below a certain threshold \cite{Fujimoto1990}.

Both groups present severe implementation difficulties.
For conservative algorithms, model introspection and static analysis tools might be very difficult to develop,
while the rollback mechanism of an optimistic algorithm may require complex entities, such as a hardware/software transactional memory \cite{Anane2015} .


** Problem statement
The prime concern of this project can now be stated;
an evaluation of the efficiency of existing conservative process synchronization algorithms when applied to the parallel simulation
of Loosely-Timed Transaction Level Models.


** Objectives
If the timing constraints stretched beyond the scope of a Master Thesis, 
the project's self-actualization would require the development/production of the following components (sorted in descending significance order):
1. At least two OoO PDE simulation mechanisms implementing proposed conservative synchronization algorithms.
2. A proof of concept application of the proposed mechanism, on a sufficiently parallel TLM model.
3. A static analysis/introspection tool for parsing the SystemC description of the model and extracting a pure representation in XML.
4. A code generation tool for realizing the model outside SystemC.
For the critical task of analyzing the model, identifying the processes and the links between them, we will follow ForSyDe SystemC's approach \cite{Hosein2012}.
Using SystemC's well defined API for module hierarchy (e.g. \texttt{get\_child\_objects()}), along with the introduction of meta objects, the system's structure can be
serialized at runtime, in the pre simulation phase of elaboration.

Given the time constraints, the primary focus falls on the first two objectives.
The automation and generality the tools could deliver will be emulated by manual and ad-hoc solutions.

_COMMENT:_ Your thesis' value (to external parties) depends highly on delivering point 4.

\clearpage

* Out of Order PDES with MPI
The goal of this chapter is to present the process synchronization algorithm that will be applied and give their implementation using the MPI API.

In units [[The Chandy/Misra/Bryant synchronization algorithm]] and [[On Demand Synchronization]] we present the conservative synchronization algorithms that will be evaluated.
In unit [[Semantics of point-to-point Communication in MPI]] and [[MPI Communication Modes]] we present the semantics of the Message Passing Interface (MPI) communication primitives.
In unit [[MPI Realization of CMB]] we provide pseudo code for the realization of the CMB using the MPI communication primitives.
In unit [[Existing PDES]] we give an overview of prior art in the field of PDES in ESLD.


** The Chandy/Misra/Bryant synchronization algorithm
The synchronization algorithm at the heart of the proposed OoO PDES is known as the *Chandy/Misra/Bryant (CMB)* \cite{Bryant} \cite{Chandy1979}.
Historically, it has been the first of the family of conservative synchronization algorithms \cite{Fujimoto1990}.

According to the algorithm, the physical system to be simulated must be modeled as a number of communicating sequential *processes*.
The system's state, a set of variables, is distributed in a disjoint way, across the processes.
Computation is reactive; it is sparked by an event and produces further events and *side-effects* (changes in a subset of the system's variables).
Each process keeps its own perspective of simulated time through a *clock* variable.
The value of the clock is equal to the timestamp of the last event selected for computation.

Based on the system's state segregation, a static determination of which processes are interdependent can be established.
This is indicated by placing a *link* for each pair of dependent processes.
From a process' perspective a link can be either *outgoing*, meaning that events are sent via the link, or *incoming* meaning that events are received through it.
An incoming link must encapsulate a First-In-First-Out (FIFO) data structure for storing incoming events, in the order they are received.

The order by which events are received is *chronological*; non decreasing timestamp order.
This system-wide property is maintained by making each process select for computation the event that has the smallest timestamp.
A formal proof of how this local property *induces* a system-wide property can be found in \cite{Bryant} \cite{Chandy1979}.
Chronological reception of events is a necessary, but not sufficient, condition for ensuring *causality*.
The algorithm deals with the "is an event safe to execute" dilemma by forcing a process to *block* until each of its incoming links contains an event.
All the above are demonstrated in Listing \ref{alg:kernel}. 
The synchronization algorithm is realized as a process' main event loop.

#+BEGIN_LATEX
\begin{algorithm}
\caption{Process event loop, without deadlock avoidance}
\label{alg:initial_CMB}
\begin{algorithmic}[1]

   \While{process clock < some T}  
      \State \textbf{Block} until each incoming link FIFO contains at least one event
      \State Pop event M, with the \textbf{smallest} timestamp across all incoming links.
      \State Set process' \textbf{clock} = timestamp(M)
      \State \textbf{React} to event M
      \State \textbf{Communicate} resulting events over the appropriate links
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX

** Deadlock Avoidance
The naive realization of the process' event loop presented in Listing \ref{alg:kernel} leads to deadlock situations like the one depicted in Figure [[fig:deadlock]].
The queues placed along the outer loop are empty, thus simulation has halted, even though there are pending events (across the queues of the inner loop).
A global simulation moderator could easily detect deadlocks and allow the process, that has access to the event with the global minimum timestamp, to resume execution.
The presence of a moderator, however, would violate the distributed nature of the simulation, thus increasing the implementation complexity of the simulation environment.
Furthermore, 

For the context of this thesis, a distributed mechanism is more favorable.
What follows is a presentation of a distributed mechanism for overcoming these situations, referred to as the *null-event deadlock avoidance* \cite{Fujimoto1999}.

#+BEGIN_SRC ditaa  :file Figures/deadlock.png :cmdline -S --font "Times New Roman"
+--------------+
|ARL           |
|@2            |----------------------+
|              |                      |
|             8|<------------------+  |
|   ?          |                   |  |
+--------------+                   |  |
    ^  |                           |  |
    |  |                           |  |
    |  |                           |  |
    |  |                           |  |
    |  |                           |  |
    |  |                           |  |
    |  |                           |  :
    :  v                           |  v
+--------------+              +--------------+
|      5       |              |       ?      |
|              |------------->|6             |
|              |              |              |
|             ?|<=------------|              |
|              |              |              |
|CDG           |              |SKG           |
|@3            |              |@5            |
+--------------+              +--------------+
#+END_SRC
#+CAPTION: Deadlock scenario justifying the use of Null messages in the CMB
#+NAME: fig:deadlock
#+RESULTS:
[[file:Figures/deadlock.png]]

Figure demonstrates an air traffic simulation, where the airports (ARL, CDG and SKG) constitute the simulation processes.
The events exchanged between the airports model flights, the time unit being arbitrary.
At deadlock, every airport is at time 5.

Furthermore, it is assumed that there is an *a priori* knowledge conserning the flight time between airports.
This knowledge is referred to as the *lookahead* and takes the form of a function $lookahead:(PxP) \rightarrow time$
For example, by selecting the distance between every airport to be 3 time units, one can deduce the following:
since SKG is at 5 then ARL or CDG should not expect any event from SKG before 8.

To communicate this fact, SKG could create a special kind of event, a *null event*, with no data value, but with a timestamp 8 (clock+lookahead) and place it on its outgoing links.
A null event is still an event, so CDG would acknowledge it during the selection phase, thus being able to receive the flight from ARL.
CDG now sits at 5 and in the same fashion it could broadcast a null event with timestamp 8.
It is evident that the deadlock has been solved, at the expense of flooding the communication links with null events.


#+BEGIN_LATEX
\begin{algorithm}
\caption{Process event loop, with deadlock avoidance}
\label{alg:null-event}
\begin{algorithmic}[1]

   \While{process clock < some T}  
      \State \textbf{Block} until each incoming link FIFO contains at least one event
      \State Remove event M with the smallest timestamp from its FIFO.
      \State Set process' clock = timestamp(M)
      \State \textbf{React} to event M
      \State \textbf{Communicate} either a null or meaningful event to each outgoing link with timestamp = clock + lookahead
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX
** Criticism
The modified, for deadlock avoidance, algorithm is described in listing \ref{alg:null-event}.
The important points one must notice with this deadlock avoidance mechanism are that:
- Null events are created when a process updates its clock, that is upon processing an event.
- Each process propagates null events on all of its outgoing links.
- The efficiency of this mechanism is highly dependent on the designer's ability to determine sufficiently large lookaheads. 
- The lookahead must be a function 

** Semantics of point-to-point Communication in MPI
_There is a problem here: There are two sections. Semantics of Nonblocking and Blocking communications in the MPI manual_

The framework chosen for implementing the PDES is the *Message Passing Interface* 3.0 (MPI).
Events are modeled as structured messages, while event diffusion/communication as message passing.
MPI is a message passing library interface specification, standardized and maintained by the Message Passing Interface Forum \cite{citation}.
It is currently available for C/C++, FORTRAN and Java from multiple vendors (Intel, IBM, OpenMPI) \cite{citation}.
MPI addresses primarily the message passing parallel programming model, 
in which data is moved from the address space of one process to that of another process through cooperative operations on each process \cite{MessagePassingInterfaceForum2012}.

The basic communication primitives are the functions \texttt{MPI\_Send(...)} and \texttt{MPI\_Recv(...)}.
Their arguments specify, among others things, a data buffer and the peer process' or processes' unique id assigned by the MPI runtime.
By default, message reception is blocking, while message transmission may or may not block.
One can think of message transfer as consisting of the following three phases
1. Data is pulled out of the send buffer and a message is assembled
2. A message is transferred from sender to receiver
3. Data is pulled from the incoming message and disassembled into the receive buffer

*Order:*
Messages are non-overtaking.
If a sender sends two messages in succession to the same destination, 
and both match the same receive (a call to \texttt{MPI\_Recv}), 
then this operation cannot receive the second message if the first one is still pending. 
If a receiver posts two receives in succession,
and both match the same message, 
then the second receive operation cannot be satisfied by this message, if the first one is still pending. 
This requirement facilitates matching of sends to receives and also guarantees that message passing code is deterministic.

*Fairness:*
MPI makes no guarantee of fairness in the handling of communication. 
Suppose that a send is posted. 
Then it is possible that the destination process repeatedly posts a receive that matches this send, 
yet the message is never received, 
because it is each time overtaken by another message, 
sent from another source. 
It is the programmer’s responsibility to prevent starvation in such situations.

_COMMENT:_ Why did you choose MPI?

** MPI Communication Modes
The MPI API contains a number of variants, or *modes*, for the basic communication primitives.
They are distinguished by a single letter prefix (e.g. \texttt{MPI\_Isend(...)}, \texttt{MPI\_Irecv(...)}).
As dictated by the MPI version 3.0, the following communication modes are supported \cite{MessagePassingInterfaceForum2012}:

*No-prefix for standard mode: \texttt{MPI\_Send(...)}*
In this mode, it is up to MPI to decide whether outgoing messages will be buffered. 
MPI may buffer outgoing messages. 
In such a case, the send call may complete before a matching receive is invoked. 
On the other hand, buffer space may be unavailable, or MPI may choose not to buffer outgoing messages, for performance reasons. 
In this case, the send call will not complete, blocking the transmitting process, until a matching receive has been posted, and the data has been moved to the receiver.

*B for buffered mode: \texttt{MPI\_Bsend(...)}* 
A buffered mode send operation can be started whether or not a matching receive has been posted. 
It may complete before a matching receive is posted. 
However, unlike the standard send, this operation is local, and its completion does not depend on the occurrence of a matching receive. 
Thus, if a send is executed and no matching receive is posted, then MPI must buffer the outgoing message, so as to allow the send call to complete. 
A buffered send operation that cannot complete because of a lack of buffer space is erroneous. 
When such a situation is detected, an error is signaled that may cause the program to terminate abnormally. 
On the other hand, a standard send operation that cannot complete because of lack of buffer space will merely block, 
waiting for buffer space to become available or for a matching receive to be posted. 
This behavior is preferable in many situations. 
Consider a situation where a producer repeatedly produces new values and sends them to a consumer. 
Assume that the producer produces new values faster than the consumer can consume them. 
If buffered sends are used, then a buffer overflow will result. 
Additional synchronization has to be added to the program so as to prevent this from occurring. 

*S for synchronous mode: \texttt{MPI\_Ssend(...)}*
A send that uses the synchronous mode can be started whether or not a matching receive was posted. 
However, the send will complete successfully only if a matching receive is posted, and the receive operation has started to receive the message sent by the synchronous send.
Thus, the completion of a synchronous send not only indicates that the send buffer can be reused, 
but it also indicates that the receiver has reached a certain point in its execution, 
namely that it has started executing the matching receive. 
If both sends and receives are blocking operations then the use of the synchronous mode provides synchronous communication semantics: 
a communication does not complete at either end before both processes *rendezvous* at the communication point.

*R for ready mode: \texttt{MPI\_Rsend(...)}*
A send that uses the ready communication mode may be started only if the matching receive is already posted. 
Otherwise, the operation is erroneous and its outcome is undefined.
Ready sends are an optimization when it can be guaranteed that a matching receive has already been posted at the destination.
On some systems, this allows the removal of a hand-shake operation that is otherwise required and results in improved performance. 
A send operation that uses the ready mode has the same semantics as a standard send operation, or a synchronous send operation; 
it is merely that the sender provides additional information to the system (namely that a matching receive is already posted), that can save some overhead. 

Maybe you should consider non-blocking communication not as a *mode*.

*I for non-blocking mode: \texttt{MPI\_Isend(...)}, \texttt{MPI\_Ibsend(...)}, \texttt{MPI\_Issend(...)} and \texttt{MPI\_Irecv(...)*
Non-blocking message passing calls return control immediately (hence the prefix I), 
but it is the user's responsibility to ensure that communication is complete, 
before modifying/using the content of the data buffer.
It is a complementary communication mode that works en tandem with all the previous.
The MPI API contains special functions for testing whether a communication is complete, or even explicitly waiting until it is finished.

** MPI Realization of CMB
Listing \ref{alg:CMB_mpi} is a pseudo code, sketching out the CMB process event loop, using MPI's communication primitives.
#+BEGIN_LATEX
\begin{algorithm}
\caption{CMB Process event loop in MPI}
\label{alg:CMB_mpi}
\begin{algorithmic}[2]

   \While{process clock < some T}  
      \State post a MPI\_Irecv on each incoming peer process
      \State post a MPI\_Wait: block until every receive has been completed
      \State save each message received in a separate, per incoming link, FIFO.
      \State identify message M with the smallest time-stamp
      \State set clock = time-stamp(M)
      \State process message M
      \State post a MPI\_Issend to each outgoing link L with time-stamp = clock + Lookahead(clock,L,...)
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX

Applications have specific communication patterns

Also provides information about the application's communication behavior to the MPI implementation.

*Topology mapping*
_One of the major features of MPI's topology interface is that it can easily be used to adapt the MPI proces layout to the underlying network and system topology._

non cartesian topologies

What is the implementation type of the event?
Let us custom pack them in one 64 bit integer.
Extract them by mapping.

Since you always send an event to your neighbors, either a meaningfull one or a null, why not broadcast?

** Evaluation Metrics
The first evaluation metric of the proposed PDES implementation will be its performance against the reference SystemC kernel.
It will be measured by experimentation on the project's use case.

The simulation's size can be easily related to the duration of the simulation (in simulated time).
Another way of describing the simulation's size is through the conception of a formula involving the number of system processes, the number of links, the system's topology and the amount of events generated.

The accuracy of the simulation can be measured by the aggregate number of causality errors.
The detection of causality errors must be facilitated in a per process level and the aggregation shall be performed at the end of the simulation.
A concrete realization of the accuracy metric comes in the form of a counter each process increments whenever it executes an event with a time-stamp lower than its clock (the time-stamp of the last processed event).
Ideally, if the synchronization algorithms have been realized correctly, no causality errors should be detected.

_COMMENT:_ This section will become more concrete when we start experimentation.

** Existing PDES
The most important:
RISC: Recoding infrastructure for SystemC \cite{Liu2015}.

Miscellaneous:
SystemC-SMP \cite{Mello2010}
SpecC \cite{Domer2011}, although the latter is not meant for SystemC.
sc\_during \cite{Moy}

_COMMENT:_ This section is incomplete that should not be incomplete in an Intermediate report. 
Are you reinventing the wheel? 
Did you try at least one of these tools?
\clearpage

* SystemC TLM 2.0
At the time of writing and to the best of our knowledge, we can not verify the existence of a comprehensive guide about system-level modeling with SystemC TLM 2.0.
Common practice among engineers that want to learn system-level modeling with SystemC TLM 2.0 is to attend courses offered by training companies.
Hence, we fill obliged to provide a quick introduction into the SystemC TLM 2.0 Loosely-Timed (LT) coding style, by means of a simple example.
The chapter assumes familiarity with C++ and SystemC.

In unit [[The Role of SystemC TLM 2.0]] we enumerate the features of the SystemC TLM 2.0 API.
In unit nomenclature
In units [[Transactions, Sockets, Initiators and Targets]] and [[Generic Payload]] we have a look at the fundamental notions of transaction, initiator and target components, socket and generic payload.
In unit [[Coding Styles and Transport Interfaces]] we present the two coding styles (Loosely Timed and Approximately Timed) and give their typical use cases.
In unit [[An Example]] we provide the implementation of a simple initiator, interconnect and target model.
In unit [[Criticism]] we present the dominant source of criticism for TLM 2.0.
Finally, in unit [[Simics and TLM 2.0]] we provide a comparison between the dominant industry frameworks for ESLD, Simics and SystemC TLM.

** The Role of SystemC TLM 2.0
As stated in unit, a Transaction Level Model is considered a virtual platform where a software application can be mapped.
The TLM 2.0 API enhances SystemC's expressiveness in order to facilitate the *modular description* and *fast simulation* of virtual platforms.
As a language, unlike VHDL or SystemC, it is not meant for describing individual functional/architectural/system blocks/modules/components (henceforth *Intellectual Properperty (IP) blocks/modules/components*).
Its role is to make these individual blocks communicate with each other, as demonstrated in figure [[fig:tlm_as_wrapper]].

#+BEGIN_SRC ditaa :file Figures/tlm_as_wrapper.png :cmdline -S --font "Times New Roman"
                                                                           +-------------------------------+
                                                                           |                               |
 ------------------------------------------------------------------------> | Native SystemC module for bus |
         |                         |                          |            |                               |
         v                         v                          v            +-------------------------------+
+--------+----------+     +--------+----------+      +--------+----------+
|    TLM Wrapper    |     |    TLM Wrapper    |      |    TLM Wrapper    |
|                   |     |                   |      |                   |
|  +-------------+  |     |  +-------------+  |      |  +-------------+  |
|  |    ISS      |  |     |  |             |  |      |  |             |  |
|  |             |  |     |  |  Algorithm  |  |      |  |             |  |
|  |+-----------+|  |     |  |             |  |      |  |    VHDL     |  |
|  ||Object Code||  |     |  |    in C     |  |      |  |             |  |
|  |+-----------+|  |     |  |             |  |      |  |             |  |
|  +-------------+  |     |  +-------------+  |      |  +-------------+  |
+-------------------+     +-------------------+      +-------------------+
#+END_SRC

#+CAPTION: TLM 2.0 as a mixed language simulation technology
#+NAME: fig:tlm_as_wrapper
[[file:Figures/tlm_as_wrapper.png]]

System modularity is equivalent to individual IP block *interoperability*, enabling the reuse of IP components in a "plug and play" fashion.
TLM is relevant at every interface where an IP block needs to be plugged into a bus.
Having a library of verified IP blocks at his disposal, the engineer is able to create new virtual platforms fast and with a minimal effort. 

To be suitable for productive software development, a virtual platform needs to be fast, booting operating systems in seconds.
It also needs to be accurate enough such that code developed using standard tools on the virtual platform will run unmodified on real hardware. \cite{Leupers2010}
Compared to a standard RTL simulation, a TLM achieves a significant speed up by replacing communication through pin-level events with a single function call. 
TLM uses the simulation engine available with SystemC.


** TLM 2.0 terminology

TLM 2.0 classifies IP blocks as initiators, targets and interconnect components.
The terms initiator and target come forth as a replacement for the anachronistic terms master and slave.


An *initiator* is a component that initiates new transactions.
It is the initiator's duty to allocate memory for the transaction object or *payload*.
Payloads are always passed by reference.


A *target* component acts as the end point of a transaction. 
As such, it is responsible for providing a response to the initiator.
Request and response are combined into a single transaction object.
Thus, the target responds by modifying certain fields in the payload.


An *interconnect* component is responsible for routing a transaction on its way from initiator to target.
The route of a transaction is not pre-defined.
Routing is dynamic; it depends on the attributes of the payload, mainly its address field.
There is no limitiation on the number of interconnect components participating in a transaction. 
An initiator can also be directly connected to a target.
Since an interconnect can be connected to multiple initiator and target components, it must be able to perform *arbitration* in case transactions "collide".


The role of a component is not statically defined and it is not limited to one.
It is determined on a transactions basis. 
For example, it may function as an interconnect component for some transactions, and as a target for other transactions.


Transactions are sent through initiator *sockets*, and received through target sockets.
It goes without saying that an initiator component must have at least one initiator socket, a target component at least on target socket and a interconnet must possess both.
_Each initiator-to-target socket connection supports both a forward and a backward path by which interface methods can be called in either direction._

All the above terms are illustrated in figure [[fig:tlm_terminology]]

#+BEGIN_SRC ditaa :file Figures/tlm_terminology.png :cmdline -S -E --font "Times New Roman"
+-----------+ Initiator        +--------------+           Target +-----------+
|           | socket           |              |           socket |           |
|           +---+          +---+              +---+          +---+           |
| Initiator | > |--------->| > | Interconnect | > |--------->| > |  Target   |
|           +---+          +---+              +---+          +---+           |
|           |                  |              |                  |           |
+-----------+                  +--------------+                  +-----------+
      :                                ^                               ^
      |                                |                               |
      |                                |                               |
      v                                |                               |
+------------+                         |                               |
|            |                         :                               :
|  Payload   |-------------------------+-------------------------------/
|            |
+------------+
#+END_SRC

#+CAPTION: A basic TLM system
#+NAME: fig:tlm_terminology
#+RESULTS:
[[file:Figures/tlm_terminology.png]]

TLM 2.0 API \cite{OpenSystemCInitiative2009} consists of the following features ([[fig:TLM_features]]):
- A set of core interfaces
  - A Blocking interface which is coupled with the *Loosely-Timed (LT)* coding style.
  - A non-blocking interface, which is coupled with the *Approximately-Timed (AT)* coding style.
  - The *Direct Memory Interface (DMI)* to enable an initiator to have direct access to a target's memory, bypassing the usual path through the interconnect components used by the transport interfaces. 
  - The *Debug transport interface* to allow an non-intrusive inspection of the system's state.
- The *global quantum* used by the *temporal decoupling* mechanism of the LT coding style, which facilitates faster simulations by reducing the number of context switches performed by the kernel. 
- Initiator and target *sockets* to denote the links (causal dependencies) between processes.
- The *generic payload* which supports the abstract modeling of memory-mapped buses.
- A set of *utilities*, in the form of pre configured sockets and interconnect components, to facilitate the rapid development of models.

#+CAPTION: TLM 2.0 use cases (adopted from \cite{OpenSystemCInitiative2009}).
#+NAME: fig:TLM_features
[[file:Figures/TLM_features.png]]


** Transactions, Sockets, Initiators and Targets
*Transactions* are non-atomic communications, normally with bidirectional data transfer, and consist of a set of messages that are usually modeled as atomic communications.
In a transaction one can distinguish two roles;
the *initiator*, the component which initiated the communication, and the *target*, the component which is supposed to service the initiator's request.
A component is not limited to either of these two roles; it can assume both.
For example, *interconnect* components encapsulate the behavior of memory-mapped buses, being responsible for routing transactions to the correct target.
From the initiator's perspective, they act as targets and from the target's perspective they act as initiators.

Implementation-wise, communication in TLM 2.0 is reduced to method calls, 
from the initiator to the target through an arbitrary number of interconnect component, without involving any context switches from the simulation kernel.

A component's role is signified by the type of *sockets* it contains.
Initiator sockets are used to forward method calls "up and out of" a component, while target sockets are used to allow method calls "down and into" a component \cite{doulos}.
Socket binding is the act of connecting components together, thus defining the component whose method call will be eventually executed to service the transaction.
From SystemC's viewpoint, a socket is basically a convenience class, wrapping a sc\_port and an sc\_export.





** Coding Styles and Transport Interfaces
LT is suited for describing virtual platforms intended for software development.
However, where additional timing accuracy is required, typically for software performance estimation and architectural analysis use cases, the AT style is employed.
Virtual platforms typically do not contain many cycle-accurate models of complex components because of the performance impact. 

_COMMENT:_ This is a quite problematic section. You need to elaborate more, do not forget LT is on your thesis title. 



** Generic Payload
The basic argument that is passed, by reference, in communicative method calls is called the *generic payload*.
It is a *structure* that encapsulates generic attributes relevant to a generic memory-mapped bus communication.
The structure possesses an extensions mechanism the designer can use to define more specific.

An *interoperable* TLM 2.0 component must depend only on the generic attributes of the generic payload.
The presence of attributes through the extension mechansim can be ignored without breaking the functionality of the model.
In such a case, the extensions mechanism carries simulation meta-data like pointers to module internal data structures or timestamps.


| Attribute           | Type                                | Modifiable        | Description                                                                                                                                                                                                                                                                  |
|---------------------+-------------------------------------+-------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Command             | \texttt{tlm\_command} (enum)         | No                | Set by the initiator to either \texttt{TLM\_READ} for read, \texttt{TLM\_WRITE} for write or TLM\_IGNORE to indicate that the command is set in the extensions mechanism.                                                                                                       |
| Address             | \texttt{uint64}                     | Interconnect only | Can be modified by interconnects since by definition an interconnect must bridge different address spaces.                                                                                                                                                                   |
| Data pointer        | \texttt{unsigned char*}             | No                | A pointer to the actual data being transfered.                                                                                                                                                                                                                               |
| Data length         | \texttt{unsigned int}               | No                | Related to the data pointer, indicates the number of bytes that are being transfered                                                                                                                                                                                         |
| Byte enable pointer | \texttt{unsigned char*}             | No                | A pointer to a byte enable mask that can be applied on the data (0xFF for data byte enabled, 0X00 for disabled)                                                                                                                                                              |
| Byte enable length  | \texttt{unsigned int}               | No                | Only relevant when the byte enable pointer is not null. If this number is less than the data length, the byte enable mask is applied repeatedly.                                                                                                                             |
| Streaming width     | \texttt{unsigned int}               | No                | Must be greated than 0. Largest address implied by the transaction is (address + streaming width - 1). Refer to *figure* for an example                                                                                                                                      |
| DMI hint            | \texttt{bool}                       | Yes               | A hint given to the initiator of whether he can bypass the transport interface and access a target's memory directly through a pointer.                                                                                                                                      |
| Response status     | \texttt{tlm\_response\_status} (enum) | Target only       | The initiator must set it to \texttt{TLM\_INCOMPLETE\_RESPONSE} prior to initiating the transaction. The target will set it to an appropriate value indicating the outcome of the transaction. For example for a successfull transaction the value is \texttt{TLM\_OK\_RESPONSE} |
| Extensions          | \texttt{tlm\_extension\_base*)[]      | Yes               | The mechanism for allowing the generic payload to carry protocol specific attributes                                                                                                                                                                                         |
  





** An Example
This unit will provide a literate code listing for the model in figure [[fig:TLM_tutorial]]
#+CAPTION: A simple system-level model. The initiator, for example, could model a processor, the interconnect component a memory bus and the target a memory.
#+NAME: fig:TLM_tutorial
[[file:Figures/TLM_tutorial.png]]


** Criticism
The root problem with TLM 2.0 lies in the elimination of explicit channels, which were a key contribution in the early days of research on system-level design.
As most researchers agreed, the concept of separation of concerns was of highest importance, 
and for system-level design in particular, this meant the clear separation of computation (in behaviors or modules) and communication (in channels).
Regrettably, SystemC TLM 2.0 chose to implement communication interfaces directly as sockets in modules and this indifference between channels and modules thus breaks the assumption of communication being safely encapsulated in channels.
Without such channels, there is very little opportunity for safe parallel execution \cite{Liu2015}.

For the above reason some designers consider TLM 2.0 a step towards the wrong direction and revert back to TLM 1.0.
Do you agree with this trend? 
Maybe tell us the major difference with TLM 1.0?

This is why SystemC TLM 2.0 model needs to be *recoded* to allow parallel execution.
The recoding must reconstitute the separation of concerns between computation and communication.
A modification of just the kernel will not suffice.



** Simics and TLM 2.0
Everything you do with SystemC TLM 2.0 you can do with Simics.
Simics is the main alternative to SystemC TLM 2.0 for system-level design.
Can you briefly outline the differences between the two tools/frameworks?
Is Simics capable of PDES?
\clearpage

* Use Case
In this chapter we describe the transaction level model we are going to use for conducting our experimentation.
The purpose of the experimentation is twofold;
verify whether we achieve better faster simulation compared to the reference SystemC kernel and evaluate the proposed process synchronization algorithms.

** Cache Hierarchy Design
Caching shared data introduces a new problem because 
the view of memory held by two different processors is through their individual caches
which without any additional precautions could end up seeing two different values.

This difficulty is generally referred to as the cache coherence problem.

Notice that the coherence problem exists because we habe both a global state, defined primarily by
the main memory, and a local state, defined by the individual caches, which are private to each processor
core.

Thus in a multicore where some level of caching may be shared, while some levels are private
the coherence problem still exists and must be solved.

Informall we could say that a memory system is coherent if any read of a data item returns
the most recently written value of that data item

A program running on multiple processors will normally have copies of the same data in several caches.

The protocols to maintain coherence for multiple processors are called cache coherence protocols
Key to implementing a cache coherence protocol is tracking the state of any sharing of a data block.
There are two classes of protocols in use, each of which uses different techniques to track the sharing status.

Directory based: The sharing status of a particular block of physical memory is kept in one location, called the directory.
There are two very different types of directory-based cache coherence.
In an SMP, we can use one centralized directory, associated with the memory or some other single serialization point,
such as the outermost cache in a multicore.

A directory keeps the state of every block that may be cached.
Information in the directory includes which caches have copies of the block,
whether it is dirty and so on.

The simplest directory implementations associate an entry in the directory with each memory block.
In such implementations, the amount of information is proportional to the product of the number of memory blocks
times the number of nodes.
This overhead is not a problem for multiprocessors with less than a few hundred processors
because the directory overhead with a reasonable block size will be tolerable.

For efficiency reasons, we also track the state of each cache block at the individual caches.

The states and transitions for the state machine at each cache are identical to what we used for the snooping cache,
although the actions on a a transition are slightly different.
The processes of invalidating and locating an exclusive copy of a data item are different,
since they both involve communication between the requesting node and the directory and between
the directory and one or more remote nodes.
In a snooping protocol, these two steps are combined through the use of a broadcast to all the nodes.

We can start with simple state diagrams that show the state transitions for an individual cache block
and the examine the state diagram for the directory entry corresponding to each block in memory.

Presented as UML state machine diagram





#+BEGIN_LATEX
\begin{tikzpicture}

  \umlbasicstate[name=invalid, fill=white, anchor=north]{invalid}
  \umlbasicstate[name=shared, right=12cm of invalid-body.north, anchor=north, fill=white]{shared}
  \umlbasicstate[name=modified, below left=4cm and 4.5cm of shared-body.south, fill=white]{modified}
  \umlstateinitial[above=1cm of invalid, name=initial]

  \umltrans{initial}{invalid}

  % Invalid transition  
  \umltrans[arg={CPU\_read/}, pos=0.7, anchor1=30, anchor2=150]{invalid}{shared}
  \umlVHtrans[anchor2=150, arg={CPU\_write/}, pos=1.6]{invalid}{modified}

  % Shared transitions
  \umltrans[anchor1=170, anchor2=10, arg={invalidate/}, pos=0.7]{shared}{invalid}
  \umlVHtrans[anchor1=245, anchor2=30, arg={CPU\_write\_hit/}, pos=1.5]{shared}{modified}
  \umlVHtrans[anchor1=280, anchor2=5, arg={CPU\_write\_miss/}, pos=1.5]{shared}{modified}
  \umltrans[pos=1.2, arg={CPU\_read\_miss || CPU\_read\_hit}, recursive=90|10|3cm, recursive direction=top to right]{shared}{shared}

  % Modified transitions
  \umlVHtrans[pos=0.5, arg={CPU\_read\_miss/}, anchor1=90, anchor2=190]{modified}{shared}
  %\umlVHtrans[arg={fetch/}, anchor1=70, anchor2=210]{modified}{shared}
  \umlHVtrans[pos=0.75, anchor1=175, anchor2=245, arg={fetch\_invalidate/}]{modified}{invalid}
  \umltrans[pos=2.2, arg={CPU\_write\_miss}, recursive=-20|280|2.3cm, recursive direction=right to bottom]{modified}{modified}
  \umltrans[pos=1.5, arg={CPU\_read\_hit || CPU\_write\_hit}, recursive=260|200|4cm, recursive direction=bottom to left]{modified}{modified}

\end{tikzpicture}
#+END_LATEX







** Platform modeling
A block diagram of the platform that will be modeled is seen in figure [[fig:Platform]].
The platform is a shared fmemory, cache-coherent, symmetric multiprocessor system based on the [[http://opencores.org/or1k/Or1ksim][OpenRisc 1000 Instruction Set Simulator]].
Cache coherence is enforced by a directory residing in the inclusive L2 cache.
Every component is/will be implemented in C/C++ and wrapped in SystemC modules using the TLM 2.0 API for communication. 
The exact number of processors is yet to be determined.

#+CAPTION: A model of a shared memory, cache-coherent, symmetric multiprocessor system
#+NAME: fig:Platform
[[file:Figures/platform.png]]


_COMMENT:_ Can you be more specific about the cache coherence protocol? Maybe provide a state diagram?



** Application modeling
We have the bare metal (newlib based) toolchain for compiling applications for the OpenRisc ISS.

_COMMENT;_ What kind of application am I going to run on this platform?
I see that most of the papers out there do some kind of mpeg2 decoding. That seems complex.


* References
\renewcommand\refname{}
\bibliography{References}
\bibliographystyle{myIEEEtran}








* Caches

** Directory Based Cache Coherence
Avoid broadcast.

The absence of any centralized data structure that tracks the state of the caches is both the fundamental
advantage of a snooping-based scheme, since it allows it to be inexpensive, as well as its Achille's heel
when it comes to scalability.

The sharing status of a particular block of physical memory is kept in one location,
called the *directory*.
There are two very different types of directory-based cache coherence.
In an *SMP*, we can use one centralized directory, 
associated with the memory or some other _single serialization point_, such as the outermost cache in a multicore.

In a *DSM*, it makes no sense to have a single directory, since that would create a single point of contention
and make it difficult to scale to many multicore chips given the memory demands of multicores with eight or more cores.

A directory keeps the state of every block that may be cached.
Information in the directory includes:
    1. which caches (or collections of caches) have copies of the block
    2. whether it is dirty, and so on.

Within a multicore with a shared outermost cache (say, L3), it is *easy* to implement a directory scheme.
_Simply keep a bit vector of the size equal to the number of cores for each L3 block._
The bit vector indicates which private caches may have copies of a blockin L3, and invalidations are only sent to those caches.

This works perfectly for a single multicore if *L3 is inclusive*, 
and _this scheme is the one used in the Intel i7_.


*** Basics

Just as with snooping protocol, there are two primary operations that a directory protocol must implement:
    1. handling a read miss
    2. handling a write to a shared ( thus clean) cache block.

To implement these operatations, a directory must track the state of each cache block.
In a simple protocol, these states could be the following:
   1. *Shared:* One of more nodes have the block cached, and the value in memory is up to date (as well as in all the caches)
   2. *Uncached:* No node has a copy of the cache block.
   3. *Modified:* Exactly one node has a copy of the cache block, 
       and it has written the block, so the memory copy is out of date. 

In addition to tracking the state of each potentially shared memory block, 
we must track which nodes have copies of that block, 
since those copies will need to be invalidated on a write.

_The simplest way to do this is to keep a bit vector for each memory block._

We can also use the bit vector to keep track of the owner of the block when the block is in the exclusive state.
_For efficiency reasons, we also track the state of each cache block at the individual caches._

The states and transitions for the state machine at each cache are identical to what we used for the snooping cache,
although the actions on a transition are slightly different.
The processes of invalidating and locating an exclusive copy of a data item are different,
since the both involve communication between the requesting node and the directory 
and between the directory an one or more remote nodes.

In a snooping protocol, these two steps are combined through the use of a broadcast to all the nodes.



*** Coherence Messages

A catalog of the message types that may be sent between the processors and the directories
for the purpose of handling misses and maintaining coherence.

| Message Type        | Source         | Destination    | Message contents | Function of this message                                                                                            |
|---------------------+----------------+----------------+------------------+---------------------------------------------------------------------------------------------------------------------|
| 1. Read Miss        | Local cache    | Home directory | P, A             | Node P has a read miss at address A; request data and make P a read sharer                                          |
| 2. Write Miss       | Local cache    | Home directory | P, A             | Node P has a write miss at address A; request data and make P the exclusive owner                                   |
| 3. Invalidate       | Local cache    | Home directory | A                | Request to send invalidates to all remote caches that are caching the block at address A                            |
| 4. Invalidate       | Home directory | Remote cache   | A                | Invalidate a shared copy of data at address A                                                                       |
| 5. Fetch            | Home directory | Remote cache   | A                | Fetch the block at address A and sent it to its home directory; change the state of A in the remote cache to shared |
| 6. Fetch/invalidate | Home directory | Remote cache   | A                | Fetch the block at address A and send it to its home directory; invalidate the block in the cache                   |
| 7. Data value reply | Home directory | Local cache    | D                | Return a data value from the home memory                                                                            |
| 8. Data write-back  | Remote cache   | Home directory | A, D             | Write-back a data value for address A                                                                               |

- The first 3 messages are requests sent by the local node to the home.
- The 4 through 6 messages are messages sent to a remote node by the home 
  when the home needs the data to satisfy a read or write miss request.
- Data value replies are used to send a value from the home node back to the requesting node.
- Data value write-backs occur for two reasons: 
     a. When a block is replaced in a cache and must be written back to its home memory
     b. In reply to fetch or fetch/invalidate messages from the home.

_Writing back the data value whenever the block becomes shared_
simplifies the number of states in the protocol since 
     a. Any dirty block must be exclusive 
     b. Any shared block is always available in the home memory.
  

*** Protocol from the Cache's Side

The basic states of a cache block in a directory-based protocol are exactly like those in a snooping protocol.
Thus, we can start with simple state diagrams that show 
    1. The state transitions for *an individual cache block*
    2. The state for the *directory entry* corresponding to each block in memory.



*** Protocol from the Diretory's Side

A message sent to a directory  causes two different types of actions:
  1. Updating the directory state.
  2. Send additional messages to satisfy the request.  

The memory block may be 
  1. Uncached by any node, 
  2. Cached in multiple nodes and readable (shared).
  3. Cached exclusively and writable in exactly one node.

In addition to the state of each block, the directory must track the set of nodes that
have a copy of a block; we use a set called Sharers to perform this function.
_Directory requests need to update the set Sharers and also read the set to perform invalidations._

The directory receives three different requests: read miss, write miss, and data write-back.

_Our simplified protocol assumes that some actions are atomic, such as requesting a value and sending it to another node;
a realistic implementation cannot use this assumption_


** MESI
Adds the state *Exclusive* to the basic MSI protocol
to indicate when a cache block is resident only in a single cache but is clean.

If a block is in the *E* state, it can be written without generating any invalidates,
which optimizes the case where a block is read by a single cache before being written by that same cache.

Of course, when a *read miss* to a block in the *E* state occurs, the block must be changed
to the *S* state to maintain coherence.

Because all sugsequent accesses are snooped, it is possible to maintain the accuracy of this state.
In particular, if another processor issues a read miss, the state is changed from exclusive to shared.
The advantage of adding this state is that a subsequent write to a block in the exclusive state
by the same core need not acquire bus access or generate any invalidate, since the block is known to be
exclusively in this local cache; the processor merely changes the state to modified.

This state is easily added by using the bit that encodes the coherent state as an exclusive state
and using the dirty bit to indicate that a block is modified.

The Intel i7 uses a variant of a MESI protocol, called MESIF, which adds a state (Forward) to designate
which sharing processor should respond to a request. It is designed to enhance performance in distributed
memory organizations.

* Or1ksim

Unfortunately the library of the or1ksim is not reentrant and thus does not allow multiple instances of the core
simulator to be executed in one address space. Historically all data is stored in global variables.

** OpenRISC GNU tool chain
From http://opencores.org/or1k/OpenRISC_GNU_tool_chain

The toolchain is available in several forms, depending on which C standard library they use:

1. or1k-elf for bare metal use, based on the *newlib* library

2. or1k-linux-uclibc for Linux application use, based on the *uClibc* library.

3. or1k-linux-musl for Linux application use, based on the *musl* library.
      















* Latex Headers 						   :noexport:
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt,twoside]
#+LATEX_HEADER: \usepackage[paper=a4paper,dvips,top=1.5cm,left=1.5cm,right=1.5cm, foot=1cm,bottom=1.5cm]{geometry}
#+LATEX_HEADER: \renewcommand{\rmdefault}{ptm} 
#+LATEX_HEADER: \usepackage[scaled=.90]{helvet}
#+LATEX_HEADER: \usepackage{courier}
#+LATEX_HEADER: \usepackage{bookmark}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \pagestyle{fancy}
#+LATEX_HEADER: \usepackage[dvipsnames*,svgnames]{xcolor} 
#+LATEX_HEADER: \usepackage{tikz,ifthen,xstring,calc,pgfkeys,pgfopts}
#+LATEX_HEADER: \usepackage{tikz-uml}
#+LATEX_HEADER: \usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,fit,positioning,calc,shapes}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[swedish,english]{babel}
#+LATEX_HEADER: \usepackage{rotating}		
#+LATEX_HEADER: \usepackage{array}		
#+LATEX_HEADER: \usepackage{graphicx}	 
#+LATEX_HEADER: \usepackage{float}	
#+LATEX_HEADER: \usepackage{color}      
#+LATEX_HEADER: \usepackage{mdwlist}
#+LATEX_HEADER: \usepackage{setspace}   
#+LATEX_HEADER: \usepackage{listings}	
#+LATEX_HEADER: \usepackage{bytefield}  
#+LATEX_HEADER: \usepackage{tabularx}	
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{algpseudocode}	
#+LATEX_HEADER: \usepackage{dcolumn}	
#+LATEX_HEADER: \usepackage{url}	
#+LATEX_HEADER: \usepackage[perpage,para,symbol]{footmisc} 
#+LATEX_HEADER: \usepackage[all]{hypcap}
#+LATEX_HEADER: \definecolor{darkblue}{rgb}{0.0,0.0,0.3} %% define a color called darkblue
#+LATEX_HEADER: \definecolor{darkred}{rgb}{0.4,0.0,0.0}
#+LATEX_HEADER: \definecolor{red}{rgb}{0.7,0.0,0.0}
#+LATEX_HEADER: \definecolor{lightgrey}{rgb}{0.8,0.8,0.8} 
#+LATEX_HEADER: \definecolor{grey}{rgb}{0.6,0.6,0.6}
#+LATEX_HEADER: \definecolor{darkgrey}{rgb}{0.4,0.4,0.4}
#+LATEX_HEADER: \hyphenpenalty=15000 
#+LATEX_HEADER: \tolerance=1000
#+LATEX_HEADER: \newcommand{\rr}{\raggedright} 
#+LATEX_HEADER: \newcommand{\rl}{\raggedleft} 
#+LATEX_HEADER: \newcommand{\tn}{\tabularnewline}
#+LATEX_HEADER: \newcommand{\colorbitbox}[3]{%
#+LATEX_HEADER: \rlap{\bitbox{#2}{\color{#1}\rule{\width}{\height}}}\bitbox{#2}{#3}}
#+LATEX_HEADER: \newcommand{\red}{\color{red}}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}{-3.25ex\@plus -1ex \@minus -.2ex}{1.5ex \@plus .2ex} {\normalfont\normalsize\bfseries}}
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \renewcommand\subparagraph{\@startsection{subparagraph}{5}{\z@}{-3.25ex\@plus -1ex \@minus -.2ex}{1.5ex \@plus .2ex} {\normalfont\normalsize\bfseries}}
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \setcounter{tocdepth}{3}
#+LATEX_HEADER: \setcounter{secnumdepth}{5}
#+LATEX_HEADER: \renewcommand{\headrulewidth}{0pt}
#+LATEX_HEADER: \lhead{Konstantinos Sotiropoulos}
#+LATEX_HEADER: \chead{Ms Thesis Intermediate Report}
#+LATEX_HEADER: \rhead{\date{\today}}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \let\ps@plain\ps@fancy 
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \setlength{\headheight}{15pt}


