#+TITLE: Project Title: Parallel Simulation of SystemC Loosely-Timed Transaction Level Models
#+OPTIONS: toc:nil email:nil title:nil author:nil date:nil
#+STARTUP: overview

#+BEGIN_LaTex
\author{
        \textsc{Konstantinos Sotiropoulos}
        \mbox{}\\
        \normalsize
            \texttt{konstantinos.sotiropoulos}
        \normalsize
            \texttt{@intel.com}
}

\date{\today}

\maketitle

\tableofcontents

\clearpage
#+END_LaTex

* Acronyms 							   
| *DE*:    | Discrete Event                      |
| *DES*:   | Discrete Event Simulator/Simulation |
| *DMI*:   | Direct Memory Interface             |
| *ESL*:   | Electronic System-Level Design      |
| *HPC*:   | High Performance Computing          |
| *MoC*:   | Model of Computation                |
| *MPSoC*: | Multicore System on Chips           |
| *OoO*:   | Out-of-Order                        |
| *PDES*:  | Parallel Discrete Event Simulation  |
| *SLDL*:  | System-Level Design Language        |
| *SMP*:   | Symmetric Multiprocessing           |
| *SoC*:   | System on Chip                      |
| *SR*:    | Synchronous Reactive                |
| *TLM*:   | Transaction Level Modeling          |
| *CMB*:   | Chandy/Misra/Bryant algorithm       |
\clearpage


* Organization
This is a Master's Thesis project that will be carried out in Intel Sweden AB and is supervised by KTH's ICT department.
Mr. Bjorn Runaker (\texttt{bjorn.runaker@intel.com}) is the project's supervisor from the company's side, 
while professor [[https://people.kth.se/~ingo/][Ingo Sanders]] (\texttt{ingo@kth.se}) and PhD student [[http://people.kth.se/~ugeorge/][George Ungureanu]] (\texttt{ugeorge@kth.se})are the examiner and supervisor from KTH. 
The project begun on 2016-01-16 and will finish on 2016-06-30, as dictated by the contract of employment that I, Konstantinos Sotiropoulos a Master's student at the Embedded Systems program, have signed with the company 
(document title: "Statement of Terms and Conditions of Fixed Term Employment"). 

The scope of this project has been and is being mutually determined by all parties. 
It is dialectically determined between the company's needs and the institute's research agenda.
As Master's Thesis project, it needs to expose a scientific ground on which the engineering effort shall be rooted.
 
All the necessary equipment (software and hardware) has been kindly provided by the company.
The exact legal context that will apply to any software produced as a result of this project is yet to be determined, 
but will conform to the general context dictated by the documents already signed (documents' titles:  "Statement of Terms and Conditions of Fixed Term Employment" and "Employee Agreement") .




* Background	
SystemC is considered to be a System-Level Design Language (SLDL), that is implemented as a C++ class library.
The language is maintained and promoted by Accellera (former Open SystemC Initiative OSCI) and has been standardized with the latest LRB being the IEEE 1666-2011 \cite{OpenSystemCInitiative2012}
The major advantage SystemC provides to the designer is that a component's functionality can be easily expressed in C/C++, the dominating languages for in systems programming (e.g. Operating System and Device Driver development).
As a result SystemC models are executable: the designer can directly simulate the behavior of a system modeled in SystemC.
The language has been demanded and supported by the semiconductor industry, in its collaborative effort to push Electronic Design Automation (EDA) into higher abstraction layers, thus giving birth to a trend
referred to as Electronic System-Level Design (ESLD).

The rest of this section is devoted into presenting the theoretical/scientific ground on which we will cultivate this project.
To avoid confusion by being precise, fundamental terms and concepts of computer science are being defined and redefined, even at the risk of being pedantic.  
Facing a labyrinth of abstraction layers, we will begin unraveling Ariadne's thread at the simple observation that SystemC is a System Level Design *Language*.

** Models of Computation
A *language* is a set of symbols, rules for combining them (its syntax), and rules for interpreting combinations of symbols (its semantics). 
Two approaches to semantics have evolved: denotational and operational.
Operational semantics, which dates back to Turing machines, gives the meaning of a language in terms of actions taken by some abstract machine. 
How the abstract machine in an operational semantics can behave is a feature of what we call the *Model of Computation (MoC)* \cite{Edwards1997}.
This definition implies that languages are not computational models themselves, but have underlying computational models \cite{Jantsch2005}.

How can this definition make sense for SLDLs and what do we expect a SLDL to be able to do? 
A SLDL should be able to describe a system as a hierarchical network of interacting components.
A MoC is therefore a collection of rules to define what constitutes a component and what are the semantics of execution, communication and concurrency of the abstract machine that will execute the model \cite{Jantsch2005} \cite{Editor2014}.
To ensure meaningful simulations the MoC of the abstract machine that simulates a model must be equivalent with that of the abstract machine that will realize the system.

#+CAPTION: Categorization of three of the most explored MoCs: State Machine, Synchronous Dataflow and Discrete Event(adopted from \cite{Editor2014})
#+NAME: fig:MoCs
[[file:Figures/MoCs.pdf]]


** The Discrete Event Model of Computation
The dominant MoC that underlies most industry standard EDA languages (VHDL, Verilog, SystemC) is called *Discrete Event (DE)*.
The components of a DE system are called *processes*.
In this context processes usually model the behavior and functionality of hardware entities.
The execution of processes is concurrent and the communication is achieved through *events*.
An event can be considered as an exchange of a time-stamped value.

Concurrent execution does not imply parallel/simultaneous execution. 
The notion of *concurrency* is more abstract. 
It can be realized as either parallel/simultaneous execution or as sequential interleaved execution, depending on the actual machine's computational resources.

Systems whose semantics are meant to be interpreted by a DE MoC, in order to be realizable, must have a *causal* behavior: they must process events in a chronological order, 
while any output events produced by a process are required to be no earlier in time than the input events that were consumed \cite{Editor2014}.
At any moment in real time, the model's time is determined by the last event processed.

In figure [[fig:MoCs]] one can observe that the DE MoC is also considered to be *Synchronous-Reactive (SR)*. 
This demonstrates the possibility of the MoC to "understand" entities with zero execution time, where output events are produced at the same time input events are consumed.
We can also extend/rephrase the previous definitions and say that Synchronous-Reactive MoCs are able to handle systems where events happen at the same time, instantaneously, in a causal way.
The DE MoC handles the aforementioned situations by extending time-stamps(the notion of model time) with the introduction of delta delays (also referred to as cycles or micro-steps).
A delta delay signifies an infinitesimal unit of time and no amount of delta delays, if summed, can result in time progression.
A time-stamp is therefore represented as a tuple of values, $(t,n)$ where $t$ indicates the model time and $n$ the number of delta delays that have advanced at $t$.




** The Discrete Event Simulation(or)
A realization of the DE abstract machine is called a *Discrete Event Simulator (DES)*.
SystemC's reference implementation of the DES is referred to as the *SystemC kernel* \cite{OpenSystemCInitiative2012}.

Concurrency of the system's processes is achieved through the co-routine mechanism (also known as co-operative multitasking). 
Processes execute without interruption. In a single core machine that means that only a single process can be running at any real time, 
and no other process instance can execute until the currently executing process instance has yielded control to the kernel.
A process shall not preempt or interrupt the execution of another process \cite{OpenSystemCInitiative2012}.

To avoid quantization errors and the non-uniform distribution of floating point values, time is expressed as an integer multiple of a real value referred to as the time resolution. 

The kernel maintains a *centralized event queue* that is sorted by time-stamp and knows which process is *running*, which processes are waiting for events and which are *runnable*.
Runnable processes have had events to which they are sensitive triggered and are waiting for the running process to yield to the kernel so that they can be scheduled.
The kernel controls the execution order by selecting the earliest event in the event queue and making its time-stamp the current simulation time.
It then determines the process the event is destined for, and finds all other events in the event queue with the same time-stamp that are destined for the same process \cite{Black2010}.
The operation of the kernel is exemplified in Alg \ref{alg:kernel}.

#+BEGIN_LATEX
\begin{algorithm}
\caption{SystemC event loop, adopted from \cite{Schumacher2010}}
\label{alg:kernel}
\begin{algorithmic}[1]

   \While{timed events to process exist}  \Comment{Simulation time progression}
      \State trigger events at that time
      \While {runnable processes exist}   \Comment{Delta cycle progression}
         \While {runnable processes exist}
	     \State run all triggered processes
             \State trigger all immediate notifications
         \EndWhile
         \State update values of changed channels
	 \State trigger all delta time events
       \EndWhile
       \State advance time to next event time
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX


** The Parallel Discrete Event Simulation(or)
The previous section has made evident that the reference implementation of the SystemC kernel assumes sequential execution and therefore can not utilize modern massively parallel host platforms. 
The most logical step in achieving faster simulations is to realize concurrency, from interleaving process execution to actual simultaneous/parallel execution.
By assigning each process to a different processing unit of the host platform (core or hardware thread) we enter the domain of *Parallel Discrete Event Simulation (PDES)*.
SystemC as a SLDL remains the same while the implementation of the DES is radically different.

By allowing processes to execute simultaneously one can allow each process to have its own perception of simulation time, determined by the last event it received.
This approach is referred to as *Out-of-Order PDES (OoO PDES)* \cite{Chen2015}.
Examples of OoO PDES simulators are the SystemC-SMP \cite{Mello2010} and SpecC \cite{Domer2011}, although the latter is not meant for SystemC.

For PDES implementations that enforce global simulation time, the term Synchronous PDES has been coined in the parSC simulator\cite{Schumacher2010}.
In Synchronous PDES, parallel execution of processes is performed within a delta cycle. 
With respect to Alg \ref{alg:kernel}, we can say that a Synchronous PDES parallelizes the execution of the innermost loop (line 4).
However, as we will see in the following section, this approach will bare no fruits in the simulation of TLM Loosely Timed simulations, since delta cycles are rarely triggered \cite{Chen2012}.

Finally, before committing into modifying the SystemC DES, we should mention the existence of less intrusive approaches, that instead of redesigning extend the reference kernel.
The example of the sc-during SystemC library \cite{Moy} is characteristic. 
To exploit parallelism, each process must be redefined as a sequence of atomic tasks that have duration (in simulation time).
The term atomic is used to represent the fact that these tasks are insensitive to input/output events for their duration.
Thus, the kernel can safely assign them to a different operating system thread and allow them to execute independently from the rest of the simulation.



** Transaction Level Modeling					  
*Transaction Level Modeling (TLM)* enhances SystemC's expressiveness in order to facilitate the more abstract description (compared to RTL) of systems.  
TLM 2.0 allows model interoperability and the rapid development of fast virtual platforms to be deployed in software development, early on in the system's design procedure.

Transaction-level models represent one specific type of the DE MoC \cite{Grotker2002}.
Transactions are non-atomic communications, normally with bidirectional data transfer, and consist of a set of messages that are usually modeled as atomic communications.
The set of messages exchanged during a transaction are modeled using function calls and represent the different phases of a communication protocol.
In a transaction one can distinguish two actors:
the *initiator*, the process which initiated the communication, and the *target*, the process which is supposed to service the target's request.

TLM 2.0 API \cite{OpenSystemCInitiative2009} consists of the following features:
- A set of core interfaces
  - A Blocking interface which is coupled with the *Loosely-Timed (LT)* coding style.
  - A non-blocking interface, which is coupled with the *Approximately-Timed (AT)* coding style.
  - The *Direct Memory Interface (DMI)* to enable an initiator to have direct access to a target's memory, bypassing the usual path through the interconnect components used by the transport interfaces. 
  - The *Debug transport interface* to allow an non-intrusive inspection of the system's state.
- The *global quantum* used by the *temporal decoupling* mechanism of the LT coding style, which facilitates faster simulations by reducing the number of context switches performed by the kernel. 
- Initiator and target *sockets* to denote the links (causal dependencies) between processes.
- The *generic payload* which supports the abstract modeling of memory-mapped buses.
- A set of *utilities* to facilitate the rapid development of models.

Figure demonstrates the typical use cases for TLM's different features.
#+CAPTION: TLM 2.0 use cases (adopted from \cite{OpenSystemCInitiative2009}).
#+NAME: fig:tlm_use_cases
[[file:Figures/tlm_use_cases.png]]

The root problem with TLM 2.0 lies in the elimination of explicit channels, which were a key contribution in the early days of research on system-level design.
As most researchers agreed, the concept of separation of concerns was of highest importance, and for system-level design in particular, this meant the clear separation of computation (in behaviors or modules) and communication (in channels)
Regrettably, SystemC TLM 2.0 chose to implement communication interfaces directly as sockets in modules and this indifference between channels and modules thus breaks the assumption of communication being safely encapsulated in channels.
Without such channels and modules thus breaks the assumption of communication being safely encapsulated in channels.
Without such channels, there is very little opportunity for safe parallel execution \cite{Liu2015}.




* Problem statement
The distribution of simulation time opens Pandora's box. 
Protecting the OoO PDES from *causality errors* demands certain assumptions and the addition of complex implementation mechanisms.

The first source of causality errors arises when the system's state variables are not distributed, in a disjoint way, among the processes \cite{Fujimoto1990}.
A trivial realization of the above scenario is depicted in figure [[fig:causality_shared_state]]. Processes $P_1$ and $P_2$ are executing simultaneously, while sharing the system's state variable $x$.
Events $E_1$ and $E_2$ are executed by $P_1$ and $P_2$ respectively. If we assume that in real time $E_2$ is executed before $E_1$, then we have implicitly broken causality, since $E_1$ might be influenced
by the value of $x$ that the execution of $E_2$ might have modified. Furthermore, one must observe that this kind of implicit interaction between $P_1$ and $P_2$ can not be expressed in a DE MoC. 
This is a meta-implication of the host platform's shared memory architecture.

#+CAPTION: Causality error caused by the sharing of the system's state variable $x$ by $P_1$ and $P_2$.
#+NAME: fig:causality_shared_state
[[file:Figures/causality_shared_state.png]]

The second and most difficult to deal with source of causality errors is depicted in figure [[fig:causality_safe_events]]. 
Event $E_1$ affects $E_2$ by scheduling a third event $E_3$ which, for the shake of argument, modifies the state of $P_2$. 
This scenario necessitates sequential execution of all three events. 
Thus the fundamental problem in PDES, in the context of this scenario, becomes the question: how can we deduce that it is safe to execute $E_2$ in parallel with $E_1$, without actually executing $E_1$ \cite{Fujimoto1990}?
However, one must notice that the kind of interaction that yields this problematic situation is explicitly stated in the model.

#+CAPTION: Causality error caused by the unsafe execution of event $E_2$ (adopted from \cite{Fujimoto1990}).
#+NAME: fig:causality_safe_events
[[file:Figures/causality_safe_events.png]]

The last example makes evident the fact that the daunting task of preserving causality in the simulation is all about *process synchronization*.
For example, each process must be able to communicate to each of its peers (processes that is linked with) the message: 
"I will not send you any event before $t_1$, so you can proceed with processing any event you have with time-stamp $t_2$ where $t_2 < t_1$".

PDES synchronization algorithms, with respect to how they deal with causality errors, have been classified into two categories: *conservative* and *optimistic* \cite{Fujimoto2015}.
Conservative mechanisms strictly avoid the possibility of any causality error ever occurring by means of model introspection and static analysis.
On the other hand, optimistic/speculative approaches use a detection and recovery approach: when causality errors are detected a rollback mechanism is invoked to restore the system.
An optimistic compared to a conservative approach will theoretically yield better performance in models where communication, thus the probability of causality errors, is below a certain threshold \cite{Fujimoto1990}.

Both groups present severe implementation difficulties.
For conservative algorithms, model introspection and static analysis tools might be very difficult to develop,
while the rollback mechanism of an optimistic algorithm may require complex entities, such as a hardware/software transactional memory \cite{Anane2015} .


* Old Problem 							   :noexport:							   
In this project we investigate the feasibility of implementing a SystemC OoO PDES, 
that can lead to scalable simulations of MPSoC Loosely-Timed Transaction Level Models,
on SMP host platforms.


* Problem
This project evaluates the efficiency of existing process synchronization algorithms when applied to the parallel simulation
of Loosely-Timed Transaction Level Models on Symmetric Multiprocessing platforms.


* Hypothesis
We hypothesize that by following a conservative approach on implementing a SystemC OoO PDES 
we will yield semantically equivalent and scalable simulations with respect to the reference SystemC DES.   


* Purpose
The vision of a fully automated and connected society, the IoT revolution has promised to deliver,
is depending on the industry's ablate to deliver novel, complex and heterogeneous cyber-physical systems with short time-to-market constraints.
To live up to these expectations, the engineering discipline of ESLD must provide an answer to a number of questions:
\newline
*High-Level Synthesis*: How a system described in a SLDL can be realized in a structured and automatic way? 
Which of its components should be mapped in hardware entities like Digital Signal Processors (DSPs), Field Programmable Gate Arrays (FPGAs) or Application Specific Integrated Circuits (ASICs)?
Which of its components could be software for some kind of Central Processing Unit (CPU)?
Which is the optimal mapping that satisfies the system's requirements and yields a minimum power consumption?
\newline
*Correct-by-design*: Can a high-level synthesis design methodology yield correct-by-design implementations?
Can a system be formally verified given its abstract representation, early on in the design procedure?
Can we free the huge amount of resources wasted in mundane testing and debugging procedures that sometimes can not even provide any formal guarantee about the system's behavior?
\newline
*Improving the co-simulation speed for hardware and software*: Can we develop a virtual prototype of the platform early on in the development cycle, 
so that software engineers can begin developing integral applications without having to wait for the silicon to arrive?
Can we make the simulation fast and accurate, utilizing all the latest developments in High Performance Computing (HPC)?
This project hopes to deliver an infinitesimal contribution in solving the latter class of questions.





* Goals and Objectives
If the timing constraints stretched beyond the scope of a Master Thesis, 
the project's self-actualization would require the development/production of the following components (sorted in descending significance order):
1. An OoO PDES implementation of the SystemC kernel.
2. A proof of concept application of the proposed kernel, on a sufficiently parallel system, running a substantially parallel application, on top of a Linux kernel.
3. The Master Thesis report document.
4. A static analysis/introspection tool for parsing the SystemC description of the system and extracting its pure representation, in terms of processes and links.
5. A code generation tool for constructing the communication and synchronization mechanisms.
6. A way of sequencing the application of the previous tools, either in the kernel's elaboration phase, or using a "gluing" script.
7. A TLM 2.0 coding style to minimize the effort and complexity of the analysis and generation tools.
8. A way to ensure the kernel's OSCI compliance.
9. A roadmap for elevating the simulation from SMP parallel to distributed, in a cluster of SMP nodes, parallel.

Given the time constraints, the primary focus falls on the first three objectives.
The automation and generality the tools could deliver will be emulated by manual and ad-hoc solutions.







* Methods
** Assumptions and delimitations
The IEEE Standard for SystemC states the following about non reference implementations of the kernel:
"An implementation running on a machine that provides hardware support for concurrent processes may permit two or more processes to run concurrently
provided that the behavior appears identical to the co-routine semantics defined in this subclause.
In other words, the implementation would be obliged to analyze any dependencies between processes and to constrain their execution to match the co-routine semantics " \cite{OpenSystemCInitiative2012}.
We assume that our implementation, since it is designed to deliver causal simulations, has a strong coverage over this directive (consider the TLM 2.0 temporal decoupling technique which often yields inaccurate simulations)

However, the feasibility of the introspection and code generation procedures, imposes certain limitations on SystemC's expressive capabilities.
This is the main reason our kernel can not be considered to be compliant with the standard.

We also state that by assuming/enforcing the principle of one process per module and not allowing a module to execute another module's functions in its context (TLM 2.0 blocking transport interface), 
we hope to avoid causality errors caused by processes sharing system variables.




** Process synchronization algorithm 
We will begin our experimentations using a class of conservative synchronization algorithms originating from the work of *Chandy/Misra/Bryant (CMB)* \cite{Bryant} \cite{Chandy1979}.
Listing \ref{alg:kernel} demonstrates how these algorithms deal with the fundamental dilemma presented in section [[Problem statement]], figure [[fig:causality_safe_events]].

#+BEGIN_LATEX
\begin{algorithm}
\caption{Process event loop, adopted from \cite{Fujimoto1999}}
\label{alg:initial_CMB}
\begin{algorithmic}[2]

   \While{simulation is not over}  
      \State \textbf{Block} until each incoming link queue contains at least one event
      \State remove event with the smallest time-stamp M from its queue.
      \State set clock = M
      \State process M
   \EndWhile

\end{algorithmic}
\end{algorithm}
#+END_LATEX

However, a naive realization of the algorithm leads to deadlock situations like the one depicted in figure [[fig:deadlock]].
The queues placed along the red loop are empty, thus simulation has halted, even though there are pending events (across the blue loop).

#+CAPTION: adopted from \cite{Fujimoto1999}
#+NAME: fig:deadlock
[[file:Figures/Deadlock.png]]

The deadlock avoidance mechanism that lies in the core of the CMB class of algorithms can be demonstrated with the following example:
Let us assume that $P_3$ is at time 5.
Furthermore, let us assume that we have the *a priori* knowledge that $P_3$ has a minimum event processing time of 3 (simulated).
We will call this knowledge *lookahead*.
$P_3$ could create a *null event*, with no data value, but with a time-stamp $t$(8) = clock(5) + lookahead(3) and place it on its outgoing links.
A null event is still an event, so $P_2$ by processing it would advance its clock to 8.
In the same fashion, let us assume that $P_2$ has a lookahead of 2 and upon processing $P_3$ null event, 
it will generate a null event for $P_1$ with time-stamp 10. 
Eventually $P_1$ can now safely process actual event with time-stamp 9, thus unfreezing the simulation.

The important points one must notice with this deadlock avoidance mechanism are that:
- Null events are created when a process updates its clock.
- Each process propagates null events on all of its outgoing links.
- This mechanism is mostly dependent to determine sufficiently large lookaheads.




** Introspection and code generation
For the critical task of analyzing the model, identifying the processes and the links between them, we will follow ForSyDe SystemC's approach \cite{Hosein2012}.
Using SystemC's well defined API for module hierarchy (e.g. \texttt{get\_child\_objects()}), along with the introduction of meta objects, the system's structure can be
serialized at runtime, in the pre simulation phase of elaboration.




** Hardware and Software tools 
To ensure efficiency and code readability, we will use the explicit threading mechanisms that come with the latest standards of C++.
The Intel Parallel Studio XE 2016 toolchain will be used for compilation, code analysis and optimization.
We will initially use the Intel® Xeon Phi™ 5120D Coprocessor as the host platform for the simulation.
The coprocessor is situated in a Intel® Xeon E5-2600M v3 server (named lovisa).



** Evaluation Metrics
The first evaluation metric of the proposed kernel will be its strong scalability against the reference SystemC kernel.
It will be determined by keeping the simulation's size constant and varying the number of processing elements.
Furthermore, we will also measure weak scalability, by varying the number of processing elements and the simulation's size symmetrically,
and trying to achieve constant time to simulation end.

The simulation's size can be easily related to the duration of the simulation (in simulated time).
Another way of describing the simulation's size is through the conception of a formula involving the number of system processes, the number of links, the system's topology and the amount of events generated.

The accuracy of the simulation can be measured by the aggregate number of causality errors.
The detection of causality errors must be facilitated in a per process level and the aggregation shall be performed at the end of the simulation.
A concrete realization of the accuracy metric comes in the form of a counter each process increments whenever it executes an event with a time-stamp lower than its clock (the time-stamp of the last processed event).



* Tasks and Time Scheduling
The majority of the remaining time has been partitioned into 3 x 3-4 week long iterations, 
where we will incrementally try to achieve our objectives.
The tasks that comprise the first iteration are the following:
  1. Setup software environment on the first experimentation server (Lovisa)
  2. Manually compile the TLM 2.0 LT example of the SystemC installation, to a multithreaded C++ application, according to the CMB algorithm.
  3. Document Task 1
  4. Find a way to create parameterizable random networks within C++, that can be serialized in a form Matlab can analyze and visualize them.
  5. Create "dummy" process functionality that resembles the behavior of common TLM actors (Instruction Set Simulators, Memory, Timers etc)
  6. Perform a number of simulations of the random systems that correspond to the random networks.
  7. Document Tasks 3-5.

The context of the next iterations will be decided based on the outcome of the first.
To summarize:

#+ATTR_LATEX: :align=|c|c|c|c|
| *Week(s)* | *End Date* |         *Task Number* | *Milestone*     |
|-----------+------------+-----------------------+-----------------|
|       5-7 |      02-18 | Initial Investigation | Project Plan v1 |
|-----------+------------+-----------------------+-----------------|
|         7 |      02-19 |                     1 |                 |
|-----------+------------+-----------------------+-----------------|
|         8 |      02-26 |                     2 |                 |
|-----------+------------+-----------------------+-----------------|
|         9 |      03-02 |                     3 | Status Report 1 |
|-----------+------------+-----------------------+-----------------|
|         9 |      03-04 |                     4 |                 |
|-----------+------------+-----------------------+-----------------|
|        10 |      03-11 |                     5 |                 |
|-----------+------------+-----------------------+-----------------|
|        11 |      03-18 |                     6 |                 |
|-----------+------------+-----------------------+-----------------|
|        12 |      03-23 |                     7 | Status Report 2 |
|-----------+------------+-----------------------+-----------------|
|     12-15 |      04-14 |      Second Iteration | Status Report 3 |
|-----------+------------+-----------------------+-----------------|
|     16-19 |      05-12 |      Third  Iteration | Status Report 4 |
|-----------+------------+-----------------------+-----------------|
|     20-21 |            |         Reserve weeks |                 |
|-----------+------------+-----------------------+-----------------|
|     22-26 |      06-30 |       Project closure | Final Report    |


* References
\renewcommand\refname{}
\bibliography{References}
\bibliographystyle{myIEEEtran}


* 1st Iteration Notes
** What is Blocking and Non-Blocking in MPI's context
The classification is with respect to whether the buffer involved in the communication primitive
is available for re-use in case of send or use in case of receive.

The 4 communication modes still apply for both categories.

_A *nonblocking send* call indicates_
that the system may start copying data out of the send buffer. 
The sender should not modify any part of the send buffer after a nonblocking send operation is called, 
until the send completes.

The completion of a send operation indicates that the sender is now free to update the locations in the send buffer 
It does not indicate that the message has been received, rather, 
it may have been buffered by the communication subsystem.

However, if a *synchronous mode* send was used, the completion of the send operation indicates 
that a matching receive was initiated, 
and that the message will eventually be received by this matching receive.


_A *nonblocking receive* call indicates_
that the system may start writing data into the receive buffer. 
The receiver should not access any part of the receive buffer after a nonblocking receive operation is called, until the receive completes.

The completion of a receive operation indicates that the receive buffer contains the received message, 
the receiver is now free to access it, and that the status object is set. 
It does not indicate that the matching send operation has completed (but indicates, of course, that the send was initiated).






** MPI_Status
The source or tag of a received message may not be known if wildcard values were used in the receive operation. 
Also, if multiple requests are completed by a single MPI function (see Section 3.7.5), a distinct error code may need to be returned for each request.

The status argument also returns information on the length of the message received.
However, this information is not directly available as a field of the status variable and a call to MPI_GET_COUNT is required to “decode” this information.


** MPI Communication modes
As dictated by the MPI version 3.0 standard the following communication modes
are supported \cite{MessagePassingInterfaceForum2012}

_No-prefix for *standard mode*_
In this mode, it is up to MPI to decide whether outgoing messages will be buffered. 

MPI may buffer outgoing messages. 
In such a case, the send call may complete before a matching receive is invoked. 

On the other hand, buffer space may be unavailable, or MPI may choose not to buffer outgoing messages, for performance reasons. 
In this case, the send call will not complete until a matching receive has been posted, and the data has been moved to the receiver.

_B for *buffered mode*_ 
A buffered mode send operation can be started whether or not a matching receive has been posted. 
It may complete before a matching receive is posted. 

However, unlike the standard send, this operation is local, and its completion does not depend on the occurrence of a matching receive. 
Thus, if a send is executed and no matching receive is posted, then MPI *must buffer the outgoing message*, so as to allow the send call to complete. 
*An error will occur if there is insufficient buffer space*. 

The amount of available buffer space is controlled by the user — see Section 3.6. 
Buffer allocation by the user may be required for the buffered mode to be effective.

_S for *synchronous mode*_
A send that uses the synchronous mode can be started whether or not a matching receive was posted. 
However, *the send will complete successfully only if a matching receive is posted*, and the receive operation has started to receive the message sent by the synchronous send. 

Thus, the completion of a synchronous send not only indicates that the send buffer can be reused, 
but it also indicates that the receiver has reached a certain point in its execution, 
namely that it has started executing the matching receive. 

If both sends and receives are blocking operations then the use of the synchronous mode provides synchronous communication semantics: 
a communication does not complete at either end before both processes *rendezvous* at the communication.

_*R for ready mode*_
A send that uses the ready communication mode may be started *only if the matching receive is already posted*. 

Otherwise, the operation is erroneous and its outcome is undefined.
Ready sends are an optimization when it can be guaranteed that a matching receive has already been posted at the destination.

On some systems, this allows the removal of a hand-shake operation that is otherwise required and results in improved performance. 

The completion of the send operation does not depend on the status of a matching receive, and merely indicates that the send buffer can be reused. 

A send operation that uses the ready mode has the same semantics as a standard send operation, or a synchronous send operation; 
it is merely that the sender provides additional information to the system (namely that a matching receive is already posted), that can save some overhead. 

In a correct program, therefore, a ready send could be replaced by a standard send with no effect on the behavior of the program other than performance.

The communication modes are indicated by a one letter prefix.


** Semantics of point-to-point communication
One can think of message transfer as consisting of the following three phases
1. Data is pulled out of the send buffer and a message is assembled
2. A message is transferred from sender to receiver
3. Data is pulled from the incoming message and disassembled into the receive buffer

In a multithreaded implementation of MPI, the system may de-schedule a thread that is blocked on a send or receive operation,
and schedule another thread for execution in the same address space.
*In such a case it is the user's responsibility not to modify a communication buffer until the communication completes*.
Otherwise, the outcome of the computation is undefined

_Order:_
Messages are non-overtaking.

If a sender sends two messages in succession to the same destination, 
and both match the same receive, 
then this operation cannot receive the second message if the first one is still pending. 

If a receiver posts two receives in succession,
 and both match the same message, 
then the second receive operation cannot be satisfied by this message, if the first one is still pending. 

This requirement facilitates matching of sends to receives. 
It guarantees that message-passing code is deterministic, 
if processes are single-threaded and the wildcard MPI_ANY_SOURCE is not used in receives. 
(Some of the calls described later, such as MPI_CANCEL or MPI_WAITANY, are additional sources of nondeterminism.)

_Progress:_
?

_Fairness:_
MPI makes no guarantee of fairness in the handling of communication. 

Suppose that a send is posted. 
Then it is possible that the destination process repeatedly posts a receive that matches this send, 
yet the message is never received, 
because it is each time overtaken by another message, 
sent from another source. 

Similarly, suppose that a receive was posted by a multithreaded process. 
Then it is possible that messages that match this receive are repeatedly received, 
yet the receive is never satisfied, 
because it is overtaken by other receives posted at this node (by other executing threads). 

It is the programmer’s responsibility to prevent starvation in such situations.

_Resource limitations:_
A buffered send operation that cannot complete because of a lack of buffer space is erroneous. 
When such a situation is detected, an error is signaled that may cause the program to terminate abnormally. 
On the other hand, a standard send operation that cannot complete because of lack of buffer space will merely block, 
waiting for buffer space to become available or for a matching receive to be posted. 
This behavior is preferable in many situations. 

Consider a situation where a producer repeatedly produces new values and sends them to a consumer. 
Assume that the producer produces new values faster than the consumer can consume them. 
If buffered sends are used, then a buffer overflow will result. 
Additional synchronization has to be added to the program so as to prevent this from occurring. 
If standard sends are used, then the producer will be automatically throttled,

_A program is *safe* if no message buffering is required for the program to complete_. 
One can replace all sends in such program with synchronous sends, and the program will still run correctly. 
This conservative programming style provides the best portability, 
since program completion does not depend on the amount of buffer space available or on the communication protocol used.


** Failed Attempt to implement the original Bryant Chandi Misra


** Fujimoto
*Lookahead:* If a logical process at simulation time T can only schedule new events with thime stamp of at least T+L, then KL is referred to as the lookahead for the logical process.

An alternative approach to sending a null message after processing each event is a demand-driven approach.
Whenever a process is about to become blocked because the incoming link with the smallest link clock value has no messages waiting to be processed,
it requests the next message (null or otherwise) from the process on the sending side of the link.
The process resumes execution when the response to this request is achieved.
This approach helps to reduce the amount of null message traffic, though a longer delay may be required to receive null messages because two message transmissions are required.


* Computer Science Cheatsheet                                      :noexport:
An _Algorithm_ is a finite description of a sequence of steps to be taken to solve a problem.
Physical processes are rarely structured as a sequence of steps; rather, they are structured as _continuous interactions between concurrent components_.

_Model vs Reality:_ You will never strike oil by drilling through the map (Golomb 1971)

_Concurrency vs Parallelism:_ Consider two "living" threads. On a multicore machine they might be executed in parallel.
On a single core the instructions of each thread are arbitrarily interleaved. In both cases the execution is these two 
threads is characterized as concurrent. Concurrency does not imply simultaneity.

_Chattering Zeno model:_ A moment in the simulation where execution is happening within delta time, not allowing the simulation time to progress.

_Zeno model:_ A model (like Achilles and the Turtle) where simulation time advances slower and slower until it reaches a point where 
it can not advance further(time increment becomes lower than the resolution) and gets trapped in delta time.

_A simulation_ is defined as the execution of model revealing the behaviour of the system being modeled.
A system can be analyzed either by being formally verified or simulated.
Simulation beyond analysis, as a means of constructing a virtual platform.

_A binary file:_ a statically linked library, a dynamically linked library, an object module, a standalone executable.
All binary files contain  meta information, such as the symbol table.

_False Sharing:_ The silent performance killer.
When cores communicate using "shared memory", they are often really just communicating through the cache coherence mechanisms.
A pathological case can occur when two cores access data that happens to lie in the same cache line. 
Normally, cache coherence protocols assign one core, the one that last modifies a cache line, to be the owner of that cache line. If two cores write to the same cache line repeatedly, they fight over ownership. 
Importantly, note that this can happen even if the cores are not writing to the same part of the cache line.
Write contention on cache lines is the single most limiting factor on achieving scalability for parallel threads of execution in an SMP system. \cite{McCool2012}em

_Design Automation_ depends on the high-level modelling and specification of systems.

_Reentrancy (vs Thread Safety):_ A subroutine is called *re-entrant* if it can be interrupted in the middle of its execution and then safely called again (re-entered, for example by the ISR) before its previous invocations complete execution.
*Recursive subroutines must be re-entrant*. A thread-safe code does not necessarily have to be re-entrant.
#+BEGIN_SRC C++
void thread_safe()
{
   acquire_lock
        if interrupted here and the ISR tries to re-enter we are fucked.
   release_lock
}
#+END_SRC

_A computer language:_ can be regarded the medium of communicating an algorithm to a machine.
We want the language to be expressive (like the greek language), portable (like the english language) and efficient (like the swedish)

_Data Parallelism:_ parallelism determined implicitly by data *independence*.

_Bash & C:_ brick and mortar


* ESLD Cheatsheet 						   :noexport:
_RTL modules are pin-accurate:_ This means that the ports of an RTL module directly correspond to wires in the real-world implementation of the module. 

_Is the MoC underlying a PDES a special form of KPN?_


* Terminology 							   :noexport:
| Process   | Provides necessary modeling of independently timed circuits                              |
| Process   | A design artifact that models the behaviour or an aspect of the behaviour an entity has. |
| ESL       | Electronic System Level modeling                                                         |
| Initiator | Historically known as Master                                                             |
| Target    | Historically known as Slave                                                              |


* C++ 								   :noexport:
** Explicit threading in C++
#+BEGIN_SRC cpp
#include <thread>
#+END_SRC


** Introspection vs Reflection
Super important to check Qt.
Although it is a GUI thing, it has a DES (maybe PDES, each QThread runs its own event loop) and a Meta Object Compiler.


** Iterators
Iterators connect algorithms to the elements in a container regardless of the type of the container.
Iterators decouple the algorithm from the data source; an algorithm has no knowledge of the container form which the data originates. 

** DANGER
#+BEGIN_SRC cpp
  class Base{
      void foo(){}
  };
  
  
  class Derived : public Base{
      void bar(){}
  };
  
  
  void dangerous(Base *p, int n){
      for(int i=0; i!=n; i++)
          p[i].foo();
  };
  
  
  void initiate_chaos(){
      Derived d[10];
      dangerous(d, 10);
  }
#+END_SRC


* SystemC 							   :noexport:
** General

*** Parsing the SystemC standard for occurences of the word kernel
Clause 4 of \cite{OpenSystemCInitiative2009} "_Elaboration and simulation semantics_", defines the behavior of the SystemC kernel
and is central to an understanding of SystemC.

The _execution_ of a SystemC application consists of _elaboration_ followed by _simulation_.
Elaboration results in the creation of the module hierarchy.
Elaboration involves the execution of application code, the public shell of the implementation, and the private kernel of the implementation.
Simulation involves the execution of the scheduler, part of the kernel, which in turn may execute processes within the application.

The purpose of the process macros is to _register the associated function with the kernel such that the scheduler can call back that member function during simulation_.

When a port is bound to a channel, the kernel shall call the member function register_port of the channel.

Simulation time is initialized to zero at the start of simulation and increases monotonically during simulation.
The physical significance of the integer value representing time within the kernel is determined by the simulation time resolution.

Since process instances execute without interruption, only a single process instance can be running at any one time,
and no other process instance can execute until the currently executing process instance has yielded control to the kernel.
_A process shall not pre-empt or interrupt the execution of another process._
_This is known as co-routine semantics or co-operative multitasking_

The SystemC sc_module class provides four routines that may be overridden, and they are executed at the boundaries of simulation.
These routines provide modelers with a place to put initialization and clean-up code that has no place to live.
For example, checking the environment, reading run-time configuration information and generating summary reports at the end of simulation.
#+BEGIN_SRC cpp :exports code
void before_end_of_elaboration(void);
void end_of_elaboration(void);
void start_of_simulation(void);
void end_of_simulation(void);
#+END_SRC

A thread of clocked thread process instance is said to be resumed when the kernel causes the process to continue execution,
starting with the statement immediately following the most recent call to function wait.

If the thread or clocked thread process executes the entire function body or executes a return statement and thus returns control to the kernel,
the associated function shall not be called again for that process instance. The process instance is then said to be terminated.

The function next_trigger does not suspend the method process instance; a method process cannot be suspended but always executes to completion before
returning control to the kernel.

The distinction between _suspend/resume_ and _disable/enable_ lies in the sensitivity of the target process during the period while it is suspended or disabled.
With _suspend_ the kernel keeps track of the sensitivity of the target process while it is suspended such that a relevant event notification or time-out 
while suspended would cause the process to become runnable immediately when resume is called.
With _disable_ the sensitivity of the target process is nullified while it is suspended such that the process is not made runnable by the call to enable, but only on the next
relevant event notification or time-out subsequent to the call to enable.

If a process kills itself, the statements following the call to kill shall not be executed again during the current simulation, and control shall return to the kernel.

_STOPPED AT OCCURENCE 44_


*** Parsing the SystemC standard for occurences of the phrase set of
Set of runnable processes
Set of update requests
Set of delta notifications
Set of time-outs
Set of timed notifications


*** Parsing the SystemC standard for occurences of the phrase simulation time
43/105:
Synchronization may be strong in the sense that the sequences of communication events
is precisely determined in advance, or weak in the sense that the sequence of communication events
is partially determined by the detailed timing of the individual processes.

Strong synchronization is easily implemented in SystemC using FIFOs or semaphores, allowing a completely
untimed modeling style where in principle simulation can run without advancing simulation time.

Untimed modeling in this sense is outside the scope of TLM 2.0. On the other hand, a fast virtual
platform model allowing multiple embedded software threads to run in parallel may use either strong or weak
synchronization. In this standard, the appropriate coding style for such a model is termed loosely-timed.


*** Port vs Export
The purpose of port and export bindings is to enable a port or export to _forward interface method calls made during simulation._
A port _requires_ the services defined by an interface.
An export _provides_ the services defined by an interface.

Forward path form initiator to target.
Backward path from target back to initiator.


*** TODO Parsing the SystemC standard for occurences of the phrase update phase 











SC_THREADs are not threads. They are coroutines.

Coroutines are subroutines that allow multiple entry points for suspending and resuming execution at certain locations.

SystemC does not offer real concurrency. It simulates concurrency using ...

The SystemC kernel implements cooperative scheduling where each SC_THREAD willingly relinquishes control to allow other SC_THREADs to execute.

In order to implement that cooperative scheduling strategy using coroutines, a threading library is used.


The scheduler advances simulation time to the time of the next event, 
then runs any processes due to run at that time of sensitive to that event.

Computations that take some time are usually modeled by instantaneous computations followed by a SystemC wait.

A _scheduler_ manages the threads by use of queues, such as READY, which contains all those that are ready to execute
and WAIT which contains threads waiting for events.

_Threads_ switch between READY and WAIT during simulation subject to event notification and time advances.

Events are delivered in an inner loop called _delta-cycle_ and simulation time advances in an outer loop _time-cycle_.


** Co-routine semantics
\cite{OpenSystemCInitiative2012}
Since process instances execute without interruption, only a single process instance can be running at any one time, 
and no other process instance can execute until the currently executing process instance has yielded control to the kernel.
A process shall not pre-empt or interrupt the execution of another process.
This is known as co-routine semantics or co-operative multitasking

An implementation running on a machine that provides hardware support for concurrent processes may permit two or more processes to run concurrently
provided that the behavior appears identical to the co-routine semantics defined in this subclause.
In other words, the implementation would be obliged to analyze any dependencies between processes and to constrain their execution to match the co-routine semantics.

Software modules that interact with one another as if they were performing I/O operations. (Conway 1963)

Co-routine semantics are linked to Kahn process networks.

*** Impediments to speed
_Context switching:_
- Every time you see a SC_THREAD -> _wait_ or a SC_METHOD -> _next_trigger() return_
- Complex bus protocols and lots of processes


** Dynamic processes with sc_spawn


* TLM 2.0 							   :noexport:
** General
The story of Virtual Platform is old

- Transaction-level memory-mapped _bus modeling_.
- Register accurate and functionally complete.
- Fast enough to boot software O/S in seconds.
- Loosely-timed and approximately-timed modeling.
- Interoperable API for memory-mapped bus modeling.
- Generic payload and extension mechanism
- Avoid adapters where possible

Facilitating the simulation of systems, with inter communicating components.
The components are modeled on a functional level.

TLM 2.0 consists of:
- A set of core interfaces
  - Blocking
  - Non-blocking
  - DMI
  - Debug transport interface
- The global quantum
- Initiator and target sockets
- Generic payload
- Base protocol
- Utilities


References to a single transaction object are passed along the forward and backward paths.


#+BEGIN_LATEX
\tikzstyle{block} = [draw, fill=blue!4!white, rectangle, minimum height=3em, minimum width=6em]
\begin{figure}
\begin{tikzpicture}[auto, node distance=2cm]

\node [block] (payload) {Generic payload};
\node [block, right of=payload] (phases)  {Phases};
\node [block, below of=payload] (sockets) {Initiator and target sockets};
\node [block, below of=sockets] (tlm)     {TLM-2 core interfaces: 
                                               \begin{itemize}
					       \item {Blocking transport interface}
					       \item {Non-blocking transport interface}
					       \item {Direct memory interface}
					       \item {Debug transport interface}
					       \end{itemize}
					       };

\draw [->] (payload) -- (sockets);
\draw [->] (phases)  -- (sockets);
\draw [->] (sockets) -- (tlm);

\end{tikzpicture}\caption{TLM 2.0 Interoperability layer for bus modeling}
\end{figure}
#+END_LATEX

The TLM LRM provides guidance for constructing interoperable models


** Transaction
A transaction is an abstraction of communication.
Two way communication.


** Sockets
A socket combines a port with an export.
An _initiator socket_ is derived from class sc_port and has an sc_export. It has the port for the forward path and the export for the backward path.
An _target_socket_    is derived from class sc_export and has an sc_port ([[~/pSystemC/src/tlm_core/tlm_2/tlm_sockets/tlm_target_socket.h][tlm_base_target_socket]])

Only the most derived classes *tlm_initiator_socket* and *tlm_target_socket* are typically used directly by applications. 
These two sockets are parameterized with a protocol traits class that defines the types used by the forward and backward interfaces.
Sockets can only be bound together if they have the identical protocol type.


** Generic Payload
It supports the _abstract modeling of memory-mapped buses_, 
together with an extension mechanism to support the modeling of specific bus protocols whilst maximizing interoperability.

The main features of the generic payload are:
- Command 
  Is it read or write?
- Address
  What is the address
- Data
  A pointer to the physical data as an array of bytes
- Byte Enable Mask
- Response
  An indication of whether the transaction was successful, and if not the nature of the error


** Initiators and Targets
A module's processes may act as either initiators or targets.
An initiator is responsible for creating a payload and calling the transport function to send it.
A target receives payloads from the transport function for processing and response.
In the case of non-blocking interfaces the target may create new transactions backwards in response to a transaction from an initiator.
Initiator calls are made through initiator sockets, target calls received through target sockets.
A module may implement both target and initiator sockets, allowing its threads to both generate and receive traffic.


** Blocking, Non-Blocking, Debug and Interfaces/Transport Call
_How does TLM contribute to performance boost:_ You do 1 wait, rather than many waits.

With the blocking interface you can have wat() on the target code.

Why does the nb_transport_if defines 4 phases?
- To enable


** Direct Memory Interface
_Characteristics:_
- Allows direct backdoor access into memory
- *Allows un-inhibited ISS execution:* 
  (Instead of roaming through the hierarchy of a buss system-Fast software execution)

** Socket
In order to pass transactions between initiators and targets, TLM-2.0 uses sockets.
An initiator sends transactions out through an _initiator socket_, and a target receives incoming transactions through a _target socket_.
A socket is basically a convinience class, wrapping up a port and an export.

[[file:Figures/tlm_socket.png]]






** Blocking interface
This interface allows only two timing points to be associated with each transaction, 
corresponding to the call to and return from the blocking transport function.



** Loosely Timed Coding Style
Notes from Video Lecture: [[http://videos.accellera.org/tlm20tutorial/David_Black/player.html][David Black, XtremeEDA USA: TLM Mechanics]]					   
_FAST-NOT ACCURATE_ (In terms of timing?): Less detail means faster simulation. Less context switching means also faster simulation.
A fast, loosely-timed model is typically expected to use the _blocking transport interface_ the _DMI_ and _temporal decoupling_.
_Older terminology:_ UnTimed - Programmer's View
_Use Cases:_
- Early Software Development
_Characteristics:_
- Only sufficient timing detail to _boot O/S and run multi-core systems. It can express the modeling of _timers and _interrupts_
- Processes can run ahead of simulation time (_temporal decoupling_)
- Each transaction has _2 timing points_: begin and end
- Uses direct memory interface (_DMI_)

_Temporal decoupling:_
Each process runs ahead up to quantum boundary.
sc_time_stamp() advances in multiples of the quantum.
Deterministic communication requires explicit synchronization.

_DMI:_
When combined with temporal decoupling may lead to completely crappy situations.
The language neither the simulator do not protect the designer.
It is like a hole in the legal system.


** Approximately-timed
_ACCURATE_ (In terms of timing?)
_Older terminology:_ Cycle Accurate
_Use cases:_
- Architectural Analysis, Software Performance Analysis
- Hardware Verification


** Loosely-timed coding style and temporal decoupling
Individual SystemC processes are permitted to run ahead in a local "time warp" without actually advancing simulation time
until they need to synchronize with the rest of the system.
Temporal decoupling can result in very fast simulation for certain systems because it increases the data and code locality and reduces scheduling overhead of the simulator.

*Each process is allowed to run for a certain time slice or quantum before switching to the next, or instead may yield control when it reaches an explicit synchronization point.*

The quantum value represents a tradeoff between simulation speed and accuracy.

For a fine grained model, the overhead of event scheduling and process context switching becomes the dominant factor in simulation speed.
Therefore allowing a process to run ahead of the simulation time will speed up the simulation.
Until it needs to interact with another process, for example read or update a variable belonging to another process.

The processs that runs ahead of simulation time creates a time warp

Each process is responsible for determining whether it can run ahead of simulation time without breaking the functionality of the model.
When a process encounters an external dependency it has two choices: either force synchronization, 
which means yielding to allow all other processes to run as normal until simulation time catches up, or sample or update the current value and continue.


* Distributed Simulations 					   :noexport:
I must say something about parallelization through running many parallel/distributed simulations since it is a common practise in the industry.
This is how they are solving the problem at the moment.
Maybe quote my supervisor.


* Super Important Issues 					   :noexport:
\cite{Fujimoto1999}
If each LP adheres to the local causality constraint, then the
parallel/distributed execution will yield exactly the same results as a sequential
executlOn of the same simulation program provided that events containing the same
timestamp are processed in the same order in both the sequential and parallel
execution. Events containing the same time stamp are referred to as simultaneous
events.

\cite{Fujimoto1999}
Messages arriving on each incoming link can be stored in a first-in-first-out
(FIFO) queue, which is also time stamp order because of the above restrictions.
*Here, we ignore "local" events that are scheduled by an LP for itself*
In practice, processing ofthese events must be interleaved with the processing ofmessages from
other LPs so that all events are processed in time stamp order, however, this is easy
to accomplish.


* AXM5500 							   :noexport:

A family of communication processors developped by Axxia, formerly owned by LSI (now Intel).

*From the press release of AXM5500:*

The AXM5500 is a flexible combination of general-purpose processors and specialized packet-processing acceleration engines.
These processors and engines use Virtual Pipeline technology, a message-passing control path to efficiently and autonomously process packets.


* Graveyard of potentially usefull plagiarisms                     :noexport:
A very _rudimentary_ look at TLM


An implementation running on a machine that provides hardware support for concurrent processes may permit two or more processes to run concurrently
provided that the behavior appears identical to the co-routine semantics defined in this subclause.
In other words, the implementation would be obliged to analyze any dependencies between processes and to constrain their execution to match the co-routine semantics.



An important limitation of SystemC regarding performance is that the reference implementation is sequential, 
and the official semantics, just like any other Discrete Event Simulator (henceforth DES), make parallel execution difficult.
Most existing work on parallelization of SystemC targets cycle-accurate simulation,
and would be inefficient on loosely timed systems since they cannot run in parallel processes that do not execute simultaneously \cite{Moy}.

\cite{Moy}
The SystemC standard allows this, "provided that the behavior appears identical to the co-routine semantics" \cite{OpenSystemCInitiative2012}
This implies two constraints on a parallel implementation:

- It should not change the order in which processes are allowed to be executed. 
  In particular, the simulated time imposes an order on the execution of processes.
  
An optimistic approach would relax this constraint having a violation detection and rollback mechanism to correct any violations afterwards.
Although this may seem to work with VHDL, with SystemC this is chaotic, since arbitrary C++ code and system calls.

- It should not introduce new race conditions.
  For example, two SystemC processes may safely execute x++ on a shared variable, but running two such processes in parallel cannot be allowed.
  The co-routine semantics of the SystemC kernel guarantee that there will be no race conditions.
  Evaluate-update paradigm


How to realize the DE MoC on top of completely heterogeneous HPC platform 


* Latex Headers 						   :noexport:
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt,twoside]
#+LATEX_HEADER: \usepackage[paper=a4paper,dvips,top=1.5cm,left=1.5cm,right=1.5cm, foot=1cm,bottom=1.5cm]{geometry}
#+LATEX_HEADER: \renewcommand{\rmdefault}{ptm} 
#+LATEX_HEADER: \usepackage[scaled=.90]{helvet}
#+LATEX_HEADER: \usepackage{courier}
#+LATEX_HEADER: \usepackage{bookmark}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \pagestyle{fancy}
#+LATEX_HEADER: \usepackage[dvipsnames*,svgnames]{xcolor} 
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,fit,positioning,calc,shapes}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[swedish,english]{babel}
#+LATEX_HEADER: \usepackage{rotating}		
#+LATEX_HEADER: \usepackage{array}		
#+LATEX_HEADER: \usepackage{graphicx}	 
#+LATEX_HEADER: \usepackage{float}	
#+LATEX_HEADER: \usepackage{color}      
#+LATEX_HEADER: \usepackage{mdwlist}
#+LATEX_HEADER: \usepackage{setspace}   
#+LATEX_HEADER: \usepackage{listings}	
#+LATEX_HEADER: \usepackage{bytefield}  
#+LATEX_HEADER: \usepackage{tabularx}	
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{algpseudocode}	
#+LATEX_HEADER: \usepackage{dcolumn}	
#+LATEX_HEADER: \usepackage{url}	
#+LATEX_HEADER: \usepackage[perpage,para,symbol]{footmisc} 
#+LATEX_HEADER: \usepackage[all]{hypcap}
#+LATEX_HEADER: \definecolor{darkblue}{rgb}{0.0,0.0,0.3} %% define a color called darkblue
#+LATEX_HEADER: \definecolor{darkred}{rgb}{0.4,0.0,0.0}
#+LATEX_HEADER: \definecolor{red}{rgb}{0.7,0.0,0.0}
#+LATEX_HEADER: \definecolor{lightgrey}{rgb}{0.8,0.8,0.8} 
#+LATEX_HEADER: \definecolor{grey}{rgb}{0.6,0.6,0.6}
#+LATEX_HEADER: \definecolor{darkgrey}{rgb}{0.4,0.4,0.4}
#+LATEX_HEADER: \hyphenpenalty=15000 
#+LATEX_HEADER: \tolerance=1000
#+LATEX_HEADER: \newcommand{\rr}{\raggedright} 
#+LATEX_HEADER: \newcommand{\rl}{\raggedleft} 
#+LATEX_HEADER: \newcommand{\tn}{\tabularnewline}
#+LATEX_HEADER: \newcommand{\colorbitbox}[3]{%
#+LATEX_HEADER: \rlap{\bitbox{#2}{\color{#1}\rule{\width}{\height}}}\bitbox{#2}{#3}}
#+LATEX_HEADER: \newcommand{\red}{\color{red}}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}{-3.25ex\@plus -1ex \@minus -.2ex}{1.5ex \@plus .2ex} {\normalfont\normalsize\bfseries}}
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \renewcommand\subparagraph{\@startsection{subparagraph}{5}{\z@}{-3.25ex\@plus -1ex \@minus -.2ex}{1.5ex \@plus .2ex} {\normalfont\normalsize\bfseries}}
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \setcounter{tocdepth}{3}
#+LATEX_HEADER: \setcounter{secnumdepth}{5}
#+LATEX_HEADER: \renewcommand{\headrulewidth}{0pt}
#+LATEX_HEADER: \lhead{K.Sotiropoulos: Master's Thesis}
#+LATEX_HEADER: \chead{Project Plan version 1}
#+LATEX_HEADER: \rhead{\date{\today}}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \let\ps@plain\ps@fancy 
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \setlength{\headheight}{15pt}
